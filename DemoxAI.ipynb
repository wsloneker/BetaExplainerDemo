{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "677ae086-6213-4e58-8dea-1dbda2cfbc85",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import random, math\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from types import MethodType\n",
    "from typing import Optional, Callable, Union\n",
    "from functools import partial\n",
    "from torch_geometric.utils import k_hop_subgraph, to_undirected\n",
    "from torch_geometric.data import Data\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import networkx as snx\n",
    "import tqdm\n",
    "import torch_geometric.utils as pyg_utils\n",
    "import itertools\n",
    "import os.path as osp\n",
    "import os\n",
    "from torch.nn import PairwiseDistance as pdist\n",
    "from typing import Optional  \n",
    "import random\n",
    "import pickle\n",
    "from copy import deepcopy\n",
    "from typing import Tuple\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.utils.convert import to_networkx\n",
    "from torch_geometric.data import Dataset, data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from typing import List, Optional, Callable, Union, Any, Tuple\n",
    "from sklearn.utils.random import sample_without_replacement\n",
    "import numbers\n",
    "from functools import partial\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_predict\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "import torch_geometric.transforms as Tr\n",
    "from torch_geometric.nn import GCNConv, SAGEConv, ChebConv, GatedGraphConv, Linear, global_mean_pool, global_max_pool\n",
    "from torch.nn import functional as F\n",
    "import pandas as pd\n",
    "import csv\n",
    "import sklearn.metrics   \n",
    "import sklearn\n",
    "import sys\n",
    "import ipdb\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import MessagePassing\n",
    "import torch\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from types import MethodType\n",
    "from typing import Callable, Union\n",
    "from functools import partial\n",
    "from torch_geometric.utils import k_hop_subgraph, to_undirected\n",
    "from torch_geometric.data import Data\n",
    "import matplotlib\n",
    "import tqdm\n",
    "import itertools\n",
    "import time\n",
    "from torch_geometric.nn import GINConv\n",
    "import optuna\n",
    "import torch.optim as optim\n",
    "from NodeBetaExplainer import BetaExplainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da87bb97-d485-4da1-8d96-fadfdced07ac",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_flag():\n",
    "    pass\n",
    "\n",
    "def set_seed(seed: int = 42) -> None:\n",
    "    '''This function allows us to set the seed for the notebook across different seeds.'''\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # When running on the CuDNN backend, two further options must be set\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    # Set a fixed value for the hash seed\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    print(f\"Random seed set as {seed}\")\n",
    "\n",
    "triangle = nx.Graph()\n",
    "triangle.add_nodes_from([0, 1, 2])\n",
    "triangle.add_edges_from([(0, 1), (1, 2), (2, 0)])\n",
    "house = nx.house_graph()\n",
    "\n",
    "def optimize_homophily(\n",
    "        x: torch.Tensor, \n",
    "        edge_index: torch.Tensor,\n",
    "        label: torch.Tensor,\n",
    "        feature_mask: torch.Tensor, \n",
    "        homophily_coef: float = 1.0, \n",
    "        epochs: int = 50, \n",
    "        connected_batch_size: int = 10,\n",
    "        disconnected_batch_size: int = 10,\n",
    "    ):\n",
    "    '''\n",
    "    Optimizes the graph features to have a set level of homophily or heterophily\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor): Initial node features. `|V| x d` tensor, where `|V|` is number of nodes,\n",
    "            `d` is dimensionality of each node feature.\n",
    "        edge_index (torch.Tensor): Edge index. Standard `2 x |E|` shape.\n",
    "        label (torch.Tensor): All node labels. Shape `|V|,` tensor.\n",
    "        feature_mask (torch.Tensor): Boolean tensor over the dimensions of each feature. Tensor\n",
    "            should be size `d,`.\n",
    "        homophily_coef (float, optional): Homophily coefficient on which to optimize the level \n",
    "            of homophily or heterophily in the graph. Positive values indicate homophily while\n",
    "            negative values indicate heterophily. (:default: :obj:`1.0`)\n",
    "        epochs (int, optional): Number of epochs on which to optimize features. (:default: :obj:`50`)\n",
    "        connected_batch_size (int, optional): Batch size at each epoch for connected nodes on which \n",
    "            to observe for the loss function. (:default: :obj:`10`) \n",
    "        disconnected_batch_size (int, optional): Batch size at each epoch for disconnected nodes on which \n",
    "            to observe for the loss function. (:default: :obj:`10`) \n",
    "\n",
    "    :rtype: `torch.Tensor`\n",
    "    Returns:\n",
    "        x (torch.Tensor): Optimized node features.\n",
    "    '''\n",
    "\n",
    "    to_opt = x.detach().clone()[:,feature_mask]\n",
    "\n",
    "    optimizer = torch.optim.Adam([to_opt], lr=0.3)\n",
    "    to_opt.requires_grad = True\n",
    "\n",
    "    # Get indices for connected nodes having same label\n",
    "    c_inds = torch.randperm(edge_index.shape[1])[:connected_batch_size]\n",
    "    c_inds = c_inds[label[edge_index.t()[c_inds][:, 0]] == label[edge_index.t()[c_inds][:, 1]]]\n",
    "\n",
    "    # Get indices for connected nodes having different label\n",
    "    nc_inds = torch.randperm(edge_index.shape[1])[:connected_batch_size]\n",
    "    nc_inds = nc_inds[label[edge_index.t()[nc_inds][:, 0]] != label[edge_index.t()[nc_inds][:, 1]]]\n",
    "\n",
    "    # Get set of nodes that are either connected or not connected, with different labels:\n",
    "    # [[a1, a2, a3, ...], [b1, b2, b3, ...]]\n",
    "    nc_list1 = torch.full((disconnected_batch_size,), -1) # Set to dummy values in beginning\n",
    "    nc_list2 = torch.full((disconnected_batch_size,), -1)\n",
    "\n",
    "    nodes = []\n",
    "    for i in range(0, x.shape[0]):\n",
    "        nodes.append(i)\n",
    "\n",
    "    for i in range(disconnected_batch_size):\n",
    "        c1, c2 = random.choice(nodes), random.choice(nodes)\n",
    "\n",
    "        # Get disconnected and with same label\n",
    "        while if_edge_exists(edge_index, c1, c2) or \\\n",
    "                torch.any((nc_list1 == c1) & (nc_list2 == c2)) or \\\n",
    "                (label[c1] != label[c2]).item():\n",
    "            c1, c2 = random.choice(nodes), random.choice(nodes)\n",
    "\n",
    "        # Fill lists if we found valid choice:\n",
    "        nc_list1[i] = c1\n",
    "        nc_list2[i] = c2\n",
    "\n",
    "        # May be problems with inifinite loops with large batch sizes\n",
    "        #   - Should control upstream to avoid\n",
    "\n",
    "    for i in range(epochs):\n",
    "        # Compute similarities for all edges in the c_inds:\n",
    "        c_cos_sim = F.cosine_similarity(to_opt[edge_index.t()[c_inds][:, 0]], to_opt[edge_index.t()[c_inds][:, 1]])\n",
    "        nc_cos_sim = F.cosine_similarity(to_opt[nc_list1], to_opt[nc_list2])\n",
    "        diff_label_sim = F.cosine_similarity(to_opt[edge_index.t()[nc_inds][:, 0]], to_opt[edge_index.t()[nc_inds][:, 1]])\n",
    "        optimizer.zero_grad()\n",
    "        loss = -homophily_coef * c_cos_sim.mean() + (homophily_coef)*(nc_cos_sim.mean() + diff_label_sim.mean())\n",
    "        #loss = -homophily_coef * c_cos_sim.mean() + ((1 - homophily_coef) / 2) * (nc_cos_sim.mean() + diff_label_sim.mean())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Assign to appropriate copies:\n",
    "    xcopy = x.detach().clone()\n",
    "    xcopy[:,feature_mask] = to_opt.detach().clone()\n",
    "\n",
    "    return xcopy\n",
    "\n",
    "\n",
    "def evaluate(out, labels):\n",
    "    \"\"\"\n",
    "    Calculates the accuracy between the prediction and the ground truth.\n",
    "    :param out: predicted outputs of the explainer\n",
    "    :param labels: ground truth of the data\n",
    "    :returns: int accuracy\n",
    "    \"\"\"\n",
    "    preds = out.argmax(dim=1)\n",
    "    correct = preds == labels\n",
    "    acc = int(correct.sum()) / int(correct.size(0))\n",
    "    return acc\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "class _BaseExplainer:\n",
    "    \"\"\"\n",
    "    Base Class for Explainers\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "            model: nn.Module,\n",
    "            emb_layer_name: Optional[str] = None,\n",
    "            is_subgraphx: Optional[bool] = False\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model (torch.nn.Module): model on which to make predictions\n",
    "                The output of the model should be unnormalized class score.\n",
    "                For example, last layer = GCNConv or Linear.\n",
    "            emb_layer_name (str, optional): name of the embedding layer\n",
    "                If not specified, use the last but one layer by default.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.L = len([module for module in self.model.modules()\n",
    "                      if isinstance(module, MessagePassing)])\n",
    "        self.explain_graph = False  # Assume node-level explanation by default\n",
    "        self.subgraphx_flag = is_subgraphx\n",
    "        self.__set_embedding_layer(emb_layer_name)\n",
    "\n",
    "    def __set_embedding_layer(self, emb_layer_name: str = None):\n",
    "        \"\"\"\n",
    "        Set the embedding layer (by default is the last but one layer).\n",
    "        \"\"\"\n",
    "        if emb_layer_name:\n",
    "            try:\n",
    "                self.emb_layer = getattr(self.model, emb_layer_name)\n",
    "            except AttributeError:\n",
    "                raise ValueError(f'{emb_layer_name} does not exist in the model')\n",
    "        else:\n",
    "            self.emb_layer = list(self.model.modules())[-2]\n",
    "\n",
    "    def _get_embedding(self, x: torch.Tensor, edge_index: torch.Tensor,\n",
    "                       forward_kwargs: dict = {}):\n",
    "        \"\"\"\n",
    "        Get the embedding.\n",
    "        \"\"\"\n",
    "        emb = self._get_activation(self.emb_layer, x, edge_index, forward_kwargs)\n",
    "        return emb\n",
    "\n",
    "    def _set_masks(self, x: torch.Tensor, edge_index: torch.Tensor,\n",
    "                   edge_mask: torch.Tensor = None, explain_feature: bool = False,\n",
    "                   device = None):\n",
    "        \"\"\"\n",
    "        Initialize the edge (and feature) masks.\n",
    "        \"\"\"\n",
    "        (n, d), m = x.shape, edge_index.shape[1]\n",
    "\n",
    "        # Initialize edge_mask and feature_mask for learning\n",
    "        std = torch.nn.init.calculate_gain('relu') * np.sqrt(2.0 / (2 * n))\n",
    "        if edge_mask is None:\n",
    "            edge_mask = (torch.randn(m) * std).to(device)\n",
    "            self.edge_mask = torch.nn.Parameter(edge_mask)\n",
    "        else:\n",
    "            self.edge_mask = torch.nn.Parameter(edge_mask)\n",
    "        if explain_feature:\n",
    "            feature_mask = (torch.randn(d) * 0.1).to(device)\n",
    "            self.feature_mask = torch.nn.Parameter(feature_mask)\n",
    "\n",
    "        self.loop_mask = edge_index[0] != edge_index[1]\n",
    "\n",
    "        # Tell pytorch geometric to apply edge masks\n",
    "        for module in self.model.modules():\n",
    "            if isinstance(module, MessagePassing):\n",
    "                module.__explain__ = True\n",
    "                module.__edge_mask__ = self.edge_mask\n",
    "                module.__loop_mask__ = self.loop_mask\n",
    "\n",
    "    def _clear_masks(self):\n",
    "        for module in self.model.modules():\n",
    "            if isinstance(module, MessagePassing):\n",
    "                module.__explain__ = False\n",
    "                module.__edge_mask__ = None\n",
    "        self.edge_mask = None\n",
    "        self.feature_mask = None\n",
    "\n",
    "    def _flow(self):\n",
    "        for module in self.model.modules():\n",
    "            if isinstance(module, MessagePassing):\n",
    "                return module.flow\n",
    "        return 'source_to_target'\n",
    "\n",
    "    def _predict(self, x: torch.Tensor, edge_index: torch.Tensor,\n",
    "                 return_type: str = 'label', forward_kwargs: dict = {}):\n",
    "        \"\"\"\n",
    "        Get the model's prediction.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor, [n x d]): node features\n",
    "            edge_index (torch.Tensor, [2 x m]): edge index of the graph\n",
    "            return_type (str): one of ['label', 'prob', 'log_prob']\n",
    "            forward_kwargs (dict, optional): additional arguments to model.forward\n",
    "                beyond x and edge_index\n",
    "\n",
    "        Returns:\n",
    "            pred (torch.Tensor, [n x ...]): model prediction\n",
    "        \"\"\"\n",
    "        # Compute unnormalized class score\n",
    "        with torch.no_grad():\n",
    "            out = self.model.to(device)(x, edge_index, **forward_kwargs)\n",
    "            if return_type == 'label':\n",
    "                out = out.argmax(dim=-1)\n",
    "            elif return_type == 'prob':\n",
    "                out = F.softmax(out, dim=-1)\n",
    "            elif return_type == 'log_prob':\n",
    "                out = F.log_softmax(out, dim=-1)\n",
    "            else:\n",
    "                raise ValueError(\"return_type must be 'label', 'prob', or 'log_prob'\")\n",
    "\n",
    "            if self.explain_graph:\n",
    "                out = out.squeeze()\n",
    "\n",
    "            return out\n",
    "\n",
    "    def _prob_score_func_graph(self, target_class: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Get a function that computes the predicted probability that the input graphs\n",
    "        are classified as target classes.\n",
    "\n",
    "        Args:\n",
    "            target_class (int): the targeted class of the graph\n",
    "\n",
    "        Returns:\n",
    "            get_prob_score (callable): the probability score function\n",
    "        \"\"\"\n",
    "        def get_prob_score(x: torch.Tensor,\n",
    "                           edge_index: torch.Tensor,\n",
    "                           forward_kwargs: dict = {}):\n",
    "            prob = self._predict(x, edge_index, return_type='prob',\n",
    "                                 forward_kwargs=forward_kwargs)\n",
    "            score = prob[:, target_class]\n",
    "            return score\n",
    "\n",
    "        return get_prob_score\n",
    "\n",
    "    def _prob_score_func_node(self, node_idx: torch.Tensor, target_class: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Get a function that computes the predicted probabilities that k specified nodes\n",
    "        in `torch_geometric.data.Batch` (disconnected union of the input graphs)\n",
    "        are classified as target classes.\n",
    "\n",
    "        Args:\n",
    "            node_idx (torch.Tensor, [k]): the indices of the k nodes interested\n",
    "            target_class (torch.Tensor, [k]): the targeted classes of the k nodes\n",
    "\n",
    "        Returns:\n",
    "            get_prob_score (callable): the probability score function\n",
    "        \"\"\"\n",
    "        if self.subgraphx_flag:\n",
    "            def get_prob_score(x: torch.Tensor,\n",
    "                            edge_index: torch.Tensor,\n",
    "                            forward_kwargs: dict = {}):\n",
    "                prob = self._predict(x, edge_index, return_type='prob',\n",
    "                                    forward_kwargs=forward_kwargs)\n",
    "                score = prob[node_idx, target_class]\n",
    "                return score\n",
    "        else:\n",
    "            def get_prob_score(x: torch.Tensor,\n",
    "                            edge_index: torch.Tensor,\n",
    "                            forward_kwargs: dict = {}):\n",
    "                prob = self._predict(x, edge_index, return_type='prob',\n",
    "                                    forward_kwargs=forward_kwargs)\n",
    "                score = prob[:, node_idx, target_class]\n",
    "                return score\n",
    "\n",
    "        return get_prob_score\n",
    "\n",
    "    def _get_activation(self, layer: nn.Module, x: torch.Tensor,\n",
    "                        edge_index: torch.Tensor, forward_kwargs: dict = {}):\n",
    "        \"\"\"\n",
    "        Get the activation of the layer.\n",
    "        \"\"\"\n",
    "        activation = {}\n",
    "        def get_activation():\n",
    "            def hook(model, inp, out):\n",
    "                activation['layer'] = out.detach()\n",
    "            return hook\n",
    "\n",
    "        layer.register_forward_hook(get_activation())\n",
    "\n",
    "        with torch.no_grad():\n",
    "            _ = self.model(x, edge_index, **forward_kwargs)\n",
    "\n",
    "        return activation['layer']\n",
    "\n",
    "    def _get_k_hop_subgraph(self, node_idx: int, x: torch.Tensor,\n",
    "                            edge_index: torch.Tensor, num_hops: int = None, **kwargs):\n",
    "        \"\"\"\n",
    "        Extract the subgraph of target node\n",
    "\n",
    "        Args:\n",
    "            node_idx (int): the node index\n",
    "            x (torch.Tensor, [n x d]): node feature matrix with shape\n",
    "            edge_index (torch.Tensor, [2 x m]): edge index\n",
    "            kwargs (dict): additional parameters of the graph\n",
    "\n",
    "        Returns:\n",
    "        \"\"\"\n",
    "        # TODO: use NamedTuple\n",
    "        khop_info = subset, sub_edge_index, mapping, _ = \\\n",
    "            k_hop_subgraph(node_idx, num_hops, edge_index,\n",
    "                           relabel_nodes=True, num_nodes=x.shape[0])\n",
    "        return khop_info\n",
    "\n",
    "    def get_explanation_node(self, node_idx: int,\n",
    "                             x: torch.Tensor,\n",
    "                             edge_index: torch.Tensor,\n",
    "                             label: torch.Tensor = None,\n",
    "                             num_hops: int = None,\n",
    "                             forward_kwargs: dict = {}):\n",
    "        \"\"\"\n",
    "        Explain a node prediction.\n",
    "\n",
    "        Args:\n",
    "            node_idx (int): index of the node to be explained\n",
    "            x (torch.Tensor, [n x d]): node features\n",
    "            edge_index (torch.Tensor, [2 x m]): edge index of the graph\n",
    "            label (torch.Tensor, optional, [n x ...]): labels to explain\n",
    "                If not provided, we use the output of the model.\n",
    "            num_hops (int, optional): number of hops to consider\n",
    "                If not provided, we use the number of graph layers of the GNN.\n",
    "            forward_kwargs (dict, optional): additional arguments to model.forward\n",
    "                beyond x and edge_index\n",
    "\n",
    "        Returns:\n",
    "            exp (dict):\n",
    "                exp['feature_imp'] (torch.Tensor, [d]): feature mask explanation\n",
    "                exp['edge_imp'] (torch.Tensor, [m]): k-hop edge importance\n",
    "                exp['node_imp'] (torch.Tensor, [m]): k-hop node importance\n",
    "            khop_info (4-tuple of torch.Tensor):\n",
    "                0. the nodes involved in the subgraph\n",
    "                1. the filtered `edge_index`\n",
    "                2. the mapping from node indices in `node_idx` to their new location\n",
    "                3. the `edge_index` mask indicating which edges were preserved\n",
    "        \"\"\"\n",
    "        # If labels are needed\n",
    "        label = self._predict(x, edge_index, return_type='label') if label is None else label\n",
    "        # If probabilities / log probabilities are needed\n",
    "        prob = self._predict(x, edge_index, return_type='prob')\n",
    "        log_prob = self._predict(x, edge_index, return_type='log_prob')\n",
    "\n",
    "        num_hops = self.L if num_hops is None else num_hops\n",
    "\n",
    "        khop_info = subset, sub_edge_index, mapping, _ = \\\n",
    "            k_hop_subgraph(node_idx, num_hops, edge_index,\n",
    "                           relabel_nodes=True, num_nodes=x.shape[0])\n",
    "        sub_x = x[subset]\n",
    "\n",
    "        exp = {'feature_imp': None, 'edge_imp': None}\n",
    "\n",
    "        # Compute exp\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        return exp, khop_info\n",
    "\n",
    "    def get_explanation_graph(self, edge_index: torch.Tensor,\n",
    "                              x: torch.Tensor, label: torch.Tensor,\n",
    "                              forward_kwargs: dict = {}):\n",
    "        \"\"\"\n",
    "        Explain a whole-graph prediction.\n",
    "\n",
    "        Args:\n",
    "            edge_index (torch.Tensor, [2 x m]): edge index of the graph\n",
    "            x (torch.Tensor, [n x d]): node features\n",
    "            label (torch.Tensor, [n x ...]): labels to explain\n",
    "            forward_kwargs (dict, optional): additional arguments to model.forward\n",
    "                beyond x and edge_index\n",
    "\n",
    "        Returns:\n",
    "            exp (dict):\n",
    "                exp['feature_imp'] (torch.Tensor, [d]): feature mask explanation\n",
    "                exp['edge_imp'] (torch.Tensor, [m]): k-hop edge importance\n",
    "                exp['node_imp'] (torch.Tensor, [m]): k-hop node importance\n",
    "        \"\"\"\n",
    "        exp = {'feature_imp': None, 'edge_imp': None}\n",
    "\n",
    "        # Compute exp\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_explanation_link(self):\n",
    "        \"\"\"\n",
    "        Explain a link prediction.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "EPS = 1e-15\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from functools import partial\n",
    "import torch.nn.functional as F\n",
    "from typing import Callable, Optional, Tuple\n",
    "from torch_geometric.utils import k_hop_subgraph\n",
    "from torch_geometric.utils.num_nodes import maybe_num_nodes\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "\n",
    "import math\n",
    "import copy\n",
    "import torch\n",
    "import networkx as nx\n",
    "from functools import partial\n",
    "from collections import Counter\n",
    "from torch_geometric.data import Batch, Data\n",
    "from torch_geometric.utils import to_networkx\n",
    "from typing import Callable\n",
    "from torch_geometric.utils.num_nodes import maybe_num_nodes\n",
    "import copy\n",
    "import torch\n",
    "import numpy as np\n",
    "from typing import Callable, Union\n",
    "from scipy.special import comb\n",
    "from itertools import combinations\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.utils import to_networkx\n",
    "from torch_geometric.data import Data, Batch, Dataset, DataLoader\n",
    "\n",
    "'''\n",
    "Code adapted from Dive into Graphs (DIG)\n",
    "Code: https://github.com/divelab/DIG\n",
    "'''\n",
    "\n",
    "empty_tuple = tuple()\n",
    "\n",
    "class MarginalSubgraphDataset(Dataset):\n",
    "    \"\"\" Collect pair-wise graph data to calculate marginal contribution. \"\"\"\n",
    "    def __init__(self, data, exclude_mask, include_mask, subgraph_build_func):\n",
    "        self.num_nodes = data.num_nodes\n",
    "        self.X = data.x\n",
    "        self.edge_index = data.edge_index\n",
    "        self.device = self.X.device\n",
    "\n",
    "        self.label = data.y\n",
    "        self.exclude_mask = torch.tensor(exclude_mask).type(torch.float32).to(self.device)\n",
    "        self.include_mask = torch.tensor(include_mask).type(torch.float32).to(self.device)\n",
    "        self.subgraph_build_func = subgraph_build_func\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.exclude_mask.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        exclude_graph_X, exclude_graph_edge_index = self.subgraph_build_func(self.X, self.edge_index, self.exclude_mask[idx])\n",
    "        include_graph_X, include_graph_edge_index = self.subgraph_build_func(self.X, self.edge_index, self.include_mask[idx])\n",
    "        exclude_data = Data(x=exclude_graph_X, edge_index=exclude_graph_edge_index)\n",
    "        include_data = Data(x=include_graph_X, edge_index=include_graph_edge_index)\n",
    "        return exclude_data, include_data\n",
    "    def len(self):\n",
    "        return self.exclude_mask.shape[0]\n",
    "    \n",
    "    def get(self, idx):\n",
    "        exclude_graph_X, exclude_graph_edge_index = self.subgraph_build_func(self.X, self.edge_index, self.exclude_mask[idx])\n",
    "        include_graph_X, include_graph_edge_index = self.subgraph_build_func(self.X, self.edge_index, self.include_mask[idx])\n",
    "        exclude_data = Data(x=exclude_graph_X, edge_index=exclude_graph_edge_index)\n",
    "        include_data = Data(x=include_graph_X, edge_index=include_graph_edge_index)\n",
    "        return exclude_data, include_data\n",
    "\n",
    "def GnnNets_GC2value_func(gnnNets, target_class, forward_kwargs = {}):\n",
    "    def value_func(batch):\n",
    "        with torch.no_grad():\n",
    "            #logits = gnnNets(data=batch)\n",
    "            logits = gnnNets(batch.x, batch.edge_index, **forward_kwargs)\n",
    "            print(forward_kwargs)\n",
    "            print(batch.batch)\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            print(probs, probs.shape)\n",
    "            score = probs[:, target_class]\n",
    "        return score\n",
    "    return value_func\n",
    "\n",
    "\n",
    "def GnnNets_NC2value_func(gnnNets_NC, node_idx: Union[int, torch.Tensor], target_class: torch.Tensor):\n",
    "    def value_func(data):\n",
    "        with torch.no_grad():\n",
    "            #logits = gnnNets_NC(data=data)\n",
    "            logits = gnnNets_NC(data.x, data.edge_index)\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # select the corresponding node prob through the node idx on all the sampling graphs\n",
    "            batch_size = data.batch.max() + 1\n",
    "            print(batch_size)\n",
    "            probs = probs.reshape(batch_size, -1, probs.shape[-1])\n",
    "            score = probs[:, node_idx, target_class]\n",
    "            return score\n",
    "    return value_func\n",
    "\n",
    "\n",
    "def get_graph_build_func(build_method):\n",
    "    if build_method.lower() == 'zero_filling':\n",
    "        return graph_build_zero_filling\n",
    "    elif build_method.lower() == 'split':\n",
    "        return graph_build_split\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "def marginal_contribution(data: Data, exclude_mask: np.ndarray, include_mask: np.ndarray,\n",
    "                          value_func, subgraph_build_func):\n",
    "    \"\"\" Calculate the marginal value for each pair. Here exclude_mask and include_mask are node mask. \"\"\"\n",
    "    marginal_subgraph_dataset = MarginalSubgraphDataset(data, exclude_mask, include_mask, subgraph_build_func)\n",
    "    dataloader = DataLoader(marginal_subgraph_dataset, batch_size=256, shuffle=False, pin_memory=False, num_workers=0)\n",
    "\n",
    "    marginal_contribution_list = []\n",
    "\n",
    "    for exclude_data, include_data in dataloader:\n",
    "        exclude_values = value_func(exclude_data)\n",
    "        include_values = value_func(include_data)\n",
    "        margin_values = include_values - exclude_values\n",
    "        marginal_contribution_list.append(margin_values)\n",
    "\n",
    "    marginal_contributions = torch.cat(marginal_contribution_list, dim=0)\n",
    "    return marginal_contributions\n",
    "\n",
    "\n",
    "def graph_build_zero_filling(X, edge_index, node_mask: torch.Tensor):\n",
    "    \"\"\" subgraph building through masking the unselected nodes with zero features \"\"\"\n",
    "    ret_X = X * node_mask.unsqueeze(1)\n",
    "    return ret_X, edge_index\n",
    "\n",
    "\n",
    "def graph_build_split(X, edge_index, node_mask: torch.Tensor):\n",
    "    \"\"\" subgraph building through spliting the selected nodes from the original graph \"\"\"\n",
    "    row, col = edge_index\n",
    "    edge_mask = (node_mask[row] == 1) & (node_mask[col] == 1)\n",
    "    ret_edge_index = edge_index[:, edge_mask]\n",
    "    return X, ret_edge_index\n",
    "\n",
    "\n",
    "def l_shapley(coalition: list, data: Data, local_raduis: int,\n",
    "              value_func: Callable, subgraph_building_method='zero_filling'):\n",
    "    \"\"\" shapley value where players are local neighbor nodes \"\"\"\n",
    "    graph = to_networkx(data)\n",
    "    num_nodes = graph.number_of_nodes()\n",
    "    subgraph_build_func = get_graph_build_func(subgraph_building_method)\n",
    "\n",
    "    local_region = copy.copy(coalition)\n",
    "    for k in range(local_raduis - 1):\n",
    "        k_neiborhoood = []\n",
    "        for node in local_region:\n",
    "            k_neiborhoood += list(graph.neighbors(node))\n",
    "        local_region += k_neiborhoood\n",
    "        local_region = list(set(local_region))\n",
    "\n",
    "    set_exclude_masks = []\n",
    "    set_include_masks = []\n",
    "    nodes_around = [node for node in local_region if node not in coalition]\n",
    "    num_nodes_around = len(nodes_around)\n",
    "\n",
    "    for subset_len in range(0, num_nodes_around + 1):\n",
    "        node_exclude_subsets = combinations(nodes_around, subset_len)\n",
    "        for node_exclude_subset in node_exclude_subsets:\n",
    "            set_exclude_mask = np.ones(num_nodes)\n",
    "            set_exclude_mask[local_region] = 0.0\n",
    "            if node_exclude_subset:\n",
    "                set_exclude_mask[list(node_exclude_subset)] = 1.0\n",
    "            set_include_mask = set_exclude_mask.copy()\n",
    "            set_include_mask[coalition] = 1.0\n",
    "\n",
    "            set_exclude_masks.append(set_exclude_mask)\n",
    "            set_include_masks.append(set_include_mask)\n",
    "\n",
    "    exclude_mask = np.stack(set_exclude_masks, axis=0)\n",
    "    include_mask = np.stack(set_include_masks, axis=0)\n",
    "    num_players = len(nodes_around) + 1\n",
    "    num_player_in_set = num_players - 1 + len(coalition) - (1 - exclude_mask).sum(axis=1)\n",
    "    p = num_players\n",
    "    S = num_player_in_set\n",
    "    coeffs = torch.tensor(1.0 / comb(p, S) / (p - S + 1e-6))\n",
    "\n",
    "    marginal_contributions = \\\n",
    "        marginal_contribution(data, exclude_mask, include_mask, value_func, subgraph_build_func)\n",
    "\n",
    "    l_shapley_value = (marginal_contributions.squeeze().cpu() * coeffs).sum().item()\n",
    "    return l_shapley_value\n",
    "\n",
    "\n",
    "def mc_shapley(coalition: list, data: Data,\n",
    "               value_func: Callable, subgraph_building_method='zero_filling',\n",
    "               sample_num=1000) -> float:\n",
    "    \"\"\" monte carlo sampling approximation of the shapley value \"\"\"\n",
    "    subset_build_func = get_graph_build_func(subgraph_building_method)\n",
    "\n",
    "    num_nodes = data.num_nodes\n",
    "    node_indices = np.arange(num_nodes)\n",
    "    coalition_placeholder = num_nodes\n",
    "    set_exclude_masks = []\n",
    "    set_include_masks = []\n",
    "\n",
    "    for example_idx in range(sample_num):\n",
    "        subset_nodes_from = [node for node in node_indices if node not in coalition]\n",
    "        random_nodes_permutation = np.array(subset_nodes_from + [coalition_placeholder])\n",
    "        random_nodes_permutation = np.random.permutation(random_nodes_permutation)\n",
    "        split_idx = np.where(random_nodes_permutation == coalition_placeholder)[0][0]\n",
    "        selected_nodes = random_nodes_permutation[:split_idx]\n",
    "        set_exclude_mask = np.zeros(num_nodes)\n",
    "        set_exclude_mask[selected_nodes] = 1.0\n",
    "        set_include_mask = set_exclude_mask.copy()\n",
    "        set_include_mask[coalition] = 1.0\n",
    "\n",
    "        set_exclude_masks.append(set_exclude_mask)\n",
    "        set_include_masks.append(set_include_mask)\n",
    "\n",
    "    exclude_mask = np.stack(set_exclude_masks, axis=0)\n",
    "    include_mask = np.stack(set_include_masks, axis=0)\n",
    "    marginal_contributions = marginal_contribution(data, exclude_mask, include_mask, value_func, subset_build_func)\n",
    "    mc_shapley_value = marginal_contributions.mean().item()\n",
    "\n",
    "    return mc_shapley_value\n",
    "\n",
    "\n",
    "def mc_l_shapley(coalition: list, data: Data, local_raduis: int,\n",
    "                 value_func: Callable, subgraph_building_method='zero_filling',\n",
    "                 sample_num=1000) -> float:\n",
    "    \"\"\" monte carlo sampling approximation of the l_shapley value \"\"\"\n",
    "    graph = to_networkx(data)\n",
    "    num_nodes = graph.number_of_nodes()\n",
    "    subgraph_build_func = get_graph_build_func(subgraph_building_method)\n",
    "\n",
    "    local_region = copy.copy(coalition)\n",
    "    for k in range(local_raduis - 1):\n",
    "        k_neiborhoood = []\n",
    "        for node in local_region:\n",
    "            k_neiborhoood += list(graph.neighbors(node))\n",
    "        local_region += k_neiborhoood\n",
    "        local_region = list(set(local_region))\n",
    "\n",
    "    coalition_placeholder = num_nodes\n",
    "    set_exclude_masks = []\n",
    "    set_include_masks = []\n",
    "    for example_idx in range(sample_num):\n",
    "        subset_nodes_from = [node for node in local_region if node not in coalition]\n",
    "        random_nodes_permutation = np.array(subset_nodes_from + [coalition_placeholder])\n",
    "        random_nodes_permutation = np.random.permutation(random_nodes_permutation)\n",
    "        split_idx = np.where(random_nodes_permutation == coalition_placeholder)[0][0]\n",
    "        selected_nodes = random_nodes_permutation[:split_idx]\n",
    "        set_exclude_mask = np.ones(num_nodes)\n",
    "        set_exclude_mask[local_region] = 0.0\n",
    "        set_exclude_mask[selected_nodes] = 1.0\n",
    "        set_include_mask = set_exclude_mask.copy()\n",
    "        set_include_mask[coalition] = 1.0\n",
    "\n",
    "        set_exclude_masks.append(set_exclude_mask)\n",
    "        set_include_masks.append(set_include_mask)\n",
    "\n",
    "    exclude_mask = np.stack(set_exclude_masks, axis=0)\n",
    "    include_mask = np.stack(set_include_masks, axis=0)\n",
    "    marginal_contributions = \\\n",
    "        marginal_contribution(data, exclude_mask, include_mask, value_func, subgraph_build_func)\n",
    "\n",
    "    mc_l_shapley_value = (marginal_contributions).mean().item()\n",
    "    return mc_l_shapley_value\n",
    "\n",
    "\n",
    "def gnn_score(coalition: list, data: Data, value_func: Callable,\n",
    "              subgraph_building_method='zero_filling') -> torch.Tensor:\n",
    "    \"\"\" the value of subgraph with selected nodes \"\"\"\n",
    "    num_nodes = data.num_nodes\n",
    "    subgraph_build_func = get_graph_build_func(subgraph_building_method)\n",
    "    mask = torch.zeros(num_nodes).type(torch.float32).to(data.x.device)\n",
    "    mask[coalition] = 1.0\n",
    "    ret_x, ret_edge_index = subgraph_build_func(data.x, data.edge_index, mask)\n",
    "    mask_data = Data(x=ret_x, edge_index=ret_edge_index)\n",
    "    mask_data = Batch.from_data_list([mask_data])\n",
    "    score = value_func(mask_data)\n",
    "    # get the score of predicted class for graph or specific node idx\n",
    "    return score.item()\n",
    "\n",
    "\n",
    "def NC_mc_l_shapley(coalition: list, data: Data, local_raduis: int,\n",
    "                    value_func: Callable, node_idx: int=-1, subgraph_building_method='zero_filling', sample_num=1000) -> float:\n",
    "    \"\"\" monte carlo approximation of l_shapley where the target node is kept in both subgraph \"\"\"\n",
    "    graph = to_networkx(data)\n",
    "    num_nodes = graph.number_of_nodes()\n",
    "    subgraph_build_func = get_graph_build_func(subgraph_building_method)\n",
    "\n",
    "    local_region = copy.copy(coalition)\n",
    "    for k in range(local_raduis - 1):\n",
    "        k_neiborhoood = []\n",
    "        for node in local_region:\n",
    "            k_neiborhoood += list(graph.neighbors(node))\n",
    "        local_region += k_neiborhoood\n",
    "        local_region = list(set(local_region))\n",
    "\n",
    "    coalition_placeholder = num_nodes\n",
    "    set_exclude_masks = []\n",
    "    set_include_masks = []\n",
    "    for example_idx in range(sample_num):\n",
    "        subset_nodes_from = [node for node in local_region if node not in coalition]\n",
    "        random_nodes_permutation = np.array(subset_nodes_from + [coalition_placeholder])\n",
    "        random_nodes_permutation = np.random.permutation(random_nodes_permutation)\n",
    "        split_idx = np.where(random_nodes_permutation == coalition_placeholder)[0][0]\n",
    "        selected_nodes = random_nodes_permutation[:split_idx]\n",
    "        set_exclude_mask = np.ones(num_nodes)\n",
    "        set_exclude_mask[local_region] = 0.0\n",
    "        set_exclude_mask[selected_nodes] = 1.0\n",
    "        if node_idx != -1:\n",
    "            set_exclude_mask[node_idx] = 1.0\n",
    "        set_include_mask = set_exclude_mask.copy()\n",
    "        set_include_mask[coalition] = 1.0  # include the node_idx\n",
    "\n",
    "        set_exclude_masks.append(set_exclude_mask)\n",
    "        set_include_masks.append(set_include_mask)\n",
    "\n",
    "    exclude_mask = np.stack(set_exclude_masks, axis=0)\n",
    "    include_mask = np.stack(set_include_masks, axis=0)\n",
    "    marginal_contributions = \\\n",
    "        marginal_contribution(data, exclude_mask, include_mask, value_func, subgraph_build_func)\n",
    "\n",
    "    mc_l_shapley_value = (marginal_contributions).mean().item()\n",
    "    return mc_l_shapley_value\n",
    "\n",
    "'''\n",
    "Code adapted from Dive into Graphs (DIG)\n",
    "Code: https://github.com/divelab/DIG\n",
    "'''\n",
    "\n",
    "def find_closest_node_result(results, max_nodes):\n",
    "    \"\"\" return the highest reward tree_node with its subgraph is smaller than max_nodes \"\"\"\n",
    "\n",
    "    results = sorted(results, key=lambda x: len(x.coalition))\n",
    "\n",
    "    result_node = results[0]\n",
    "    for result_idx in range(len(results)):\n",
    "        x = results[result_idx]\n",
    "        if len(x.coalition) <= max_nodes and x.P > result_node.P:\n",
    "            result_node = x\n",
    "    return result_node\n",
    "\n",
    "\n",
    "def reward_func(reward_method, value_func, node_idx=None,\n",
    "                local_radius=4, sample_num=100,\n",
    "                subgraph_building_method='zero_filling'):\n",
    "    if reward_method.lower() == 'gnn_score':\n",
    "        return partial(gnn_score,\n",
    "                       value_func=value_func,\n",
    "                       subgraph_building_method=subgraph_building_method)\n",
    "\n",
    "    elif reward_method.lower() == 'mc_shapley':\n",
    "        return partial(mc_shapley,\n",
    "                       value_func=value_func,\n",
    "                       subgraph_building_method=subgraph_building_method,\n",
    "                       sample_num=sample_num)\n",
    "\n",
    "    elif reward_method.lower() == 'l_shapley':\n",
    "        return partial(l_shapley,\n",
    "                       local_raduis=local_radius,\n",
    "                       value_func=value_func,\n",
    "                       subgraph_building_method=subgraph_building_method)\n",
    "\n",
    "    elif reward_method.lower() == 'mc_l_shapley':\n",
    "        return partial(mc_l_shapley,\n",
    "                       local_raduis=local_radius,\n",
    "                       value_func=value_func,\n",
    "                       subgraph_building_method=subgraph_building_method,\n",
    "                       sample_num=sample_num)\n",
    "\n",
    "    elif reward_method.lower() == 'nc_mc_l_shapley':\n",
    "        assert node_idx is not None, \" Wrong node idx input \"\n",
    "        return partial(NC_mc_l_shapley,\n",
    "                       node_idx=node_idx,\n",
    "                       local_raduis=local_radius,\n",
    "                       value_func=value_func,\n",
    "                       subgraph_building_method=subgraph_building_method,\n",
    "                       sample_num=sample_num)\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "def k_hop_subgraph_with_default_whole_graph(\n",
    "        edge_index, node_idx=None, num_hops=3, relabel_nodes=False,\n",
    "        num_nodes=None, flow='source_to_target'):\n",
    "    r\"\"\"Computes the :math:`k`-hop subgraph of :obj:`edge_index` around node\n",
    "    :attr:`node_idx`.\n",
    "    It returns (1) the nodes involved in the subgraph, (2) the filtered\n",
    "    :obj:`edge_index` connectivity, (3) the mapping from node indices in\n",
    "    :obj:`node_idx` to their new location, and (4) the edge mask indicating\n",
    "    which edges were preserved.\n",
    "    Args:\n",
    "        node_idx (int, list, tuple or :obj:`torch.Tensor`): The central\n",
    "            node(s).\n",
    "        num_hops: (int): The number of hops :math:`k`.\n",
    "        edge_index (LongTensor): The edge indices.\n",
    "        relabel_nodes (bool, optional): If set to :obj:`True`, the resulting\n",
    "            :obj:`edge_index` will be relabeled to hold consecutive indices\n",
    "            starting from zero. (default: :obj:`False`)\n",
    "        num_nodes (int, optional): The number of nodes, *i.e.*\n",
    "            :obj:`max_val + 1` of :attr:`edge_index`. (default: :obj:`None`)\n",
    "        flow (string, optional): The flow direction of :math:`k`-hop\n",
    "            aggregation (:obj:`\"source_to_target\"` or\n",
    "            :obj:`\"target_to_source\"`). (default: :obj:`\"source_to_target\"`)\n",
    "    :rtype: (:class:`LongTensor`, :class:`LongTensor`, :class:`LongTensor`,\n",
    "             :class:`BoolTensor`)\n",
    "    \"\"\"\n",
    "\n",
    "    num_nodes = maybe_num_nodes(edge_index, num_nodes)\n",
    "\n",
    "    assert flow in ['source_to_target', 'target_to_source']\n",
    "    if flow == 'target_to_source':\n",
    "        row, col = edge_index\n",
    "    else:\n",
    "        col, row = edge_index  # edge_index 0 to 1, col: source, row: target\n",
    "\n",
    "    node_mask = row.new_empty(num_nodes, dtype=torch.bool)\n",
    "    edge_mask = row.new_empty(row.size(0), dtype=torch.bool)\n",
    "\n",
    "    inv = None\n",
    "\n",
    "    if node_idx is None:\n",
    "        subsets = torch.tensor([0])\n",
    "        cur_subsets = subsets\n",
    "        while 1:\n",
    "            node_mask.fill_(False)\n",
    "            node_mask[subsets] = True\n",
    "            torch.index_select(node_mask, 0, row, out=edge_mask)\n",
    "            subsets = torch.cat([subsets, col[edge_mask]]).unique()\n",
    "            if not cur_subsets.equal(subsets):\n",
    "                cur_subsets = subsets\n",
    "            else:\n",
    "                subset = subsets\n",
    "                break\n",
    "    else:\n",
    "        if isinstance(node_idx, (int, list, tuple)):\n",
    "            node_idx = torch.tensor([node_idx], device=row.device, dtype=torch.int64).flatten()\n",
    "        elif isinstance(node_idx, torch.Tensor) and len(node_idx.shape) == 0:\n",
    "            node_idx = torch.tensor([node_idx], device=row.device)\n",
    "        else:\n",
    "            node_idx = node_idx.to(row.device)\n",
    "\n",
    "        subsets = [node_idx]\n",
    "        for _ in range(num_hops):\n",
    "            node_mask.fill_(False)\n",
    "            node_mask[subsets[-1]] = True\n",
    "            torch.index_select(node_mask, 0, row, out=edge_mask)\n",
    "            subsets.append(col[edge_mask])\n",
    "        subset, inv = torch.cat(subsets).unique(return_inverse=True)\n",
    "        inv = inv[:node_idx.numel()]\n",
    "\n",
    "    node_mask.fill_(False)\n",
    "    node_mask[subset] = True\n",
    "    edge_mask = node_mask[row] & node_mask[col]\n",
    "\n",
    "    edge_index = edge_index[:, edge_mask]\n",
    "\n",
    "    if relabel_nodes:\n",
    "        node_idx = row.new_full((num_nodes,), -1)\n",
    "        node_idx[subset] = torch.arange(subset.size(0), device=row.device)\n",
    "        edge_index = node_idx[edge_index]\n",
    "\n",
    "    return subset, edge_index, inv, edge_mask  # subset: key new node idx; value original node idx\n",
    "\n",
    "\n",
    "def compute_scores(score_func, children):\n",
    "    results = []\n",
    "    for child in children:\n",
    "        if child.P == 0:\n",
    "            score = score_func(child.coalition, child.data)\n",
    "        else:\n",
    "            score = child.P\n",
    "        results.append(score)\n",
    "    return results\n",
    "\n",
    "class MCTSNode(object):\n",
    "\n",
    "    def __init__(self, coalition: list, data: Data, ori_graph: nx.Graph,\n",
    "                 c_puct: float = 10.0, W: float = 0, N: int = 0, P: float = 0,\n",
    "                 mapping = None):\n",
    "        self.data = data\n",
    "        self.coalition = coalition # Coalition of possible subsets of players\n",
    "        self.ori_graph = ori_graph # Original input graph\n",
    "        self.c_puct = c_puct # Hyperparameter in search algorithm\n",
    "        self.children = [] # Children within MCTS tree\n",
    "        self.W = W  # sum of node value\n",
    "        self.N = N  # times of arrival\n",
    "        self.P = P  # property score (reward)\n",
    "\n",
    "        self.mapping = mapping # ADDED from OWEN\n",
    "\n",
    "    def Q(self): # Average of W\n",
    "        return self.W / self.N if self.N > 0 else 0\n",
    "\n",
    "    def U(self, n): # Action selection criteria for MCTS\n",
    "        return self.c_puct * self.P * math.sqrt(n) / (1 + self.N)\n",
    "\n",
    "\n",
    "class MCTS(object):\n",
    "    r\"\"\"\n",
    "    Monte Carlo Tree Search Method\n",
    "\n",
    "    Args:\n",
    "        X (:obj:`torch.Tensor`): Input node features\n",
    "        edge_index (:obj:`torch.Tensor`): The edge indices.\n",
    "        num_hops (:obj:`int`): The number of hops :math:`k`.\n",
    "        n_rollout (:obj:`int`): The number of sequence to build the monte carlo tree.\n",
    "        min_atoms (:obj:`int`): The number of atoms for the subgraph in the monte carlo tree leaf node.\n",
    "        c_puct (:obj:`float`): The hyper-parameter to encourage exploration while searching.\n",
    "        expand_atoms (:obj:`int`): The number of children to expand.\n",
    "        high2low (:obj:`bool`): Whether to expand children tree node from high degree nodes to low degree nodes.\n",
    "        node_idx (:obj:`int`): The target node index to extract the neighborhood.\n",
    "        score_func (:obj:`Callable`): The reward function for tree node, such as mc_shapely and mc_l_shapely.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, X: torch.Tensor, edge_index: torch.Tensor, num_hops: int,\n",
    "                 n_rollout: int = 10, min_atoms: int = 3, c_puct: float = 10.0,\n",
    "                 expand_atoms: int = 14, high2low: bool = False,\n",
    "                 node_idx: int = None, score_func: Callable = None):\n",
    "\n",
    "        self.X = X\n",
    "        self.edge_index = edge_index\n",
    "        self.num_hops = num_hops\n",
    "        self.data = Data(x=self.X, edge_index=self.edge_index)\n",
    "        self.graph = to_networkx(self.data, to_undirected=True) # NETWORKX VERSION OF GRAPH\n",
    "        self.data = Batch.from_data_list([self.data])\n",
    "        self.num_nodes = self.graph.number_of_nodes()\n",
    "        self.score_func = score_func\n",
    "        self.n_rollout = n_rollout\n",
    "        self.min_atoms = min_atoms\n",
    "        self.c_puct = c_puct\n",
    "        self.expand_atoms = expand_atoms\n",
    "        self.high2low = high2low\n",
    "\n",
    "        inv_mapping = None\n",
    "\n",
    "        # extract the sub-graph and change the node indices.\n",
    "        if node_idx is not None:\n",
    "            self.ori_node_idx = node_idx\n",
    "            self.ori_graph = copy.copy(self.graph)\n",
    "            x, edge_index, subset, edge_mask, kwargs = \\\n",
    "                self.__subgraph__(node_idx, self.X, self.edge_index, self.num_hops)\n",
    "            self.data = Batch.from_data_list([Data(x=x, edge_index=edge_index)])\n",
    "            self.graph = self.ori_graph.subgraph(subset.tolist())\n",
    "            mapping = {int(v): k for k, v in enumerate(subset)}\n",
    "            inv_mapping = {v:k for k, v in mapping.items()}\n",
    "            self.graph = nx.relabel_nodes(self.graph, mapping)\n",
    "            self.node_idx = torch.where(subset == self.ori_node_idx)[0]\n",
    "            self.num_nodes = self.graph.number_of_nodes()\n",
    "            self.subset = subset\n",
    "\n",
    "        self.root_coalition = sorted([node for node in range(self.num_nodes)])\n",
    "        self.MCTSNodeClass = partial(MCTSNode, data=self.data, ori_graph=self.graph, c_puct=self.c_puct, mapping = inv_mapping)\n",
    "        self.root = self.MCTSNodeClass(self.root_coalition) # Root of tree\n",
    "        self.state_map = {str(self.root.coalition): self.root}\n",
    "\n",
    "    def set_score_func(self, score_func):\n",
    "        self.score_func = score_func\n",
    "\n",
    "    @staticmethod\n",
    "    def __subgraph__(node_idx, x, edge_index, num_hops, **kwargs):\n",
    "        num_nodes, num_edges = x.size(0), edge_index.size(1)\n",
    "        subset, edge_index, _, edge_mask = k_hop_subgraph_with_default_whole_graph(\n",
    "            edge_index, node_idx, num_hops, relabel_nodes=True, num_nodes=num_nodes)\n",
    "\n",
    "        x = x[subset]\n",
    "        for key, item in kwargs.items():\n",
    "            if torch.is_tensor(item) and item.size(0) == num_nodes:\n",
    "                item = item[subset]\n",
    "            elif torch.is_tensor(item) and item.size(0) == num_edges:\n",
    "                item = item[edge_mask]\n",
    "            kwargs[key] = item\n",
    "\n",
    "        return x, edge_index, subset, edge_mask, kwargs\n",
    "\n",
    "    def mcts_rollout(self, tree_node):\n",
    "        cur_graph_coalition = tree_node.coalition\n",
    "        if len(cur_graph_coalition) <= self.min_atoms:\n",
    "            return tree_node.P\n",
    "\n",
    "        # Expand if this node has never been visited\n",
    "        if len(tree_node.children) == 0:\n",
    "            node_degree_list = list(self.graph.subgraph(cur_graph_coalition).degree)\n",
    "            node_degree_list = sorted(node_degree_list, key=lambda x: x[1], reverse=self.high2low)\n",
    "            all_nodes = [x[0] for x in node_degree_list]\n",
    "\n",
    "            if len(all_nodes) < self.expand_atoms:\n",
    "                expand_nodes = all_nodes\n",
    "            else:\n",
    "                expand_nodes = all_nodes[:self.expand_atoms]\n",
    "\n",
    "            for each_node in expand_nodes:\n",
    "                # for each node, pruning it and get the remaining sub-graph\n",
    "                # here we check the resulting sub-graphs and only keep the largest one\n",
    "                subgraph_coalition = [node for node in all_nodes if node != each_node]\n",
    "\n",
    "                subgraphs = [self.graph.subgraph(c)\n",
    "                             for c in nx.connected_components(self.graph.subgraph(subgraph_coalition))]\n",
    "                main_sub = subgraphs[0]\n",
    "                for sub in subgraphs:\n",
    "                    if sub.number_of_nodes() > main_sub.number_of_nodes():\n",
    "                        main_sub = sub\n",
    "\n",
    "                new_graph_coalition = sorted(list(main_sub.nodes()))\n",
    "\n",
    "                # check the state map and merge the same sub-graph\n",
    "                Find_same = False\n",
    "                for old_graph_node in self.state_map.values():\n",
    "                    if Counter(old_graph_node.coalition) == Counter(new_graph_coalition):\n",
    "                        new_node = old_graph_node\n",
    "                        Find_same = True\n",
    "\n",
    "                if Find_same == False:\n",
    "                    new_node = self.MCTSNodeClass(new_graph_coalition)\n",
    "                    self.state_map[str(new_graph_coalition)] = new_node\n",
    "\n",
    "                Find_same_child = False\n",
    "                for cur_child in tree_node.children:\n",
    "                    if Counter(cur_child.coalition) == Counter(new_graph_coalition):\n",
    "                        Find_same_child = True\n",
    "\n",
    "                if Find_same_child == False:\n",
    "                    tree_node.children.append(new_node)\n",
    "\n",
    "            scores = compute_scores(self.score_func, tree_node.children)\n",
    "            for child, score in zip(tree_node.children, scores):\n",
    "                child.P = score\n",
    "\n",
    "        sum_count = sum([c.N for c in tree_node.children])\n",
    "        selected_node = max(tree_node.children, key=lambda x: x.Q() + x.U(sum_count))\n",
    "        v = self.mcts_rollout(selected_node)\n",
    "        selected_node.W += v\n",
    "        selected_node.N += 1\n",
    "        return v\n",
    "\n",
    "    def mcts(self, verbose=True):\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"The nodes in graph is {self.graph.number_of_nodes()}\")\n",
    "        for rollout_idx in range(self.n_rollout):\n",
    "            self.mcts_rollout(self.root)\n",
    "            if verbose:\n",
    "                print(f\"At the {rollout_idx} rollout, {len(self.state_map)} states that have been explored.\")\n",
    "\n",
    "        explanations = [node for _, node in self.state_map.items()]\n",
    "        explanations = sorted(explanations, key=lambda x: x.P, reverse=True)\n",
    "        # Sorts explanations based on P value (i.e. Score(.,.,.) function in MCTS)\n",
    "        return explanations\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional\n",
    "\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import k_hop_subgraph\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "class SubgraphX(_BaseExplainer):\n",
    "    r\"\"\"\n",
    "    Code adapted from Dive into Graphs (DIG)\n",
    "    Code: https://github.com/divelab/DIG\n",
    "\n",
    "    The implementation of paper\n",
    "    `On Explainability of Graph Neural Networks via Subgraph Explorations <https://arxiv.org/abs/2102.05152>`_.\n",
    "\n",
    "    Args:\n",
    "        model (:obj:`torch.nn.Module`): The target model prepared to explain\n",
    "        num_hops(:obj:`int`, :obj:`None`): The number of hops to extract neighborhood of target node\n",
    "          (default: :obj:`None`)\n",
    "        rollout(:obj:`int`): Number of iteration to get the prediction\n",
    "        min_atoms(:obj:`int`): Number of atoms of the leaf node in search tree\n",
    "        c_puct(:obj:`float`): The hyperparameter which encourages the exploration\n",
    "        expand_atoms(:obj:`int`): The number of atoms to expand\n",
    "          when extend the child nodes in the search tree\n",
    "        high2low(:obj:`bool`): Whether to expand children nodes from high degree to low degree when\n",
    "          extend the child nodes in the search tree (default: :obj:`False`)\n",
    "        local_radius(:obj:`int`): Number of local radius to calculate :obj:`l_shapley`, :obj:`mc_l_shapley`\n",
    "        sample_num(:obj:`int`): Sampling time of monte carlo sampling approximation for\n",
    "          :obj:`mc_shapley`, :obj:`mc_l_shapley` (default: :obj:`mc_l_shapley`)\n",
    "        reward_method(:obj:`str`): The command string to select the\n",
    "        subgraph_building_method(:obj:`str`): The command string for different subgraph building method,\n",
    "          such as :obj:`zero_filling`, :obj:`split` (default: :obj:`zero_filling`)\n",
    "\n",
    "    Example:\n",
    "        >>> # For graph classification task\n",
    "        >>> subgraphx = SubgraphX(model=model, num_classes=2)\n",
    "        >>> _, explanation_results, related_preds = subgraphx(x, edge_index)\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, model, num_hops: Optional[int] = None,\n",
    "                 rollout: int = 10, min_atoms: int = 3, c_puct: float = 10.0, expand_atoms=14,\n",
    "                 high2low=False, local_radius=4, sample_num=100, reward_method='mc_l_shapley',\n",
    "                 subgraph_building_method='zero_filling'):\n",
    "\n",
    "        super().__init__(model=model, is_subgraphx=True)\n",
    "        self.model.eval()\n",
    "        self.num_hops = self.update_num_hops(num_hops)\n",
    "\n",
    "        # mcts hyper-parameters\n",
    "        self.rollout = rollout\n",
    "        self.min_atoms = min_atoms # N_{min}\n",
    "        self.c_puct = c_puct\n",
    "        self.expand_atoms = expand_atoms\n",
    "        self.high2low = high2low\n",
    "\n",
    "        # reward function hyper-parameters\n",
    "        self.local_radius = local_radius\n",
    "        self.sample_num = sample_num\n",
    "        self.reward_method = reward_method\n",
    "        self.subgraph_building_method = subgraph_building_method\n",
    "\n",
    "    def update_num_hops(self, num_hops):\n",
    "        if num_hops is not None:\n",
    "            return num_hops\n",
    "\n",
    "        k = 0\n",
    "        for module in self.model.modules():\n",
    "            if isinstance(module, MessagePassing):\n",
    "                k += 1\n",
    "        return k\n",
    "\n",
    "    def get_reward_func(self, value_func, node_idx=None, explain_graph = False):\n",
    "        if explain_graph:\n",
    "            node_idx = None\n",
    "        else:\n",
    "            assert node_idx is not None\n",
    "        return reward_func(reward_method=self.reward_method,\n",
    "                           value_func=value_func,\n",
    "                           node_idx=node_idx,\n",
    "                           local_radius=self.local_radius,\n",
    "                           sample_num=self.sample_num,\n",
    "                           subgraph_building_method=self.subgraph_building_method)\n",
    "\n",
    "    def get_mcts_class(self, x, edge_index, node_idx: int = None, score_func: Callable = None, explain_graph = False):\n",
    "        if explain_graph:\n",
    "            node_idx = None\n",
    "        else:\n",
    "            assert node_idx is not None\n",
    "        return MCTS(x, edge_index,\n",
    "                    node_idx=node_idx,\n",
    "                    score_func=score_func,\n",
    "                    num_hops=self.num_hops,\n",
    "                    n_rollout=self.rollout,\n",
    "                    min_atoms=self.min_atoms,\n",
    "                    c_puct=self.c_puct,\n",
    "                    expand_atoms=self.expand_atoms,\n",
    "                    high2low=self.high2low)\n",
    "\n",
    "    def get_explanation_node(self, \n",
    "            x: Tensor, \n",
    "            edge_index: Tensor, \n",
    "            node_idx: int, \n",
    "            label: int = None, \n",
    "            y = None,\n",
    "            max_nodes: int = 14, \n",
    "            forward_kwargs: dict = {}\n",
    "        ) -> Tuple[dict, Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]]:\n",
    "        '''\n",
    "        Get explanation for a single node within a graph.\n",
    "        Args:\n",
    "            x (torch.Tensor): Input features for every node in the graph.\n",
    "            edge_index (torch.Tensor): Edge index for entire input graph.\n",
    "            node_idx (int): Node index for which to generate an explanation.\n",
    "            label (int, optional): Label for which to assume as a prediction from \n",
    "                the model when generating an explanation. If `None`, this argument \n",
    "                is set to the prediction directly from the model. (default: :obj:`None`)\n",
    "            max_nodes (int, optional): Maximum number of nodes to include in the subgraph \n",
    "                generated from the explanation. (default: :obj:`14`)\n",
    "            forward_kwargs (dict, optional): Additional arguments to model.forward \n",
    "                beyond x and edge_index. Must be keyed on argument name. \n",
    "                (default: :obj:`{}`)\n",
    "\n",
    "        :rtype: :class:`Explanation`\n",
    "        Returns:\n",
    "            exp (dict):\n",
    "                exp['feature_imp'] is `None` because no feature explanations are generated.\n",
    "                exp['node_imp'] (torch.Tensor, (n,)): Node mask of size `(n,)` where `n` \n",
    "                    is number of nodes in the entire graph described by `edge_index`. \n",
    "                    Type is `torch.bool`, with `True` indices corresponding to nodes \n",
    "                    included in the subgraph.\n",
    "                exp['edge_imp'] (torch.Tensor, (e,)): Edge mask of size `(e,)` where `e` \n",
    "                    is number of edges in the entire graph described by `edge_index`. \n",
    "                    Type is `torch.bool`, with `True` indices corresponding to edges \n",
    "                    included in the subgraph.\n",
    "            khop_info (4-tuple of torch.Tensor):\n",
    "                0. the nodes involved in the subgraph\n",
    "                1. the filtered `edge_index`\n",
    "                2. the mapping from node indices in `node_idx` to their new location\n",
    "                3. the `edge_index` mask indicating which edges were preserved \n",
    "        '''\n",
    "\n",
    "        if y is not None:\n",
    "            label = y[node_idx] # Get node_idx of label\n",
    "\n",
    "        if label is None:\n",
    "            self.model.eval()\n",
    "            pred = self.model(x.to(device), edge_index.to(device), **forward_kwargs)\n",
    "            label = int(pred.argmax(dim=1).item())\n",
    "        else:\n",
    "            label = int(label)\n",
    "\n",
    "        # collect all the class index\n",
    "        logits = self.model(x.to(device), edge_index.to(device), **forward_kwargs)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        probs = probs.squeeze()\n",
    "\n",
    "        prediction = probs[node_idx].argmax(-1)\n",
    "        self.mcts_state_map = self.get_mcts_class(x, edge_index, node_idx=node_idx)\n",
    "        self.node_idx = self.mcts_state_map.node_idx\n",
    "        # mcts will extract the subgraph and relabel the nodes\n",
    "        # value_func = GnnNets_NC2value_func(self.model,\n",
    "        #                                     node_idx=self.mcts_state_map.node_idx,\n",
    "        #                                     target_class=label)\n",
    "        value_func = self._prob_score_func_node(\n",
    "            node_idx = self.mcts_state_map.node_idx,\n",
    "            target_class = label\n",
    "        )\n",
    "        #value_func = partial(value_func, forward_kwargs=forward_kwargs)\n",
    "        def wrap_value_func(data):\n",
    "            return value_func(x=data.x.to(device), edge_index=data.edge_index.to(device), forward_kwargs=forward_kwargs)\n",
    "\n",
    "        payoff_func = self.get_reward_func(wrap_value_func, node_idx=self.mcts_state_map.node_idx, explain_graph = False)\n",
    "        self.mcts_state_map.set_score_func(payoff_func)\n",
    "        results = self.mcts_state_map.mcts(verbose=False)\n",
    "\n",
    "        # Get best result that has less than max nodes:\n",
    "        best_result = find_closest_node_result(results, max_nodes=max_nodes)\n",
    "\n",
    "        # Need to parse results:\n",
    "        node_mask, edge_mask = self.__parse_results(best_result, edge_index)\n",
    "\n",
    "        #print('args', node_idx, self.L, edge_index)\n",
    "        khop_info = k_hop_subgraph(node_idx, self.L, edge_index)\n",
    "        subgraph_edge_mask = khop_info[3] # Mask over edges\n",
    "\n",
    "        # Set explanation\n",
    "        # exp = Explanation(\n",
    "        #     node_imp = 1*node_mask[khop_info[0]], # Apply node mask\n",
    "        #     edge_imp = 1*edge_mask[subgraph_edge_mask],\n",
    "        #     node_idx = node_idx\n",
    "        # )\n",
    "        exp = Explanation(\n",
    "            node_imp = 1*node_mask, # Apply node mask\n",
    "            edge_imp = 1*edge_mask,\n",
    "            node_idx = node_idx\n",
    "        )\n",
    "\n",
    "        exp.set_enclosing_subgraph(khop_info)\n",
    "\n",
    "        return exp\n",
    "\n",
    "    def get_explanation_graph(self, \n",
    "            x: Tensor, \n",
    "            edge_index: Tensor, \n",
    "            label: int = None, \n",
    "            max_nodes: int = 14, \n",
    "            forward_kwargs: dict = {}, \n",
    "        ):\n",
    "        '''\n",
    "        Get explanation for a whole graph prediction.\n",
    "        Args:\n",
    "            x (torch.Tensor): Input features for every node in the graph.\n",
    "            edge_index (torch.Tensor): Edge index for entire input graph.\n",
    "            label (int, optional): Label for which to assume as a prediction from \n",
    "                the model when generating an explanation. If `None`, this argument \n",
    "                is set to the prediction directly from the model. (default: :obj:`None`)\n",
    "            max_nodes (int, optional): Maximum number of nodes to include in the subgraph \n",
    "                generated from the explanation. (default: :obj:`14`)\n",
    "            forward_kwargs (dict, optional): Additional arguments to model.forward \n",
    "                beyond x and edge_index. Must be keyed on argument name. \n",
    "                (default: :obj:`{}`)\n",
    "\n",
    "        :rtype: :class:`Explanation`\n",
    "        Returns:\n",
    "            exp (dict):\n",
    "                exp['feature_imp'] is `None` because no feature explanations are generated.\n",
    "                exp['node_imp'] (torch.Tensor, (n,)): Node mask of size `(n,)` where `n` \n",
    "                    is number of nodes in the entire graph described by `edge_index`. \n",
    "                    Type is `torch.bool`, with `True` indices corresponding to nodes \n",
    "                    included in the subgraph.\n",
    "                exp['edge_imp'] (torch.Tensor, (e,)): Edge mask of size `(e,)` where `e` \n",
    "                    is number of edges in the entire graph described by `edge_index`. \n",
    "                    Type is `torch.bool`, with `True` indices corresponding to edges \n",
    "                    included in the subgraph.\n",
    "        '''\n",
    "        if label is None:\n",
    "            self.model.eval()\n",
    "            pred = self.model(x, edge_index, **forward_kwargs).argmax(dim=1)\n",
    "            #label = int(pred.argmax(dim=1).item())\n",
    "        # collect all the class index\n",
    "        logits = self.model(x, edge_index, **forward_kwargs)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        probs = probs.squeeze()\n",
    "\n",
    "        prediction = probs.argmax(-1)\n",
    "        value_func = self._prob_score_func_graph(target_class = label)\n",
    "        def wrap_value_func(data):\n",
    "            return value_func(x=data.x, edge_index=data.edge_index, forward_kwargs=forward_kwargs)\n",
    "\n",
    "        payoff_func = self.get_reward_func(wrap_value_func, explain_graph = True)\n",
    "        self.mcts_state_map = self.get_mcts_class(x, edge_index, score_func=payoff_func, explain_graph = True)\n",
    "        results = self.mcts_state_map.mcts(verbose=False)\n",
    "        best_result = find_closest_node_result(results, max_nodes=max_nodes)\n",
    "\n",
    "        node_mask, edge_mask = self.__parse_results(best_result, edge_index)\n",
    "        exp = Explanation(\n",
    "            node_imp = node_mask.float(),\n",
    "            edge_imp = edge_mask.float()\n",
    "        )\n",
    "        # exp.node_imp = node_mask\n",
    "        # exp.edge_imp = edge_mask\n",
    "        exp.set_whole_graph(Data(x=x, edge_index=edge_index))\n",
    "\n",
    "        #return {'feature_imp': None, 'node_imp': node_mask, 'edge_imp': edge_mask}\n",
    "        return exp\n",
    "\n",
    "    def __parse_results(self, best_subgraph, edge_index):\n",
    "        # Function strongly based on torch_geometric.utils.subgraph function\n",
    "        # Citation: https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/utils/subgraph.html#subgraph\n",
    "\n",
    "        # Get mapping\n",
    "        map = best_subgraph.mapping\n",
    "\n",
    "        all_nodes = torch.unique(edge_index)\n",
    "\n",
    "        subgraph_nodes = torch.tensor([map[c] for c in best_subgraph.coalition], dtype=torch.long) if map is not None \\\n",
    "            else torch.tensor(best_subgraph.coalition, dtype=torch.long)\n",
    "\n",
    "        # Create node mask:\n",
    "        node_mask = torch.zeros(all_nodes.shape, dtype=torch.bool)\n",
    "        node_mask[subgraph_nodes] = 1\n",
    "\n",
    "        # Create edge_index mask\n",
    "        num_nodes = maybe_num_nodes(edge_index)\n",
    "        n_mask = torch.zeros(num_nodes, dtype = torch.bool)\n",
    "        n_mask[subgraph_nodes] = 1\n",
    "\n",
    "        edge_mask = n_mask[edge_index[0]] & n_mask[edge_index[1]]\n",
    "        return node_mask, edge_mask\n",
    "\n",
    "\n",
    "    \n",
    "def graph_exp_acc(gt_exp, generated_exp, node_thresh_factor = 0.5) -> float:\n",
    "    '''\n",
    "    Args:\n",
    "        gt_exp (Explanation): Ground truth explanation from the dataset.\n",
    "        generated_exp (Explanation): Explanation output by an explainer.\n",
    "    '''\n",
    "    EPS = 1e-09\n",
    "    JAC_feat = None\n",
    "    JAC_node = None\n",
    "    JAC_edge = None\n",
    "\n",
    "    JAC_edge = []\n",
    "    prec_edge = []\n",
    "    rec_edge = []\n",
    "    TPs = []\n",
    "    FPs = []\n",
    "    FNs = []\n",
    "    true_edges = torch.where(gt_exp.edge_imp == 1)[0]\n",
    "    for edge in range(gt_exp.edge_imp.shape[0]):\n",
    "        if generated_exp[edge] >= node_thresh_factor:\n",
    "            if edge in true_edges:\n",
    "                TPs.append(edge)\n",
    "            else:\n",
    "                FPs.append(edge)\n",
    "        else:\n",
    "            if edge in true_edges:\n",
    "                FNs.append(edge)\n",
    "    TP = len(TPs)\n",
    "    FP = len(FPs)\n",
    "    FN = len(FNs)\n",
    "    JAC = TP / (TP + FP + FN + EPS)\n",
    "    prec = TP / (TP + FP + EPS)\n",
    "    rec = TP / (TP + FN + EPS)\n",
    "    num = (2 * prec * rec)\n",
    "    if num == 0:\n",
    "        F1 = 0\n",
    "    else:\n",
    "        F1 = num / (prec + rec)\n",
    "    return JAC, prec, rec, F1\n",
    "    \n",
    "def to_networkx_conv(data, node_attrs=None, edge_attrs=None, to_undirected=False,\n",
    "                remove_self_loops=False, get_map = False):\n",
    "    r\"\"\"Converts a :class:`torch_geometric.data.Data` instance to a\n",
    "    :obj:`networkx.Graph` if :attr:`to_undir\n",
    "    ected` is set to :obj:`True`, or\n",
    "    a directed :obj:`networkx.DiGraph` otherwise.\n",
    "\n",
    "    Args:\n",
    "        data (torch_geometric.data.Data): The data object.\n",
    "        node_attrs (iterable of str, optional): The node attributes to be\n",
    "            copied. (default: :obj:`None`)\n",
    "        edge_attrs (iterable of str, optional): The edge attributes to be\n",
    "            copied. (default: :obj:`None`)\n",
    "        to_undirected (bool, optional): If set to :obj:`True`, will return a\n",
    "            a :obj:`networkx.Graph` instead of a :obj:`networkx.DiGraph`. The\n",
    "            undirected graph will correspond to the upper triangle of the\n",
    "            corresponding adjacency matrix. (default: :obj:`False`)\n",
    "        remove_self_loops (bool, optional): If set to :obj:`True`, will not\n",
    "            include self loops in the resulting graph. (default: :obj:`False`)\n",
    "        get_map (bool, optional): If `True`, returns a tuple where the second\n",
    "            element is a map from original node indices to new ones.\n",
    "            (default: :obj:`False`)\n",
    "    \"\"\"\n",
    "    if to_undirected:\n",
    "        G = nx.Graph()\n",
    "        #data.edge_index = pyg_utils.to_undirected(data.edge_index)\n",
    "    else:\n",
    "        G = nx.DiGraph()\n",
    "\n",
    "    node_list = sorted(torch.unique(data.edge_index).tolist())\n",
    "    #node_list = np.arange(data.x.shape[0])\n",
    "    map_norm = {node_list[i]:i for i in range(len(node_list))}\n",
    "    rev_map_norm = {v:k for k, v in map_norm.items()}\n",
    "    G.add_nodes_from([map_norm[n] for n in node_list])\n",
    "\n",
    "    values = {}\n",
    "    for key, item in data:\n",
    "        if torch.is_tensor(item):\n",
    "            values[key] = item.squeeze().tolist()\n",
    "        else:\n",
    "            values[key] = item\n",
    "        if isinstance(values[key], (list, tuple)) and len(values[key]) == 1:\n",
    "            values[key] = item[0]\n",
    "\n",
    "    for i, (u, v) in enumerate(data.edge_index.t().tolist()):\n",
    "        u = map_norm[u]\n",
    "        v = map_norm[v]\n",
    "\n",
    "        if to_undirected and v > u:\n",
    "            continue\n",
    "\n",
    "        if remove_self_loops and u == v:\n",
    "            continue\n",
    "\n",
    "        G.add_edge(u, v)\n",
    "        for key in edge_attrs if edge_attrs is not None else []:\n",
    "            G[u][v][key] = values[key][i]\n",
    "\n",
    "    for key in node_attrs if node_attrs is not None else []:\n",
    "        for i, feat_dict in G.nodes(data=True):\n",
    "            feat_dict.update({key: values[key][i]})\n",
    "\n",
    "    if get_map:\n",
    "        return G, map_norm\n",
    "    else:\n",
    "        G = nx.relabel_nodes(G, mapping=rev_map_norm)\n",
    "        return G\n",
    "\n",
    "def mask_graph(edge_index: torch.Tensor, \n",
    "        node_mask: torch.Tensor = None, \n",
    "        edge_mask: torch.Tensor = None):\n",
    "    '''\n",
    "    Masks the edge_index of a graph given either node_mask or edge_mask\n",
    "    Args:\n",
    "        edge_index (torch.tensor, dtype=torch.int)\n",
    "        node_mask (torch.tensor, dtype=bool)\n",
    "        edge_mask (torch.tensor, dtype=bool)\n",
    "    '''\n",
    "    # edge_index's are always size (2,e) with e=number of edges\n",
    "    if node_mask is not None:\n",
    "        nodes = node_mask.nonzero(as_tuple=True)[0].tolist()\n",
    "        created_edge_mask = torch.zeros(edge_index.shape[1])\n",
    "        for i in range(edge_index.shape[1]):\n",
    "            edge = edge_index[:,i]\n",
    "            if (edge[0] in nodes) or (edge[1] in nodes):\n",
    "                created_edge_mask[i] = 1\n",
    "        created_edge_mask = created_edge_mask.type(bool)\n",
    "        edge_index = edge_index[:,created_edge_mask]\n",
    "    elif edge_mask is not None:\n",
    "        edge_index = edge_index[:,edge_mask]\n",
    "    return edge_index\n",
    "\n",
    "def whole_graph_mask_to_subgraph(node_mask, edge_mask = None, subgraph_nodes = None, subgraph_eidx = None):\n",
    "    '''Converts mask of whole graph to a mask of a subgraph'''\n",
    "    nodes = node_mask.nonzero(as_tuple=True)[0]\n",
    "    subgraph_node_mask = torch.tensor([n.item() in nodes.tolist() for n in subgraph_nodes], dtype = torch.bool) \\\n",
    "            if subgraph_nodes is not None else None\n",
    "    return subgraph_node_mask, None\n",
    "\n",
    "def khop_subgraph_nx(\n",
    "        node_idx: int,\n",
    "        num_hops: int, \n",
    "        G: nx.Graph\n",
    "    ):\n",
    "    '''\n",
    "    Finds k-hop neighborhood in a networkx graph. Uses a BFS of depth num_hops\n",
    "        on the networkx Graph provided to find edges.\n",
    "    ..note:: Includes node_idx within subgraph\n",
    "    Args:\n",
    "        node_idx (int): Node for which we are to find a subgraph around.\n",
    "        num_hops (int): Number of hops for which to search.\n",
    "        G (nx.Graph): Graph on which to find k-hop subgraph\n",
    "    :rtype: list\n",
    "        nodes (list): Nodes in the k-hop subgraph\n",
    "    '''\n",
    "    edges = list(nx.bfs_edges(G, node_idx, depth_limit = num_hops))\n",
    "    return list(np.unique(edges))\n",
    "\n",
    " \n",
    "from torch_geometric.utils import from_networkx, k_hop_subgraph, subgraph\n",
    "from torch_geometric.data import Data   \n",
    "def match_torch_to_nx_edges(G: nx.Graph, edge_index: torch.Tensor):\n",
    "    '''\n",
    "    Gives dictionary matching index in edge_index to G.edges\n",
    "        - Supports matching for undirected edges\n",
    "        - Mainly for plotting\n",
    "    '''\n",
    "\n",
    "    edges_list = list(G.edges)\n",
    "\n",
    "    edges_map = dict()\n",
    "\n",
    "    for i in range(len(edges_list)):\n",
    "        e1, e2 = edges_list[i]\n",
    "\n",
    "        # Check e1 -> 0, e2 -> 1\n",
    "        # cond1 = ((e1 == edge_index[0,:]) & (e2 == edge_index[1,:])).nonzero(as_tuple=True)[0]\n",
    "        # cond2 = ((e2 == edge_index[0,:]) & (e1 == edge_index[1,:])).nonzero(as_tuple=True)[0]\n",
    "        cond1 = ((e1 == edge_index[0,:]) & (e2 == edge_index[1,:])).nonzero(as_tuple=True)[0]\n",
    "        cond2 = ((e2 == edge_index[0,:]) & (e1 == edge_index[1,:])).nonzero(as_tuple=True)[0]\n",
    "        #print(cond1)\n",
    "\n",
    "        if cond1.shape[0] > 0:\n",
    "            edges_map[(e1, e2)] = cond1[0].item()\n",
    "            edges_map[(e2, e1)] = cond1[0].item()\n",
    "        elif cond2.shape[0] > 0:\n",
    "            edges_map[(e1, e2)] = cond2[0].item()\n",
    "            edges_map[(e2, e1)] = cond2[0].item()\n",
    "        else:\n",
    "            raise ValueError('Edge not in graph')\n",
    "    return edges_map\n",
    "\n",
    "def remove_duplicate_edges(edge_index):\n",
    "    # Removes duplicate edges from edge_index, making it arbitrarily directed (random positioning):\n",
    "\n",
    "    new_edge_index = []\n",
    "    added_nodes = set()\n",
    "    dict_tracker = dict()\n",
    "\n",
    "    edge_mask = torch.zeros(edge_index.shape[1], dtype=bool)\n",
    "\n",
    "    for i in range(edge_index.shape[1]):\n",
    "        e1 = edge_index[0,i].item()\n",
    "        e2 = edge_index[1,i].item()\n",
    "        if e1 in added_nodes:\n",
    "            if (e2 in dict_tracker[e1]):\n",
    "                continue\n",
    "            dict_tracker[e1].append(e2)\n",
    "        else:\n",
    "            dict_tracker[e1] = [e2]\n",
    "            added_nodes.add(e1)\n",
    "        if e2 in added_nodes:\n",
    "            if (e1 in dict_tracker[e2]):\n",
    "                continue\n",
    "            dict_tracker[e2].append(e1)\n",
    "        else:\n",
    "            dict_tracker[e2] = [e1]\n",
    "            added_nodes.add(e2)\n",
    "\n",
    "        new_edge_index.append((e1, e2)) # Append only one version\n",
    "        edge_mask[i] = True\n",
    "    return torch.tensor(new_edge_index).t().contiguous(), edge_mask\n",
    "    \n",
    "from torch.nn import PairwiseDistance as pdist\n",
    "def make_node_ref(nodes: torch.Tensor):\n",
    "    '''\n",
    "    Makes a node reference to unite node indicies across explanations. \n",
    "        Returns a dictionary keyed on node indices in tensor provided.\n",
    "    Args:\n",
    "        nodes (torch.tensor): Tensor of nodes to reference.\n",
    "    \n",
    "    :rtype: :obj:`Dict`\n",
    "    '''\n",
    "    node_reference = {nodes[i].item():i for i in range(nodes.shape[0])}\n",
    "    return node_reference\n",
    "\n",
    "def node_mask_from_edge_mask(node_subset: torch.Tensor, edge_index: torch.Tensor, edge_mask: torch.Tensor = None):\n",
    "    '''\n",
    "    Gets node mask from an edge_mask:\n",
    "\n",
    "    Args:\n",
    "        node_subset (torch.Tensor): Subset of nodes to include in the mask (i.e. that become True).\n",
    "        edge_index (torch.Tensor): Full edge index of graph.\n",
    "        edge_mask (torch.Tensor): Boolean mask over all edges in edge_index. Shape: (edge_index.shape[1],).\n",
    "    '''\n",
    "    if edge_mask is not None:\n",
    "        mask_eidx = edge_index[:,edge_mask]\n",
    "    else:\n",
    "        mask_eidx = edge_index\n",
    "\n",
    "    unique_nodes = torch.unique(mask_eidx)\n",
    "\n",
    "    node_mask = torch.tensor([node_subset[i] in unique_nodes for i in range(node_subset.shape[0])])\n",
    "    \n",
    "    return node_mask.float()\n",
    "\n",
    "def edge_mask_from_node_mask(node_mask: torch.Tensor, edge_index: torch.Tensor):\n",
    "    '''\n",
    "    Convert edge_mask to node_mask\n",
    "\n",
    "    Args:\n",
    "        node_mask (torch.Tensor): Boolean mask over all nodes included in edge_index. Indices must \n",
    "            match to those in edge index. This is straightforward for graph-level prediction, but \n",
    "            converting over subgraphs must be done carefully to match indices in both edge_index and\n",
    "            the node_mask.\n",
    "    '''\n",
    "\n",
    "    node_numbers = node_mask.nonzero(as_tuple=True)[0]\n",
    "\n",
    "    iter_mask = torch.zeros((edge_index.shape[1],))\n",
    "\n",
    "    # See if edges have both ends in the node mask\n",
    "    for i in range(edge_index.shape[1]):\n",
    "        iter_mask[i] = (edge_index[0,i] in node_numbers) and (edge_index[1,i] in node_numbers) \n",
    "    \n",
    "    return iter_mask\n",
    "\n",
    "\n",
    "def top_k_mask(to_mask: torch.Tensor, top_k: int):\n",
    "    '''\n",
    "    Perform a top-k mask on to_mask tensor.\n",
    "\n",
    "    ..note:: Deals with identical values in the same way as\n",
    "        torch.sort.\n",
    "\n",
    "    Args:\n",
    "        to_mask (torch.Tensor): Tensor to mask.\n",
    "        top_k (int): How many features in Tensor to select.\n",
    "\n",
    "    :rtype: :obj:`torch.Tensor`\n",
    "    Returns:\n",
    "        torch.Tensor: Masked version of to_mask\n",
    "    '''\n",
    "    inds = torch.argsort(to_mask)[-int(top_k):]\n",
    "    mask = torch.zeros_like(to_mask)\n",
    "    mask[inds] = 1\n",
    "    return mask.long()\n",
    "\n",
    "def threshold_mask(to_mask: torch.Tensor, threshold: float):\n",
    "    '''\n",
    "    Perform a threshold mask on to_mask tensor.\n",
    "\n",
    "    Args:\n",
    "        to_mask (torch.Tensor): Tensor to mask.\n",
    "        threshold (float): Select all values greater than this threshold.\n",
    "\n",
    "    :rtype: :obj:`torch.Tensor`\n",
    "    Returns:\n",
    "        torch.Tensor: Masked version of to_mask.\n",
    "    '''\n",
    "    return (to_mask > threshold).long()\n",
    "\n",
    "def distance(emb_1: torch.tensor, emb_2: torch.tensor, p=2) -> float:\n",
    "    '''\n",
    "    Calculates the distance between embeddings generated by a GNN model\n",
    "    Args:\n",
    "        emb_1 (torch.tensor): embeddings for the clean graph\n",
    "        emb_2 (torch.tensor): embeddings for the perturbed graph\n",
    "    '''\n",
    "    if p == 0:\n",
    "        return torch.dist(emb_1, emb_2, p=0).item()\n",
    "    elif p == 1:\n",
    "        return torch.dist(emb_1, emb_2, p=1).item()\n",
    "    elif p == 2:\n",
    "        return torch.dist(emb_1, emb_2, p=2).item()\n",
    "    else:\n",
    "        print('Invalid choice! Exiting..')\n",
    "\n",
    "def match_edge_presence(edge_index, node_idx):\n",
    "    '''\n",
    "    Returns edge mask with the spots containing node_idx highlighted\n",
    "    '''\n",
    "\n",
    "    emask = torch.zeros(edge_index.shape[1]).bool()\n",
    "\n",
    "    if isinstance(node_idx, torch.Tensor):\n",
    "        if node_idx.shape[0] > 1:\n",
    "            for ni in node_idx:\n",
    "                emask = emask | ((edge_index[0,:] == ni) | (edge_index[1,:] == ni))\n",
    "        else:\n",
    "            emask = ((edge_index[0,:] == node_idx) | (edge_index[1,:] == node_idx))\n",
    "    else:\n",
    "        emask = ((edge_index[0,:] == node_idx) | (edge_index[1,:] == node_idx))\n",
    "\n",
    "    return emask\n",
    "\n",
    "class EnclosingSubgraph:\n",
    "    '''\n",
    "    Args: \n",
    "        nodes (torch.LongTensor): Nodes in subgraph.\n",
    "        edge_index (torch.LongTensor): Edge index for subgraph \n",
    "        inv (torch.LongTensor): Inversion of nodes in subgraph (see\n",
    "            torch_geometric.utils.k_hop_subgraph method.)\n",
    "        edge_mask (torch.BoolTensor): Mask of edges in entire graph.\n",
    "        directed (bool, optional): If True, subgraph is directed. \n",
    "            (:default: :obj:`False`)\n",
    "    '''\n",
    "    def __init__(self, \n",
    "            nodes: torch.LongTensor, \n",
    "            edge_index: torch.LongTensor, \n",
    "            inv: torch.LongTensor, \n",
    "            edge_mask: torch.BoolTensor, \n",
    "            directed: Optional[bool] = False\n",
    "        ):\n",
    "\n",
    "        self.nodes = nodes\n",
    "        self.edge_index = edge_index\n",
    "        self.inv = inv\n",
    "        self.edge_mask = edge_mask\n",
    "        self.directed = directed\n",
    "\n",
    "    def draw(self, show = False):\n",
    "        G = to_networkx_conv(Data(edge_index=self.edge_index), to_undirected=True)\n",
    "        nx.draw(G)\n",
    "        if show:\n",
    "            plt.show()\n",
    "            \n",
    "class Explanation:\n",
    "    '''\n",
    "    Members:\n",
    "        feature_imp (torch.Tensor): Feature importance scores\n",
    "            - Size: (x1,) with x1 = number of features\n",
    "        node_imp (torch.Tensor): Node importance scores\n",
    "            - Size: (n,) with n = number of nodes in subgraph or graph\n",
    "        edge_imp (torch.Tensor): Edge importance scores\n",
    "            - Size: (e,) with e = number of edges in subgraph or graph\n",
    "        node_idx (int): Index for node explained by this instance\n",
    "        node_reference (tensor of ints): Tensor matching length of `node_reference` \n",
    "            which maps each index onto original node in the graph\n",
    "        edge_reference (tensor of ints): Tensor maching lenght of `edge_reference`\n",
    "            which maps each index onto original edge in the graph's edge\n",
    "            index\n",
    "        graph (torch_geometric.data.Data): Original graph on which explanation\n",
    "            was computed\n",
    "            - Optional member, can be left None if graph is too large\n",
    "    Optional members:\n",
    "        enc_subgraph (Subgraph): k-hop subgraph around \n",
    "            - Corresponds to nodes and edges comprising computational graph around node\n",
    "    '''\n",
    "    def __init__(self,\n",
    "        feature_imp: Optional[torch.tensor] = None,\n",
    "        node_imp: Optional[torch.tensor] = None,\n",
    "        edge_imp: Optional[torch.tensor] = None,\n",
    "        node_idx: Optional[torch.tensor] = None,\n",
    "        node_reference: Optional[torch.tensor] = None,\n",
    "        edge_reference: Optional[torch.tensor] = None,\n",
    "        graph = None):\n",
    "\n",
    "        # Establish basic properties\n",
    "        self.feature_imp = feature_imp\n",
    "        self.node_imp = node_imp\n",
    "        self.edge_imp = edge_imp\n",
    "\n",
    "        # Only established if passed explicitly in init, not overwritten by enclosing subgraph \n",
    "        #   unless explicitly specified\n",
    "        self.node_reference = node_reference\n",
    "        self.edge_reference = edge_reference\n",
    "\n",
    "        self.node_idx = node_idx # Set this for node-level prediction explanations\n",
    "        self.graph = graph\n",
    "\n",
    "    def set_enclosing_subgraph(self, subgraph):\n",
    "        '''\n",
    "        Args:\n",
    "            subgraph (tuple, EnclosingSubgraph, or nx.Graph): Return value from torch_geometric.utils.k_hop_subgraph\n",
    "        '''\n",
    "        if isinstance(subgraph, EnclosingSubgraph):\n",
    "            self.enc_subgraph = subgraph\n",
    "        elif isinstance(subgraph, nx.Graph):\n",
    "            # Convert from nx.Graph\n",
    "            data = from_networkx(subgraph)\n",
    "            nodes = torch.unique(data.edge_index)\n",
    "            # TODO: Support inv and edge_mask through networkx\n",
    "            self.enc_subgraph = EnclosingSubgraph(\n",
    "                nodes = nodes,\n",
    "                edge_index = data.edge_index,\n",
    "                inv = None,\n",
    "                edge_mask = None\n",
    "            )\n",
    "        else: # Assumed to be a tuple:\n",
    "            self.enc_subgraph = EnclosingSubgraph(*subgraph)\n",
    "\n",
    "        if self.node_reference is None:\n",
    "            self.node_reference = make_node_ref(self.enc_subgraph.nodes)\n",
    "\n",
    "    def apply_subgraph_mask(self, \n",
    "        mask_node: Optional[bool] = False, \n",
    "        mask_edge: Optional[bool] = False):\n",
    "        '''\n",
    "        Performs automatic masking on the node and edge importance members\n",
    "\n",
    "        Args:\n",
    "            mask_node (bool, optional): If True, performs masking on node_imp based on enclosing subgraph nodes.\n",
    "                Assumes that node_imp is set for entire graph and then applies mask.\n",
    "            mask_edge (bool, optional): If True, masks edges in edge_imp based on enclosing subgraph edge mask.\n",
    "\n",
    "        Example workflow:\n",
    "        >>> exp = Explanation()\n",
    "        >>> exp.node_imp = node_importance_tensor\n",
    "        >>> exp.edge_imp = edge_importance_tensor\n",
    "        >>> exp.set_enclosing_subgraph(k_hop_subgraph(node_idx, k, edge_index))\n",
    "        >>> exp.apply_subgraph_mask(True, True) # Masks both node and edge importance\n",
    "        '''\n",
    "        if mask_edge:\n",
    "            mask_inds = self.enc_subgraph.edge_mask.nonzero(as_tuple=True)[0]\n",
    "            self.edge_imp = self.edge_imp[mask_inds] # Perform masking on current member\n",
    "        if mask_node:\n",
    "            self.node_imp = self.node_imp[self.enc_subgraph.nodes]\n",
    "\n",
    "    def set_whole_graph(self, data: Data):\n",
    "        '''\n",
    "        Args:\n",
    "            data (torch_geometric.data.Data): Data object representing the graph to store.\n",
    "        \n",
    "        :rtype: :obj:`None`\n",
    "        '''\n",
    "        self.graph = data\n",
    "\n",
    "    def graph_to_networkx(self, \n",
    "        to_undirected=False, \n",
    "        remove_self_loops: Optional[bool]=False,\n",
    "        get_map: Optional[bool] = False):\n",
    "        '''\n",
    "        Convert graph to Networkx Graph\n",
    "\n",
    "        Args:\n",
    "            to_undirected (bool, optional): If True, graph is undirected. (:default: :obj:`False`)\n",
    "            remove_self_loops (bool, optional): If True, removes all self-loops in graph.\n",
    "                (:default: :obj:`False`)\n",
    "            get_map (bool, optional): If True, returns a map of nodes in graph \n",
    "                to nodes in the Networkx graph. (:default: :obj:`False`)\n",
    "\n",
    "        :rtype: :class:`Networkx.Graph` or :class:`Networkx.DiGraph`\n",
    "            If `get_map == True`, returns tuple: (:class:`Networkx.Graph`, :class:`dict`)\n",
    "        '''\n",
    "\n",
    "        if to_undirected:\n",
    "            G = nx.Graph()\n",
    "        else:\n",
    "            G = nx.DiGraph()\n",
    "\n",
    "        node_list = sorted(torch.unique(self.graph.edge_index).tolist())\n",
    "        map_norm =  {node_list[i]:i for i in range(len(node_list))}\n",
    "\n",
    "        G.add_nodes_from([map_norm[n] for n in node_list])\n",
    "\n",
    "        # Assign values to each node:\n",
    "        # Skipping for now\n",
    "\n",
    "        for i, (u, v) in enumerate(self.graph.edge_index.t().tolist()):\n",
    "            u = map_norm[u]\n",
    "            v = map_norm[v]\n",
    "\n",
    "            if to_undirected and v > u:\n",
    "                continue\n",
    "\n",
    "            if remove_self_loops and u == v:\n",
    "                continue\n",
    "\n",
    "            G.add_edge(u, v)\n",
    "\n",
    "            # No edge_attr additions added now\n",
    "            if self.edge_imp is not None:\n",
    "                G[u][v]['edge_imp'] = self.edge_imp[i].item()\n",
    "            # for key in edge_attrs if edge_attrs is not None else []:\n",
    "            #     G[u][v][key] = values[key][i]\n",
    "\n",
    "        if self.node_imp is not None:\n",
    "            for i, feat_dict in G.nodes(data=True):\n",
    "                # self.node_imp[i] should be a scalar value\n",
    "                feat_dict.update({'node_imp': self.node_imp[map_norm[i]].item()})\n",
    "\n",
    "        if get_map:\n",
    "            return G, map_norm\n",
    "\n",
    "        return G\n",
    "\n",
    "    def enc_subgraph_to_networkx(self, \n",
    "        to_undirected=False, \n",
    "        remove_self_loops: Optional[bool]=False,\n",
    "        get_map: Optional[bool] = False):\n",
    "        '''\n",
    "        Convert enclosing subgraph to Networkx Graph\n",
    "\n",
    "        Args:\n",
    "            to_undirected (bool, optional): If True, graph is undirected. (:default: :obj:`False`)\n",
    "            remove_self_loops (bool, optional): If True, removes all self-loops in graph.\n",
    "                (:default: :obj:`False`)\n",
    "            get_map (bool, optional): If True, returns a map of nodes in enclosing subgraph \n",
    "                to nodes in the Networkx graph. (:default: :obj:`False`)\n",
    "\n",
    "        :rtype: :class:`Networkx.Graph` or :class:`Networkx.DiGraph`\n",
    "            If `get_map == True`, returns tuple: (:class:`Networkx.Graph`, :class:`dict`)\n",
    "        '''\n",
    "\n",
    "        if to_undirected:\n",
    "            G = nx.Graph()\n",
    "        else:\n",
    "            G = nx.DiGraph()\n",
    "\n",
    "        node_list = sorted(torch.unique(self.enc_subgraph.edge_index).tolist())\n",
    "        map_norm =  {node_list[i]:i for i in range(len(node_list))}\n",
    "        rev_map = {v:k for k,v in map_norm.items()}\n",
    "\n",
    "        G.add_nodes_from([map_norm[n] for n in node_list])\n",
    "\n",
    "        # Assign values to each node:\n",
    "        # Skipping for now\n",
    "\n",
    "        for i, (u, v) in enumerate(self.enc_subgraph.edge_index.t().tolist()):\n",
    "            u = map_norm[u]\n",
    "            v = map_norm[v]\n",
    "\n",
    "            if to_undirected and v > u:\n",
    "                continue\n",
    "\n",
    "            if remove_self_loops and u == v:\n",
    "                continue\n",
    "\n",
    "            G.add_edge(u, v)\n",
    "\n",
    "            if self.edge_imp is not None:\n",
    "                G.edges[u, v]['edge_imp'] = self.edge_imp[i].item()\n",
    "\n",
    "        if self.node_imp is not None:\n",
    "            for i, feat_dict in G.nodes(data=True):\n",
    "                # self.node_imp[i] should be a scalar value\n",
    "                feat_dict.update({'node_imp': self.node_imp[i].item()})\n",
    "\n",
    "        if get_map:\n",
    "            return G, map_norm\n",
    "\n",
    "        return G\n",
    "\n",
    "    def top_k_node_imp(self, top_k: int, inplace = False):\n",
    "        '''\n",
    "        Top-k masking of the node importance for this Explanation.\n",
    "\n",
    "        Args:\n",
    "            top_k (int): How many highest scores to include in the mask.\n",
    "            inplace (bool, optional): If True, masks the node_imp member \n",
    "                of the class.\n",
    "\n",
    "        :rtype: :obj:`torch.Tensor`\n",
    "        '''\n",
    "\n",
    "        if inplace:\n",
    "            self.node_imp = top_k_mask(self.node_imp, top_k)\n",
    "        else:\n",
    "            return top_k_mask(self.node_imp, top_k)\n",
    "\n",
    "    def top_k_edge_imp(self, top_k: int, inplace = False):\n",
    "        '''\n",
    "        Top-k masking of the edge importance for this Explanation.\n",
    "\n",
    "        Args:\n",
    "            top_k (int): How many highest scores to include in the mask.\n",
    "            inplace (bool, optional): If True, masks the node_imp member \n",
    "                of the class.\n",
    "\n",
    "        :rtype: :obj:`torch.Tensor`\n",
    "        '''\n",
    "        if inplace:\n",
    "            self.edge_imp = top_k_mask(self.edge_imp, top_k)\n",
    "        else:\n",
    "            return top_k_mask(self.edge_imp, top_k)\n",
    "\n",
    "    def top_k_feature_imp(self, top_k: int, inplace = False):\n",
    "        '''\n",
    "        Top-k masking of the feature importance for this Explanation.\n",
    "\n",
    "        Args:\n",
    "            top_k (int): How many highest scores to include in the mask.\n",
    "            inplace (bool, optional): If True, masks the node_imp member \n",
    "                of the class.\n",
    "\n",
    "        :rtype: :obj:`torch.Tensor`\n",
    "        '''\n",
    "        if inplace:\n",
    "            self.feature_imp = top_k_mask(self.feature_imp, top_k)\n",
    "        else:\n",
    "            return top_k_mask(self.feature_imp, top_k)\n",
    "\n",
    "    def thresh_node_imp(self, threshold: float, inplace = False):\n",
    "        '''\n",
    "        Threshold mask the node importance\n",
    "\n",
    "        Args:\n",
    "            threshold (float): Select all values greater than this value.\n",
    "            inplace (bool, optional): If True, masks the node_imp member \n",
    "                of the class.\n",
    "\n",
    "        :rtype: :obj:`torch.Tensor`\n",
    "        '''\n",
    "        if inplace:\n",
    "            self.node_imp = threshold_mask(self.node_imp, threshold)\n",
    "        else:\n",
    "            return threshold_mask(self.node_imp, threshold)\n",
    "\n",
    "    def thresh_edge_imp(self, threshold: float, inplace = False):\n",
    "        '''\n",
    "        Threshold mask the edge importance\n",
    "\n",
    "        Args:\n",
    "            threshold (float): Select all values greater than this value.\n",
    "            inplace (bool, optional): If True, masks the node_imp member \n",
    "                of the class.\n",
    "\n",
    "        :rtype: :obj:`torch.Tensor`\n",
    "        '''\n",
    "        if inplace:\n",
    "            self.edge_imp = threshold_mask(self.edge_imp, threshold)\n",
    "        else:\n",
    "            return threshold_mask(self.edge_imp, threshold)\n",
    "\n",
    "    def thresh_feature_imp(self, threshold: float, inplace = False):\n",
    "        '''\n",
    "        Threshold mask the feature importance\n",
    "\n",
    "        Args:\n",
    "            threshold (float): Select all values greater than this value.\n",
    "            inplace (bool, optional): If True, masks the node_imp member \n",
    "                of the class.\n",
    "\n",
    "        :rtype: :obj:`torch.Tensor`\n",
    "        '''\n",
    "        if inplace:\n",
    "            self.feature_imp = threshold_mask(self.feature_imp, threshold)\n",
    "        else:\n",
    "            return threshold_mask(self.feature_imp, threshold)\n",
    "\n",
    "    def visualize_node(self, \n",
    "            num_hops: int,\n",
    "            graph_data: Data = None,\n",
    "            additional_hops: int = 1, \n",
    "            heat_by_prescence: bool = False, \n",
    "            heat_by_exp: bool = True, \n",
    "            node_agg_method: str = 'sum',\n",
    "            ax: matplotlib.axes.Axes = None,\n",
    "            show: bool = False,\n",
    "            show_node_labels: bool = False,\n",
    "            norm_imps = False,\n",
    "        ):\n",
    "        '''\n",
    "        Shows the explanation in context of a few more hops out than its k-hop neighborhood. Used for\n",
    "            visualizing the explanation for a node-level prediction task.\n",
    "        \n",
    "        ..note:: If neither `heat_by_prescence` or `heat_by_exp` are true, the method plots a simple\n",
    "            visualization of the subgraph around the focal node.\n",
    "\n",
    "        Args:\n",
    "            num_hops (int): Number of hops in the enclosing subgraph.\n",
    "            graph_data (torch_geometric.data.Data, optional): Data object containing graph. Don't provide\n",
    "                if already stored in the dataset. Used so large graphs can be stored externally and used\n",
    "                for visualization. (:default: :obj:`None`)\n",
    "            additional_hops (int, optional): Additional number of hops to include for the visualization.\n",
    "                If the size of the enclosing subgraph for a node `v` with respect to some model `f` \n",
    "                is `n`, then we would show the `n + additional_hops`-hop neighborhood around `v`.\n",
    "                (:default: :obj:`1`)\n",
    "            heat_by_prescence (bool, optional): If True, only highlights nodes in the enclosing subgraph.\n",
    "                Useful for debugging or non-explanation visualization. (:default: :obj:`False`)\n",
    "            heat_by_exp (bool, optional): If True, highlights nodes and edges by explanation values. \n",
    "                (:default: :obj:`True`)\n",
    "            node_agg_method (str, optional): Aggregation method to use for showing multi-dimensional\n",
    "                node importance scores (i.e. across features, such as GuidedBP or Vanilla Gradient).\n",
    "                Options: :obj:`'sum'` and :obj:`'max'`. (:default: :obj:`'sum'`)\n",
    "            ax (matplotlib.axes.Axes, optional): Axis on which to draw. If not provided, draws directly\n",
    "                to plt. (:default: :obj:`None`)\n",
    "            show (bool, optional): If True, shows the plot immediately after drawing. (:default: :obj:`False`)\n",
    "            show_node_labels (bool, optional): If True, shows the node labels as integers overlaid on the \n",
    "                plot. (:default: :obj:`False`)\n",
    "        '''\n",
    "\n",
    "        assert self.node_idx is not None, \"visualize_node only for node-level explanations, but node_idx is None\" \n",
    "\n",
    "        #data_G = self.graph.get_Data()\n",
    "        wholeG = to_networkx_conv(graph_data, to_undirected=True)\n",
    "        kadd_hop_neighborhood = khop_subgraph_nx(\n",
    "                G = wholeG, \n",
    "                num_hops= num_hops + additional_hops, \n",
    "                node_idx=self.node_idx\n",
    "            )\n",
    "\n",
    "        subG = wholeG.subgraph(kadd_hop_neighborhood)\n",
    "\n",
    "        node_agg = torch.sum if node_agg_method == 'sum' else torch.max\n",
    "\n",
    "        # Identify highlighting nodes:\n",
    "        exp_nodes = self.enc_subgraph.nodes\n",
    "\n",
    "        draw_args = dict()\n",
    "\n",
    "        if norm_imps:\n",
    "            # Normalize all importance scores:\n",
    "            save_imps = [self.node_imp, self.edge_imp, self.feature_imp]\n",
    "            save_imps = [s.clone() if s is not None else s for s in save_imps ]\n",
    "            for s in (self.node_imp, self.edge_imp, self.feature_imp):\n",
    "                if s is not None:\n",
    "                    s = s / s.sum()\n",
    "\n",
    "        if heat_by_prescence:\n",
    "            if self.node_imp is not None:\n",
    "                node_c = [int(i in exp_nodes) for i in subG.nodes]\n",
    "                draw_args['node_color'] = node_c\n",
    "\n",
    "        if heat_by_exp:\n",
    "            if self.node_imp is not None:\n",
    "                node_c = []\n",
    "                for i in subG.nodes:\n",
    "                    if i in self.enc_subgraph.nodes:\n",
    "                        if isinstance(self.node_imp[self.node_reference[i]], torch.Tensor):\n",
    "                            if self.node_imp[self.node_reference[i]].dim() > 0:\n",
    "                                c = node_agg(self.node_imp[self.node_reference[i]]).item()\n",
    "                            else:\n",
    "                                c = self.node_imp[self.node_reference[i]].item()\n",
    "                        else:\n",
    "                            c = self.node_imp[self.node_reference[i]]\n",
    "                    else:\n",
    "                        c = 0\n",
    "\n",
    "                    node_c.append(c)\n",
    "\n",
    "                draw_args['node_color'] = node_c\n",
    "\n",
    "            if self.edge_imp is not None:\n",
    "                whole_edge_index, _ = subgraph(kadd_hop_neighborhood, edge_index = graph_data.edge_index)\n",
    "\n",
    "                # Need to match edge indices across edge_index and edges in graph\n",
    "                tuple_edge_index = [(whole_edge_index[0,i].item(), whole_edge_index[1,i].item()) \\\n",
    "                    for i in range(whole_edge_index.shape[1])]\n",
    "\n",
    "                _, emask = remove_duplicate_edges(self.enc_subgraph.edge_index)\n",
    "                # Remove self loops:\n",
    "                emask_2 = torch.logical_not(self.enc_subgraph.edge_index[0,:] == self.enc_subgraph.edge_index[1,:])\n",
    "                emask = emask & emask_2\n",
    "\n",
    "                trimmed_enc_subg_edge_index = self.enc_subgraph.edge_index[:,emask]\n",
    "\n",
    "                mask_edge_imp = self.edge_imp[emask].clone()\n",
    "\n",
    "                # Find where edge_imp is applied on one duplicate edge but not another:\n",
    "                masked_out_by_rmdup = self.enc_subgraph.edge_index[:,torch.logical_not(emask)]\n",
    "                ones_in_rmdup = self.edge_imp[torch.logical_not(emask)].nonzero(as_tuple=True)[0]\n",
    "                for j in ones_in_rmdup:\n",
    "                    edge = masked_out_by_rmdup[:,j].tolist()\n",
    "                    # Reverse the edge:\n",
    "                    edge = edge[::-1] \n",
    "                    \n",
    "                    trim_loc_mask = (trimmed_enc_subg_edge_index[0,:] == edge[0]) & (trimmed_enc_subg_edge_index[1,:] == edge[1])\n",
    "                    trim_loc = (trim_loc_mask).nonzero(as_tuple=True)[0] \n",
    "                    if trim_loc.shape[0] > 0:\n",
    "                        # Should be over 0 if we found it\n",
    "                        trim_loc = trim_loc[0].item()\n",
    "                        mask_edge_imp[trim_loc] = 1 # Ensure this edge is also one\n",
    "                    # Don't do anything if we didn't find it\n",
    "\n",
    "                positive_edge_indices = mask_edge_imp.nonzero(as_tuple=True)[0]\n",
    "\n",
    "                # TODO: fix edge imp vis. to handle continuous edge importance scores\n",
    "                mask_edge_imp = self.edge_imp[positive_edge_indices]\n",
    "\n",
    "                positive_edges = [(trimmed_enc_subg_edge_index[0,e].item(), trimmed_enc_subg_edge_index[1,e].item()) \\\n",
    "                    for e in positive_edge_indices]\n",
    "\n",
    "                # Tuples in list should be hashable\n",
    "                edge_list = list(subG.edges)\n",
    "\n",
    "                # Get dictionary with mapping from edge index to networkx graph\n",
    "                #edge_matcher = match_torch_to_nx_edges(subG, remove_duplicate_edges(whole_edge_index)[0])\n",
    "                edge_matcher = {edge_list[i]:i for i in range(len(edge_list))}\n",
    "                for i in range(len(edge_list)):\n",
    "                    forward_tup = edge_list[i]\n",
    "                    backward_tup = tuple(list(edge_list[i])[::-1])\n",
    "                    edge_matcher[forward_tup] = i\n",
    "                    edge_matcher[backward_tup] = i\n",
    "\n",
    "                edge_heat = torch.zeros(len(edge_list))\n",
    "                #edge_heat = torch.zeros(whole_edge_index.shape[1])\n",
    "\n",
    "                for e in positive_edges:\n",
    "                    #e = positive_edges[i]\n",
    "                    # Must find index, which is not very efficient\n",
    "                    edge_heat[edge_matcher[e]] = 1\n",
    "\n",
    "                draw_args['edge_color'] = edge_heat.tolist()\n",
    "                #coolwarm cmap:\n",
    "                draw_args['edge_cmap'] = plt.cm.coolwarm\n",
    "\n",
    "            # Heat edge explanations if given\n",
    "\n",
    "        # Seed the position to stay consistent:\n",
    "        pos = nx.spring_layout(subG, seed = 1234)\n",
    "        nx.draw(subG, pos, ax = ax, **draw_args, with_labels = show_node_labels)\n",
    "\n",
    "        # Highlight the center node index:\n",
    "        nx.draw(subG.subgraph(self.node_idx), pos, node_color = 'red', \n",
    "                node_size = 400, ax = ax)\n",
    "\n",
    "        if norm_imps:\n",
    "            self.node_imp, self.edge_imp, self.feature_imp = save_imps[0], save_imps[1], save_imps[2]\n",
    "\n",
    "        if show:\n",
    "            plt.show()\n",
    "\n",
    "def get_flag():\n",
    "    pass\n",
    "\n",
    "# Set common shapes:\n",
    "house = nx.house_graph()\n",
    "house_x = nx.house_x_graph()\n",
    "diamond = nx.diamond_graph()\n",
    "pentagon = nx.cycle_graph(n=5)\n",
    "wheel = nx.wheel_graph(n=6)\n",
    "star = nx.star_graph(n=5)\n",
    "flag = None\n",
    "\n",
    "triangle = nx.Graph()\n",
    "triangle.add_nodes_from([0, 1, 2])\n",
    "triangle.add_edges_from([(0, 1), (1, 2), (2, 0)])\n",
    "\n",
    "def random_shape(n) -> nx.Graph:\n",
    "    '''\n",
    "    Outputs a random shape as nx.Graph\n",
    "\n",
    "    ..note:: set `random.seed()` for seeding\n",
    "    \n",
    "    Args:\n",
    "        n (int): Number of shapes in the bank to draw from\n",
    "    \n",
    "    '''\n",
    "    shape_list = [\n",
    "        house,\n",
    "        pentagon,\n",
    "        wheel\n",
    "    ]\n",
    "    i = random.choice(list(range(len(shape_list))))\n",
    "    return shape_list[i], i + 1\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class NodeDataset:\n",
    "    def __init__(self, \n",
    "        name, \n",
    "        num_hops: int,\n",
    "        download: Optional[bool] = False,\n",
    "        root: Optional[str] = None\n",
    "        ):\n",
    "        self.name = name\n",
    "        self.num_hops = num_hops\n",
    "    def get_graph(self, \n",
    "        use_fixed_split: bool = True, \n",
    "        split_sizes: Tuple = (0.7, 0.2, 0.1),\n",
    "        stratify: bool = True, \n",
    "        seed: int = None):\n",
    "        if sum(split_sizes) != 1: # Normalize split sizes\n",
    "            split_sizes = np.array(split_sizes) / sum(split_sizes)\n",
    "        if use_fixed_split:\n",
    "            self.graph.train_mask = self.fixed_train_mask\n",
    "            self.graph.valid_mask = self.fixed_valid_mask\n",
    "            self.graph.test_mask  = self.fixed_test_mask\n",
    "        else:\n",
    "            assert len(split_sizes) == 3, \"split_sizes must contain (train_size, test_size, valid_size)\"\n",
    "            # Create a split for user (based on seed, etc.)\n",
    "            train_mask, test_mask = train_test_split(list(range(self.graph.num_nodes)), \n",
    "                                test_size = split_sizes[1] + split_sizes[2], \n",
    "                                random_state = seed, stratify = self.graph.y.tolist() if stratify else None)\n",
    "            if split_sizes[2] > 0:\n",
    "                valid_mask, test_mask = train_test_split(test_mask, \n",
    "                                    test_size = split_sizes[2] / split_sizes[1],\n",
    "                                    random_state = seed, stratify = self.graph.y[test_mask].tolist() if stratify else None)\n",
    "                self.graph.valid_mask = torch.tensor([i in valid_mask for i in range(self.graph.num_nodes)], dtype = torch.bool)\n",
    "            self.graph.train_mask = torch.tensor([i in train_mask for i in range(self.graph.num_nodes)], dtype = torch.bool)\n",
    "            self.graph.test_mask  = torch.tensor([i in test_mask  for i in range(self.graph.num_nodes)], dtype = torch.bool)\n",
    "        return self.graph\n",
    "    def download(self):\n",
    "        '''TODO: Implement'''\n",
    "        pass\n",
    "    def get_enclosing_subgraph(self, node_idx: int):\n",
    "        '''\n",
    "        Args:\n",
    "            node_idx (int): Node index for which to get subgraph around\n",
    "        '''\n",
    "        k_hop_tuple = k_hop_subgraph(node_idx, \n",
    "            num_hops = self.num_hops, \n",
    "            edge_index = self.graph.edge_index)\n",
    "        return EnclosingSubgraph(*k_hop_tuple)\n",
    "    def nodes_with_label(self, label = 0, mask = None) -> torch.Tensor:\n",
    "        '''\n",
    "        Get all nodes that are a certain label\n",
    "        Args:\n",
    "            label (int, optional): Label for which to find nodes.\n",
    "                (:default: :obj:`0`)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Indices of nodes that are of the label\n",
    "        '''\n",
    "        if mask is not None:\n",
    "            return ((self.graph.y == label) & (mask)).nonzero(as_tuple=True)[0]\n",
    "        return (self.graph.y == label).nonzero(as_tuple=True)[0]\n",
    "\n",
    "    def choose_node_with_label(self, label = 0, mask = None):\n",
    "        '''\n",
    "        Choose a random node with a given label\n",
    "        Args:\n",
    "            label (int, optional): Label for which to find node.\n",
    "                (:default: :obj:`0`)\n",
    "\n",
    "        Returns:\n",
    "            tuple(int, Explanation):\n",
    "                int: Node index found\n",
    "                Explanation: explanation corresponding to that node index\n",
    "        '''\n",
    "        nodes = self.nodes_with_label(label = label, mask = mask)\n",
    "        node_idx = random.choice(nodes).item()\n",
    "        return node_idx, self.explanations[node_idx]\n",
    "\n",
    "    def nodes_in_shape(self, inshape = True, mask = None):\n",
    "        '''\n",
    "        Get a group of nodes by shape membership.\n",
    "\n",
    "        Args:\n",
    "            inshape (bool, optional): If the nodes are in a shape.\n",
    "                :obj:`True` means that the nodes returned are in a shape.\n",
    "                :obj:`False` means that the nodes are not in a shape.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: All node indices for nodes in or not in a shape.\n",
    "        '''\n",
    "        # Get all nodes in a shape\n",
    "        condition = (lambda n: self.G.nodes[n]['shape'] > 0) if inshape \\\n",
    "                else (lambda n: self.G.nodes[n]['shape'] == 0)\n",
    "        if mask is not None:\n",
    "            condition = (lambda n: (condition(n) and mask[n].item()))\n",
    "        return torch.tensor([n for n in self.G.nodes if condition(n)]).long()\n",
    "\n",
    "    def choose_node_in_shape(self, inshape = True, mask = None):\n",
    "        '''\n",
    "        Gets a random node by shape membership.\n",
    "\n",
    "        Args:\n",
    "            inshape (bool, optional): If the node is in a shape.\n",
    "                :obj:`True` means that the node returned is in a shape.\n",
    "                :obj:`False` means that the node is not in a shape.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[int, Explanation]\n",
    "                int: Node index found\n",
    "                Explanation: Explanation corresponding to that node index\n",
    "        '''\n",
    "        nodes = self.nodes_in_shape(inshape = inshape, mask = mask)\n",
    "        node_idx = random.choice(nodes).item()\n",
    "        return node_idx, self.explanations[node_idx]\n",
    "\n",
    "\n",
    "    def choose_node(self, inshape = None, label = None, split = None):\n",
    "        '''\n",
    "        Chooses random nodes in the graph. Has support for multiple logical\n",
    "            indexing.\n",
    "\n",
    "        Args:\n",
    "            inshape (bool, optional): If the node is in a shape.\n",
    "                :obj:`True` means that the node returned is in a shape.\n",
    "                :obj:`False` means that the node is not in a shape.\n",
    "            label (int, optional): Label for which to find node.\n",
    "                (:default: :obj:`0`)\n",
    "        \n",
    "        Returns:\n",
    "        '''\n",
    "        split = split.lower() if split is not None else None\n",
    "\n",
    "        if split == 'validation' or split == 'valid' or split == 'val':\n",
    "            split = 'val'\n",
    "\n",
    "        map_to_mask = {\n",
    "            'train': self.graph.train_mask,\n",
    "            'val': self.graph.valid_mask,\n",
    "            'test': self.graph.test_mask,\n",
    "        }\n",
    "        \n",
    "        # Get mask based on provided string:\n",
    "        mask = None if split is None else map_to_mask[split]\n",
    "\n",
    "        if inshape is None:\n",
    "            if label is None:\n",
    "                to_choose = torch.arange(end = self.num_nodes)\n",
    "            else:\n",
    "                to_choose = self.nodes_with_label(label = label, mask = mask)\n",
    "        \n",
    "        elif label is None:\n",
    "            to_choose = self.nodes_in_shape(inshape = inshape, mask = mask)\n",
    "\n",
    "        else:\n",
    "            t_inshape = self.nodes_in_shape(inshape = inshape, mask = mask)\n",
    "            t_label = self.nodes_with_label(label = label, make = mask)\n",
    "\n",
    "            # Joint masking over shapes and labels:\n",
    "            to_choose = torch.as_tensor([n.item() for n in t_label if n in t_inshape]).long()\n",
    "\n",
    "        assert_fmt = 'Could not find a node in {} with inshape={}, label={}'\n",
    "        assert to_choose.nelement() > 0, assert_fmt.format(self.name, inshape, label)\n",
    "\n",
    "        node_idx = random.choice(to_choose).item()\n",
    "        return node_idx, self.explanations[node_idx]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return 1 # There is always just one graph\n",
    "\n",
    "    def dump(self, fname = None):\n",
    "        fname = self.name + '.pickle' if fname is None else fname\n",
    "        torch.save(self, open(fname, 'wb'))\n",
    "    @property\n",
    "    def x(self):\n",
    "        return self.graph.x\n",
    "\n",
    "    @property\n",
    "    def edge_index(self):\n",
    "        return self.graph.edge_index\n",
    "\n",
    "    def y(self):\n",
    "        return self.graph.y\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        assert idx == 0, 'Dataset has only one graph'\n",
    "        return self.graph, self.explanation\n",
    "\n",
    "class GraphDataset:\n",
    "    def __init__(self, name, split_sizes = (0.7, 0.2, 0.1), seed = None, device = None):\n",
    "        self.name = name\n",
    "        self.seed = seed\n",
    "        self.device = device\n",
    "        if split_sizes[1] > 0:\n",
    "            self.train_index, self.test_index = train_test_split(torch.arange(start = 0, end = len(self.graphs)), \n",
    "                test_size = split_sizes[1] + split_sizes[2], random_state=self.seed, shuffle = False)\n",
    "        else:\n",
    "            self.test_index = None\n",
    "            self.train_index = torch.arange(start = 0, end = len(self.graphs))\n",
    "        if split_sizes[2] > 0:\n",
    "            self.test_index, self.val_index = train_test_split(self.test_index, \n",
    "                test_size = split_sizes[2] / (split_sizes[1] + split_sizes[2]),\n",
    "                random_state = self.seed, shuffle = False)\n",
    "        else:\n",
    "            self.val_index = None\n",
    "        self.Y = torch.tensor([self.graphs[i].y for i in range(len(self.graphs))]).to(self.device)\n",
    "    def get_data_list(\n",
    "            self,\n",
    "            index,\n",
    "        ):\n",
    "        data_list = [self.graphs[i].to(self.device) for i in index]\n",
    "        exp_list = [self.explanations[i] for i in index]\n",
    "        return data_list, exp_list\n",
    "    def get_loader(\n",
    "            self, \n",
    "            index,\n",
    "            batch_size = 16,\n",
    "            **kwargs\n",
    "        ):\n",
    "        data_list, exp_list = self.get_data_list(index)\n",
    "        for i in range(len(data_list)):\n",
    "            data_list[i].exp_key = [i]\n",
    "        loader = DataLoader(data_list, batch_size = batch_size, shuffle = True)\n",
    "        return loader, exp_list\n",
    "    def get_train_loader(self, batch_size = 16):\n",
    "        return self.get_loader(index=self.train_index, batch_size = batch_size)\n",
    "    def get_train_list(self):\n",
    "        return self.get_data_list(index = self.train_index)\n",
    "    def get_test_loader(self):\n",
    "        assert self.test_index is not None, 'test_index is None'\n",
    "        return self.get_loader(index=self.test_index, batch_size = 1)\n",
    "    def get_test_list(self):\n",
    "        assert self.test_index is not None, 'test_index is None'\n",
    "        return self.get_data_list(index = self.test_index)\n",
    "    def get_val_loader(self):\n",
    "        assert self.test_index is not None, 'val_index is None'\n",
    "        return self.get_loader(index=self.val_index, batch_size = 1)\n",
    "    def get_val_list(self):\n",
    "        assert self.val_index is not None, 'val_index is None'\n",
    "        return self.get_data_list(index = self.val_index)\n",
    "    def get_train_w_label(self, label):\n",
    "        inds_to_choose = (self.Y[self.train_index] == label).nonzero(as_tuple=True)[0]\n",
    "        in_train_idx = inds_to_choose[torch.randint(low = 0, high = inds_to_choose.shape[0], size = (1,))]\n",
    "        chosen = self.train_index[in_train_idx.item()]\n",
    "        return self.graphs[chosen], self.explanations[chosen]\n",
    "    def get_test_w_label(self, label):\n",
    "        assert self.test_index is not None, 'test_index is None'\n",
    "        inds_to_choose = (self.Y[self.test_index] == label).nonzero(as_tuple=True)[0]\n",
    "        in_test_idx = inds_to_choose[torch.randint(low = 0, high = inds_to_choose.shape[0], size = (1,))]\n",
    "        chosen = self.test_index[in_test_idx.item()]\n",
    "        return self.graphs[chosen], self.explanations[chosen]\n",
    "    def get_graph_as_networkx(self, graph_idx):\n",
    "        '''\n",
    "        Get a given graph as networkx graph\n",
    "        '''\n",
    "        g = self.graphs[graph_idx]\n",
    "        return to_networkx_conv(g, node_attrs = ['x'], to_undirected=True)\n",
    "    def download(self):\n",
    "        pass\n",
    "    def __getitem__(self, idx):\n",
    "        return self.graphs[idx], self.explanations[idx]\n",
    "    def __len__(self):\n",
    "        return len(self.graphs)\n",
    "\n",
    "def check_random_state(seed):\n",
    "    \"\"\"Turn seed into a np.random.RandomState instance\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    seed : None, int or instance of RandomState\n",
    "        If seed is None, return the RandomState singleton used by np.random.\n",
    "        If seed is an int, return a new RandomState instance seeded with seed.\n",
    "        If seed is already a RandomState instance, return it.\n",
    "        Otherwise raise ValueError.\n",
    "    \"\"\"\n",
    "    if seed is None or seed is np.random:\n",
    "        return np.random.mtrand._rand\n",
    "    if isinstance(seed, numbers.Integral):\n",
    "        return np.random.RandomState(seed)\n",
    "    if isinstance(seed, np.random.RandomState):\n",
    "        return seed\n",
    "    raise ValueError('%r cannot be used to seed a numpy.random.RandomState'\n",
    "                     ' instance' % seed)\n",
    "                     \n",
    "def _generate_hypercube(samples, dimensions, rng):\n",
    "    \"\"\"\n",
    "    Returns distinct binary samples of length dimensions.\n",
    "    \"\"\"\n",
    "    if dimensions > 30:\n",
    "        return np.hstack([rng.randint(2, size=(samples, dimensions - 30)),\n",
    "                          _generate_hypercube(samples, 30, rng)])\n",
    "    out = sample_without_replacement(2 ** dimensions, samples,\n",
    "                                     random_state=rng).astype(dtype='>u4', copy=False)\n",
    "    out = np.unpackbits(out.view('>u1')).reshape((-1, 32))[:, -dimensions:]\n",
    "    return out\n",
    "\n",
    "\n",
    "def make_structured_feature(y: torch.Tensor, n_features=5, n_informative=2,\n",
    "                            n_redundant=0, n_repeated=0, n_clusters_per_class=2,\n",
    "                            unique_explanation=True, flip_y=0.01,\n",
    "                            class_sep=1.0, hypercube=True, shift=0.0, scale=1.0,\n",
    "                            shuffle=True, seed=None):\n",
    "    \"\"\"This function is based on sklearn.datasets.make_classification.\n",
    "\n",
    "    Generate structured features for the given labels.\n",
    "\n",
    "    This initially creates clusters of points normally distributed (std=1)\n",
    "    about vertices of an ``n_informative``-dimensional hypercube with sides of\n",
    "    length ``2*class_sep`` and assigns an equal number of clusters to each\n",
    "    class. It introduces interdependence between these features and adds\n",
    "    various types of further noise to the data.\n",
    "\n",
    "    Without shuffling, ``X`` horizontally stacks features in the following\n",
    "    order: the primary ``n_informative`` features, followed by ``n_redundant``\n",
    "    linear combinations of the informative features, followed by ``n_repeated``\n",
    "    duplicates, drawn randomly with replacement from the informative and\n",
    "    redundant features. The remaining features are filled with random noise.\n",
    "    Thus, without shuffling, all useful features are contained in the columns\n",
    "    ``X[:, :n_informative + n_redundant + n_repeated]``.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : ndarray of shape (n_samples,)\n",
    "        The integer labels for class membership of each sample.\n",
    "\n",
    "    n_samples : int, default=100\n",
    "        The number of samples.\n",
    "\n",
    "    n_features : int, default=20\n",
    "        The total number of features. These comprise ``n_informative``\n",
    "        informative features, ``n_redundant`` redundant features,\n",
    "        ``n_repeated`` duplicated features and\n",
    "        ``n_features-n_informative-n_redundant-n_repeated`` useless features\n",
    "        drawn at random.\n",
    "\n",
    "    n_informative : int, default=2\n",
    "        The number of informative features. Each class is composed of a number\n",
    "        of gaussian clusters each located around the vertices of a hypercube\n",
    "        in a subspace of dimension ``n_informative``. For each cluster,\n",
    "        informative features are drawn independently from  N(0, 1) and then\n",
    "        randomly linearly combined within each cluster in order to add\n",
    "        covariance. The clusters are then placed on the vertices of the\n",
    "        hypercube.\n",
    "\n",
    "    n_redundant : int, default=2\n",
    "        The number of redundant features. These features are generated as\n",
    "        random linear combinations of the informative features.\n",
    "\n",
    "    n_repeated : int, default=0\n",
    "        The number of duplicated features, drawn randomly from the informative\n",
    "        and the redundant features.\n",
    "\n",
    "    n_clusters_per_class : int, default=2\n",
    "        The number of clusters per class.\n",
    "\n",
    "    flip_y : float, default=0.01\n",
    "        The fraction of samples whose class is assigned randomly. Larger\n",
    "        values introduce noise in the labels and make the classification\n",
    "        task harder. Note that the default setting flip_y > 0 might lead\n",
    "        to less than ``n_classes`` in y in some cases.\n",
    "\n",
    "    class_sep : float, default=1.0\n",
    "        The factor multiplying the hypercube size.  Larger values spread\n",
    "        out the clusters/classes and make the classification task easier.\n",
    "\n",
    "    hypercube : bool, default=True\n",
    "        If True, the clusters are put on the vertices of a hypercube. If\n",
    "        False, the clusters are put on the vertices of a random polytope.\n",
    "\n",
    "    shift : float, ndarray of shape (n_features,) or None, default=0.0\n",
    "        Shift features by the specified value. If None, then features\n",
    "        are shifted by a random value drawn in [-class_sep, class_sep].\n",
    "\n",
    "    scale : float, ndarray of shape (n_features,) or None, default=1.0\n",
    "        Multiply features by the specified value. If None, then features\n",
    "        are scaled by a random value drawn in [1, 100]. Note that scaling\n",
    "        happens after shifting.\n",
    "\n",
    "    shuffle : bool, default=True\n",
    "        Shuffle the samples and the features.\n",
    "\n",
    "    seed : int, RandomState instance or None, default=None\n",
    "        Determines random number generation for dataset creation. Pass an int\n",
    "        for reproducible output across multiple function calls.\n",
    "        See :term:`Glossary <random_state>`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X : ndarray of shape (n_samples, n_features)\n",
    "        The generated samples.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The algorithm is adapted from Guyon [1] and was designed to generate\n",
    "    the \"Madelon\" dataset.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] I. Guyon, \"Design of experiments for the NIPS 2003 variable\n",
    "           selection benchmark\", 2003.\n",
    "\n",
    "    See Also\n",
    "    --------\n",
    "    make_blobs : Simplified variant.\n",
    "    make_multilabel_classification : Unrelated rng for multilabel tasks.\n",
    "    \"\"\"\n",
    "\n",
    "    Yorg = y.clone().numpy()\n",
    "\n",
    "    if isinstance(y, torch.Tensor):\n",
    "        y = y.clone().numpy()\n",
    "\n",
    "    n_samples = y.shape[0]\n",
    "    labels, n_samples_per_class = np.unique(y, return_counts=True)\n",
    "    n_classes = len(labels)\n",
    "\n",
    "    rng = check_random_state(seed)\n",
    "\n",
    "    # Set n_redundant and n_repeated to 0 if unique explanation\n",
    "    if unique_explanation:\n",
    "        n_redundant = n_repeated = 0\n",
    "\n",
    "    # Count features, clusters and samples\n",
    "    if n_informative + n_redundant + n_repeated > n_features:\n",
    "        raise ValueError(\"Number of informative, redundant and repeated \"\n",
    "                         \"features must sum to less than the number of total\"\n",
    "                         \" features\")\n",
    "    # Use log2 to avoid overflow errors\n",
    "    if n_informative < np.log2(n_classes * n_clusters_per_class):\n",
    "        msg = \"n_classes({}) * n_clusters_per_class({}) must be\"\n",
    "        msg += \" smaller or equal 2**n_informative({})={}\"\n",
    "        raise ValueError(msg.format(n_classes, n_clusters_per_class,\n",
    "                                    n_informative, 2**n_informative))\n",
    "\n",
    "    n_useless = n_features - n_informative - n_redundant - n_repeated\n",
    "    n_clusters = n_classes * n_clusters_per_class\n",
    "\n",
    "    # Distribute samples among clusters\n",
    "    n_samples_per_cluster = [\n",
    "        int(n_samples_per_class[k % n_classes] / n_clusters_per_class)\n",
    "        for k in range(n_clusters)\n",
    "    ]\n",
    "\n",
    "    for i in range(n_samples - sum(n_samples_per_cluster)):\n",
    "        n_samples_per_cluster[i % n_clusters] += 1\n",
    "\n",
    "    # Initialize X\n",
    "    X = np.zeros((n_samples, n_features))\n",
    "\n",
    "    # Build the polytope whose vertices become cluster centroids\n",
    "    centroids = _generate_hypercube(n_clusters, n_informative,\n",
    "                                    rng).astype(float, copy=False)\n",
    "\n",
    "    centroids *= 2 * class_sep\n",
    "    centroids -= class_sep\n",
    "    if not hypercube:\n",
    "        centroids *= rng.rand(n_clusters, 1)\n",
    "        centroids *= rng.rand(1, n_informative)\n",
    "\n",
    "    # Initially draw informative features from the standard normal\n",
    "    X[:, :n_informative] = rng.randn(n_samples, n_informative)\n",
    "\n",
    "    # Create each cluster; a variant of make_blobs\n",
    "    stop = 0\n",
    "    for k, centroid in enumerate(centroids):\n",
    "        start, stop = stop, stop + n_samples_per_cluster[k]\n",
    "        y[start:stop] = k % n_classes  # assign labels\n",
    "        X_k = X[start:stop, :n_informative]  # slice a view of the cluster\n",
    "\n",
    "        A = 2 * rng.rand(n_informative, n_informative) - 1\n",
    "        X_k[...] = np.dot(X_k, A)  # introduce random covariance\n",
    "\n",
    "        X_k += centroid  # shift the cluster to a vertex\n",
    "        #print('k', k)\n",
    "\n",
    "    # Create redundant features\n",
    "    if n_redundant > 0:\n",
    "        B = 2 * rng.rand(n_informative, n_redundant) - 1\n",
    "        X[:, n_informative:n_informative + n_redundant] = \\\n",
    "            np.dot(X[:, :n_informative], B)\n",
    "\n",
    "    # Repeat some features\n",
    "    if n_repeated > 0:\n",
    "        n = n_informative + n_redundant\n",
    "        indices = ((n - 1) * rng.rand(n_repeated) + 0.5).astype(np.intp)\n",
    "        X[:, n:n + n_repeated] = X[:, indices]\n",
    "\n",
    "    # Fill useless features\n",
    "    if n_useless > 0:\n",
    "        X[:, -n_useless:] = rng.randn(n_samples, n_useless)\n",
    "\n",
    "    # Randomly replace labels\n",
    "    if flip_y >= 0.0:\n",
    "        flip_mask = rng.rand(n_samples) < flip_y\n",
    "        y[flip_mask] = rng.randint(n_classes, size=flip_mask.sum())\n",
    "\n",
    "    # Randomly shift and scale\n",
    "    if shift is None:\n",
    "        shift = (2 * rng.rand(n_features) - 1) * class_sep\n",
    "    X += shift\n",
    "\n",
    "    if scale is None:\n",
    "        scale = 1 + 100 * rng.rand(n_features)\n",
    "    X *= scale\n",
    "\n",
    "    # The binary feature mask (1 for informative features) if unique explanation\n",
    "    if unique_explanation:\n",
    "        feature_mask = np.zeros(n_features, dtype=bool)\n",
    "        feature_mask[:n_informative] = True\n",
    "\n",
    "    #print('y before shuffle', y)\n",
    "\n",
    "    if shuffle:\n",
    "        # Randomly permute features\n",
    "        indices = np.arange(n_features)\n",
    "        rng.shuffle(indices)\n",
    "        X[:, :] = X[:, indices]\n",
    "        #y = np.array([y[i] for i in indices])\n",
    "        if unique_explanation:\n",
    "            feature_mask[:] = feature_mask[indices]\n",
    "\n",
    "    unique_y = np.sort(np.unique(Yorg))\n",
    "\n",
    "    #print('y', y)\n",
    "\n",
    "    #print(unique_y)\n",
    "    #ysort = np.sort(y)\n",
    "\n",
    "    Xnew = np.zeros_like(X)\n",
    "    \n",
    "    for yval in unique_y:\n",
    "        ingenerated = np.argwhere(y == yval).flatten()\n",
    "        #print('ingenerated', ingenerated)\n",
    "        inorg = np.argwhere(Yorg == yval).flatten()\n",
    "        #print('inorg', inorg)\n",
    "\n",
    "        for gen, org in zip(ingenerated, inorg): # Move \n",
    "            Xnew[org, :] = X[gen, :]\n",
    "\n",
    "    #print(Xnew[:10, :])\n",
    "\n",
    "    # Convert to tensor\n",
    "    Xnew = torch.from_numpy(Xnew).float()\n",
    "    feature_mask = torch.from_numpy(feature_mask)\n",
    "\n",
    "    if unique_explanation:\n",
    "        return Xnew, feature_mask\n",
    "    else:\n",
    "        return Xnew\n",
    "    \n",
    "def BBG_old(\n",
    "        shape: Optional[nx.Graph] = house, \n",
    "        num_subgraphs: Optional[int] = 5, \n",
    "        inter_sg_connections: Optional[int] = 1,\n",
    "        prob_connection: Optional[float] = 1,\n",
    "        num_hops: Optional[int] = 2,\n",
    "        base_graph: Optional[str] = 'ba',\n",
    "        seed = None,\n",
    "        **kwargs,\n",
    "        ) -> nx.Graph:\n",
    "    '''\n",
    "    Creates a synthetic graph with one or two motifs within a given neighborhood and\n",
    "        then labeling nodes based on the number of motifs around them. \n",
    "    Can be thought of as building unique explanations for each node, with either one\n",
    "        or two motifs being the explanation.\n",
    "    Args:\n",
    "        shape (nx.Graph, optional): Motif to be inserted.\n",
    "        num_subgraphs (int, optional): Number of initial subgraphs to create. Roughly\n",
    "            controls number of nodes in the graph.\n",
    "        inter_sg_connections (int, optional): How many connections to be made between\n",
    "            subgraphs. Higher value will create more inter-connected graph. \n",
    "        prob_connection (float, optional): Probability of making connection between \n",
    "            subgraphs. Can introduce sparsity and stochasticity to graph generation.\n",
    "        num_hops (int, optional): Number of hops to consider for labeling a node.\n",
    "        base_graph (str, optional): Base graph algorithm used to generate each subgraph.\n",
    "            Options are `'ba'` (Barabasi-Albert) (:default: :obj:`'ba'`)\n",
    "    '''\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "    # Create graph:\n",
    "    if base_graph == 'ba':\n",
    "        if 'n_ba' in kwargs:\n",
    "            subgraph_generator = partial(nx.barabasi_albert_graph, n=kwargs['n_ba'], m=1)\n",
    "        else:\n",
    "            subgraph_generator = partial(nx.barabasi_albert_graph, n=5 * num_hops, m=1)\n",
    "\n",
    "    subgraphs = []\n",
    "    shape_node_per_subgraph = []\n",
    "    original_shapes = []\n",
    "    floor_counter = 0\n",
    "    shape_number = 1\n",
    "    for i in range(num_subgraphs):\n",
    "        current_shape = shape.copy()\n",
    "        #nx.set_node_attributes(current_shape, 1, 'shape')\n",
    "        #nx.set_node_attributes(current_shape, shape_number, 'shape_number')\n",
    "        nx.set_node_attributes(current_shape, shape_number, 'shape')\n",
    "\n",
    "        s = subgraph_generator()\n",
    "        relabeler = {ns: floor_counter + ns for ns in s.nodes}\n",
    "        s = nx.relabel.relabel_nodes(s, relabeler)\n",
    "        nx.set_node_attributes(s, 0, 'shape')\n",
    "        #nx.set_node_attributes(s, 0, 'shape_number')\n",
    "\n",
    "        # Join s and shape together:\n",
    "        to_pivot = random.choice(list(shape.nodes))\n",
    "        pivot = random.choice(list(s.nodes))\n",
    "\n",
    "        shape_node_per_subgraph.append(pivot) # This node represents the shape in the graph\n",
    "\n",
    "        convert = {to_pivot: pivot}\n",
    "\n",
    "        mx_nodes = max(list(s.nodes))\n",
    "        i = 1\n",
    "        for n in current_shape.nodes:\n",
    "            if not (n == to_pivot):\n",
    "                convert[n] = mx_nodes + i\n",
    "            i += 1\n",
    "\n",
    "        current_shape = nx.relabel.relabel_nodes(current_shape, convert)\n",
    "        \n",
    "        s.add_nodes_from(current_shape.nodes(data=True))\n",
    "        s.add_edges_from(current_shape.edges)\n",
    "\n",
    "        # Find k-hop from pivot:\n",
    "        in_house = khop_subgraph_nx(node_idx = pivot, num_hops = num_hops, G = s)\n",
    "        s.remove_nodes_from(set(s.nodes) - set(in_house) - set(current_shape.nodes))\n",
    "        nx.set_node_attributes(s, 1, 'shapes_in_khop')\n",
    "\n",
    "        # Ensure that pivot is assigned to proper shape:\n",
    "        #s.nodes[pivot]['shape_number'] = shape_number\n",
    "        s.nodes[pivot]['shape'] = shape_number\n",
    "\n",
    "\n",
    "        subgraphs.append(s.copy())\n",
    "        floor_counter = max(list(s.nodes)) + 1\n",
    "        original_shapes.append(current_shape.copy())\n",
    "\n",
    "        shape_number += 1\n",
    "\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    for i in range(len(subgraphs)):\n",
    "        G.add_edges_from(subgraphs[i].edges)\n",
    "        G.add_nodes_from(subgraphs[i].nodes(data=True))\n",
    "\n",
    "    G = G.to_undirected()\n",
    "\n",
    "    # Join subgraphs via inner-subgraph connections\n",
    "    for i in range(len(subgraphs)):\n",
    "        for j in range(i + 1, len(subgraphs)):\n",
    "            #if i == j: # Don't connect the same subgraph\n",
    "            #    continue\n",
    "\n",
    "            s = subgraphs[i]\n",
    "            # Try to make connections between subgraphs i, j:\n",
    "            for k in range(inter_sg_connections):\n",
    "\n",
    "                # Screen whether to try to make a connection:\n",
    "                if np.random.rand() > prob_connection:\n",
    "                    continue\n",
    "\n",
    "                x, y = np.meshgrid(list(subgraphs[i].nodes), list(subgraphs[j].nodes))\n",
    "                possible_edges = list(zip(x.flatten(), y.flatten()))\n",
    "\n",
    "                rand_edge = None\n",
    "\n",
    "                tempG = G.copy()\n",
    "\n",
    "                while len(possible_edges) > 0:\n",
    "\n",
    "                    rand_edge = random.choice(possible_edges)\n",
    "                    possible_edges.remove(rand_edge) # Remove b/c we're searching this edge possibility\n",
    "\n",
    "                    # Make edge between the two:\n",
    "                    tempG.add_edge(rand_edge[0], rand_edge[1])\n",
    "                    tempG.add_edge(rand_edge[1], rand_edge[0])\n",
    "                    #print('rand_edge 1', rand_edge)\n",
    "\n",
    "                    khop_union = set()\n",
    "\n",
    "                    # Constant number of t's for each (10)\n",
    "                    for t in list(original_shapes[i].nodes) + list(original_shapes[j].nodes):\n",
    "                        khop_union = khop_union.union(set(khop_subgraph_nx(node_idx = t, num_hops = num_hops, G = tempG)))\n",
    "\n",
    "                    incr_ret = incr_on_unique_houses(\n",
    "                        nodes_to_search = list(khop_union),   \n",
    "                        G = tempG, \n",
    "                        num_hops = num_hops, \n",
    "                        attr_measure = 'shapes_in_khop', \n",
    "                        lower_bound = 1, \n",
    "                        upper_bound = 2)\n",
    "\n",
    "                    if incr_ret is None:\n",
    "                        #print('rand_edge 2', rand_edge)\n",
    "                        tempG.remove_edge(rand_edge[0], rand_edge[1])\n",
    "                        #tempG.remove_edge(rand_edge[1], rand_edge[0])\n",
    "\n",
    "                        rand_edge = None\n",
    "                        continue\n",
    "                    else:\n",
    "                        tempG = incr_ret\n",
    "                        break\n",
    "\n",
    "                if rand_edge is not None: # If we found a valid edge\n",
    "                    #print('Made change')\n",
    "                    G = tempG.copy()\n",
    "\n",
    "    # Ensure that G is connected\n",
    "    G = G.subgraph(sorted(nx.connected_components(G), key = len, reverse = True)[0])\n",
    "\n",
    "\n",
    "    # Renumber nodes to be constantly increasing integers starting from 0\n",
    "    mapping = {n:i for i, n in enumerate(G.nodes)}\n",
    "    G = nx.relabel_nodes(G, mapping = mapping, copy = True)\n",
    "\n",
    "    return G\n",
    "\n",
    "\n",
    "def incr_on_unique_houses(nodes_to_search, G, num_hops, attr_measure, lower_bound, upper_bound):\n",
    "    #G = G.copy()\n",
    "\n",
    "    incr_tuples = {}\n",
    "\n",
    "    for n in nodes_to_search:\n",
    "        khop = khop_subgraph_nx(node_idx = n, num_hops = num_hops, G = G)\n",
    "\n",
    "        #unique_shapes = torch.unique(torch.tensor([G.nodes[i]['shape_number'] for i in khop]))\n",
    "        unique_shapes = torch.unique(torch.tensor([G.nodes[i]['shape'] for i in khop]))\n",
    "        num_unique = unique_shapes.shape[0] - 1 if 0 in unique_shapes else unique_shapes.shape[0]\n",
    "\n",
    "        if num_unique < lower_bound or num_unique > upper_bound:\n",
    "            return None\n",
    "        else:\n",
    "\n",
    "            incr_tuples[n] = (num_unique, unique_shapes)\n",
    "\n",
    "            # G.nodes[n][attr_measure] = num_unique\n",
    "            # G.nodes[n]['nearby_shapes'] = unique_shapes\n",
    "\n",
    "    for k, v in incr_tuples.items():\n",
    "        G.nodes[k][attr_measure] = v[0]\n",
    "        G.nodes[k]['nearby_shapes'] = v[1]\n",
    "\n",
    "    return G\n",
    "\n",
    "def ba_around_shape(shape: nx.Graph, add_size: int, show_subgraphs: bool = False):\n",
    "    '''\n",
    "    Incrementally adds nodes around a shape in a Barabasi-Albert style\n",
    "\n",
    "    Args:\n",
    "        shape (nx.Graph): Shape on which to start the subgraph.\n",
    "        add_size (int): Additional size of the subgraph, i.e. number of\n",
    "            nodes to add to the shape to create full subgraph.\n",
    "        show_subgraphs (bool, optional): If True, shows each subgraph\n",
    "            through nx.draw. (:default: :obj:`False`)\n",
    "    '''\n",
    "    # Get degree, probability distribution of shape\n",
    "\n",
    "    original_nodes = set(shape.nodes())\n",
    "\n",
    "    def get_dist():\n",
    "        degs = [d for n, d in shape.degree() if n in original_nodes]\n",
    "        total_degree = sum(degs)\n",
    "        dist = [degs[i]/total_degree for i in range(len(degs))]\n",
    "        return dist\n",
    "\n",
    "    node_list = list(shape.nodes())\n",
    "    top_nodes = max(node_list)\n",
    "\n",
    "    for i in range(add_size):\n",
    "        # Must connect to only nodes within original graph\n",
    "        connect_node = np.random.choice(node_list, p = get_dist())\n",
    "        new_node = top_nodes + i + 1\n",
    "        shape.add_node(new_node)\n",
    "        shape.add_edge(connect_node, new_node) # Just add one edge b/c shape is undirected\n",
    "        shape.nodes[new_node]['shape'] = 0 # Set to zero because its not in a shape\n",
    "    \n",
    "    if show_subgraphs:\n",
    "        c = [int(not (i in node_list)) for i in shape.nodes]\n",
    "        nx.draw(shape, node_color = c, cmap = 'brg')\n",
    "        plt.show()\n",
    "\n",
    "    return shape\n",
    "\n",
    "def BBG_PA(\n",
    "        shape: Optional[nx.Graph] = house, \n",
    "        num_subgraphs: Optional[int] = 5, \n",
    "        prob_connection: Optional[float] = 1,\n",
    "        subgraph_size: int = 13,\n",
    "        seed: int = None,\n",
    "        **kwargs,\n",
    "        ) -> nx.Graph:\n",
    "    '''\n",
    "    Creates a synthetic graph with one or two motifs within a given neighborhood and\n",
    "        then labeling nodes based on the number of motifs around them. \n",
    "    Can be thought of as building unique explanations for each node, with either one\n",
    "        or two motifs being the explanation.\n",
    "    Args:\n",
    "        shape (nx.Graph, optional): Motif to be inserted.\n",
    "        num_subgraphs (int, optional): Number of initial subgraphs to create. Roughly\n",
    "            controls number of nodes in the graph.\n",
    "        prob_connection (float, optional): Probability of making connection between \n",
    "            subgraphs. Can introduce sparsity and stochasticity to graph generation.\n",
    "        kwargs: Optional arguments\n",
    "            show_subgraphs (bool): If True, shows each subgraph that is generated during\n",
    "                initial subgraph generation. (:default: :obj:`False`)\n",
    "    '''\n",
    "\n",
    "    subgraphs = []\n",
    "    original_shapes = []\n",
    "    floor_counter = 0\n",
    "    shape_number = 1\n",
    "\n",
    "    # Option to show individual subgraphs\n",
    "    show_subgraphs = False if ('show_subgraphs' not in kwargs) or num_subgraphs > 10 else kwargs['show_subgraphs']\n",
    "\n",
    "    nodes_in_shape = shape.number_of_nodes()\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    #torch.seed(seed)\n",
    "\n",
    "    for i in range(num_subgraphs):\n",
    "        current_shape = shape.copy()\n",
    "\n",
    "        nx.set_node_attributes(current_shape, shape_number, 'shape')\n",
    "\n",
    "        relabeler = {ns: floor_counter + ns for ns in current_shape.nodes}\n",
    "        current_shape = nx.relabel.relabel_nodes(current_shape, relabeler)\n",
    "        original_shapes.append(current_shape.copy())\n",
    "\n",
    "        subi_size = np.random.poisson(lam = subgraph_size - nodes_in_shape)\n",
    "        s = ba_around_shape(current_shape, add_size = subi_size, show_subgraphs = show_subgraphs)\n",
    "\n",
    "        # All nodes have one shape in their k-hop (guaranteed by building procedure)\n",
    "        nx.set_node_attributes(s, 1, 'shapes_in_khop')\n",
    "\n",
    "        # Append a copy of subgraph to subgraphs vector\n",
    "        subgraphs.append(s.copy())\n",
    "\n",
    "        # Increment floor counter and shape number:\n",
    "        floor_counter = max(list(s.nodes)) + 1\n",
    "        shape_number += 1\n",
    "\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    for i in range(len(subgraphs)):\n",
    "        G.add_edges_from(subgraphs[i].edges)\n",
    "        G.add_nodes_from(subgraphs[i].nodes(data=True))\n",
    "\n",
    "    G = G.to_undirected()\n",
    "\n",
    "    # Make list of possible connections between subgraphs:\n",
    "    connections = np.array(list(itertools.combinations(np.arange(len(subgraphs)), r = 2)))\n",
    "    sample_mask = np.random.binomial(n=2, p = prob_connection, size = len(connections)).astype(bool)\n",
    "    iter_edges = connections[sample_mask]\n",
    "\n",
    "    # Join subgraphs via inner-subgraph connections\n",
    "    for i, j in tqdm.tqdm(iter_edges):\n",
    "        # Try to make connections between subgraphs i, j:\n",
    "\n",
    "        x, y = np.meshgrid(list(subgraphs[i].nodes), list(subgraphs[j].nodes))\n",
    "        possible_edges = list(zip(x.flatten(), y.flatten()))\n",
    "\n",
    "        # Create preferential attachment distribution: -------------------\n",
    "        deg_dist = np.array([(subgraphs[i].degree(ni) + subgraphs[j].degree(nj)) for ni, nj in possible_edges])\n",
    "        running_mask = np.ones(deg_dist.shape[0])\n",
    "        indices_to_choose = np.arange(len(possible_edges))\n",
    "        # ----------------------------------------------------------------\n",
    "\n",
    "        rand_edge = None\n",
    "\n",
    "        #tempG = G.copy()\n",
    "\n",
    "        while np.sum(running_mask) > 0:\n",
    "\n",
    "            # -----------\n",
    "            rand_i = np.random.choice(indices_to_choose, p = deg_dist / np.sum(deg_dist))\n",
    "            rand_edge = possible_edges[rand_i]\n",
    "            old_deg = deg_dist[rand_i]\n",
    "            running_mask[rand_i] = 0\n",
    "\n",
    "            if np.sum(running_mask) > 0:\n",
    "                deg_dist = (deg_dist + old_deg/np.sum(running_mask) * running_mask) * running_mask\n",
    "            # -----------\n",
    "\n",
    "            # Make edge between the two:\n",
    "            # tempG.add_edge(rand_edge[0], rand_edge[1])\n",
    "            # tempG.add_edge(rand_edge[1], rand_edge[0])\n",
    "            G.add_edge(rand_edge[0], rand_edge[1])\n",
    "            #print('rand_edge 1', rand_edge)\n",
    "\n",
    "            khop_union = set()\n",
    "\n",
    "            # Constant number of t's for each (10)\n",
    "            for t in list(original_shapes[i].nodes) + list(original_shapes[j].nodes):\n",
    "                khop_union = khop_union.union(set(khop_subgraph_nx(node_idx = t, num_hops = 1, G = G)))\n",
    "\n",
    "            incr_ret = incr_on_unique_houses(\n",
    "                nodes_to_search = list(khop_union),   \n",
    "                G = G, \n",
    "                num_hops = 1, \n",
    "                attr_measure = 'shapes_in_khop', \n",
    "                lower_bound = 1, \n",
    "                upper_bound = 2)\n",
    "\n",
    "            if incr_ret is None:\n",
    "                #print('rand_edge 2', rand_edge)\n",
    "                #empG.remove_edge(rand_edge[0], rand_edge[1])\n",
    "                G.remove_edge(rand_edge[0], rand_edge[1])\n",
    "                #tempG.remove_edge(rand_edge[1], rand_edge[0])\n",
    "\n",
    "                rand_edge = None\n",
    "                continue\n",
    "            else:\n",
    "                #tempG = incr_ret\n",
    "                G = incr_ret\n",
    "                break\n",
    "\n",
    "    # Ensure that G is connected\n",
    "    G = G.subgraph(sorted(nx.connected_components(G), key = len, reverse = True)[0])\n",
    "\n",
    "    # Renumber nodes to be constantly increasing integers starting from 0\n",
    "    mapping = {n:i for i, n in enumerate(G.nodes)}\n",
    "    G = nx.relabel_nodes(G, mapping = mapping, copy = True)\n",
    "\n",
    "    return G\n",
    "    \n",
    "def verify_motifs(G: nx.Graph, motif_subgraph: nx.Graph):\n",
    "    '''\n",
    "    Verifies that all motifs within a graph are \"good\" motifs\n",
    "        i.e. they were planted by the building algorithm\n",
    "\n",
    "    Args:\n",
    "        G (nx.Graph): Networkx graph on which to search.\n",
    "        motif_subgraph (nx.Graph): Motif to search for (query graph).\n",
    "\n",
    "    Returns:\n",
    "        :rtype: :obj:`bool`\n",
    "        False if there exists at least one \"bad\" shape\n",
    "        True if all motifs/shapes in the graph were planted\n",
    "    '''\n",
    "\n",
    "    matcher = nx.algorithms.isomorphism.ISMAGS(graph = G, subgraph = motif_subgraph)\n",
    "\n",
    "    for iso in matcher.find_isomorphisms():\n",
    "        nodes_found = iso.keys()\n",
    "        shapes = [G.nodes[n]['shape'] for n in nodes_found]\n",
    "\n",
    "        if (sum([int(shapes[i] != shapes[i-1]) for i in range(1, len(shapes))]) > 0) \\\n",
    "            or (sum(shapes) == 0):\n",
    "            # Found a bad one\n",
    "            return False\n",
    "\n",
    "    return True\n",
    "\n",
    "def if_edge_exists(edge_index: torch.Tensor, node1: int, node2: int):\n",
    "    '''\n",
    "    Quick lookup for if an edge exists b/w `node1` and `node2`\n",
    "    '''\n",
    "    \n",
    "    p1 = torch.any((edge_index[0,:] == node1) & (edge_index[1,:] == node2))\n",
    "    p2 = torch.any((edge_index[1,:] == node1) & (edge_index[0,:] == node2))\n",
    "\n",
    "    return (p1 or p2).item()\n",
    "\n",
    "    \n",
    "def gaussian_lv_generator(\n",
    "        G: nx.Graph, \n",
    "        yvals: torch.Tensor,  \n",
    "        n_features: int = 10,       \n",
    "        flip_y: float = 0.01,\n",
    "        class_sep: float = 1.0,\n",
    "        n_informative: int = 4,\n",
    "        n_clusters_per_class: int = 2,\n",
    "        seed = None):\n",
    "    '''\n",
    "    Args:\n",
    "        G (nx.Graph): \n",
    "        yvals (torch.Tensor): \n",
    "        seed (seed): (:default: :obj:`None`)\n",
    "    '''\n",
    "\n",
    "    x, feature_imp_true = make_structured_feature(\n",
    "            yvals, \n",
    "            n_features = n_features,\n",
    "            n_informative = n_informative, \n",
    "            flip_y = flip_y,\n",
    "            class_sep=class_sep,\n",
    "            n_clusters_per_class=n_clusters_per_class,\n",
    "            seed = seed)\n",
    "\n",
    "    Gitems = list(G.nodes.items())\n",
    "    node_map = {Gitems[i][0]:i for i in range(G.number_of_nodes())}\n",
    "\n",
    "    def get_feature(node_idx):\n",
    "        return x[node_map[node_idx],:]\n",
    "\n",
    "    return get_feature, feature_imp_true\n",
    "    \n",
    "def motif_id_label(G, num_hops):\n",
    "    '''\n",
    "    Gets labels based on motif label in the neighborhood\n",
    "    '''\n",
    "    def get_label(node_idx):\n",
    "        nodes_in_khop = khop_subgraph_nx(node_idx, num_hops, G)\n",
    "        # For now, sum motif id's in k-hop (min is 0 for no motifs)\n",
    "        motif_in_khop = torch.sum(torch.unique([G.nodes[ni]['motif_id'] for ni in nodes_in_khop])).item()\n",
    "        return torch.tensor(motif_in_khop, dtype=torch.long)\n",
    "\n",
    "    return get_label\n",
    "\n",
    "def binary_feature_label(G, method = 'median'):\n",
    "    '''\n",
    "    Labeling based solely on features, no edge information\n",
    "        - Keywords can be given based on type of labeling split\n",
    "\n",
    "    Args:\n",
    "        G (nx.Graph): Graph on which the nodes are labeled on\n",
    "        method (str): Method by which to split the features\n",
    "    '''\n",
    "    max_node = len(list(G.nodes))\n",
    "    node_attr = nx.get_node_attributes(G, 'x')\n",
    "    if method == 'median':\n",
    "        x1 = [node_attr[i][1] for i in range(max_node)]\n",
    "        split = torch.median(x1).item()\n",
    "    def get_label(node_idx):\n",
    "        return torch.tensor(int(x1[node_idx] > split), dtype=torch.long)\n",
    "\n",
    "    return get_label\n",
    "\n",
    "def number_motif_equal_label(G, num_hops, equal_number=1):\n",
    "    def get_label(node_idx):\n",
    "        nodes_in_khop = khop_subgraph_nx(node_idx, num_hops, G)\n",
    "        num_unique_houses = torch.unique([G.nodes[ni]['shape'] \\\n",
    "            for ni in nodes_in_khop if G.nodes[ni]['shape'] > 0 ]).shape[0]\n",
    "        return torch.tensor(int(num_unique_houses == equal_number), dtype=torch.long)\n",
    "\n",
    "    return get_label\n",
    "\n",
    "def bound_graph_label(G: nx.Graph):\n",
    "    '''\n",
    "    Args:\n",
    "        G (nx.Graph): Graph on which the labels are based on\n",
    "    '''\n",
    "    sh = nx.get_node_attributes(G, 'shapes_in_khop')\n",
    "    def get_label(node_idx):\n",
    "        return torch.tensor(sh[node_idx] - 1, dtype=torch.long)\n",
    "\n",
    "    return get_label\n",
    "\n",
    "def logical_edge_feature_label(G, num_hops = None, feature_method = 'median'):\n",
    "\n",
    "    if feature_method == 'median':\n",
    "        # Calculate median (as for feature):\n",
    "        node_attr = nx.get_node_attributes(G, 'x')\n",
    "        x1 = [node_attr[i][1] for i in range(G.number_of_nodes())]\n",
    "        split = torch.median(x1).item()\n",
    "\n",
    "    def get_label(node_idx):\n",
    "        nodes_in_khop = khop_subgraph_nx(node_idx, num_hops, G)\n",
    "        num_unique_houses = torch.unique([G.nodes[ni]['shape'] \\\n",
    "            for ni in nodes_in_khop if G.nodes[ni]['shape'] > 0 ]).shape[0]\n",
    "        return torch.tensor(int(num_unique_houses == 1 and x1[node_idx] > split), dtype=torch.long)\n",
    "\n",
    "    return get_label\n",
    "    \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_predict\n",
    "\n",
    "class ShapeGGen(NodeDataset):\n",
    "    '''\n",
    "    Full ShapeGGen dataset implementation\n",
    "\n",
    "    ..note:: Flag and circle shapes not yet implemented\n",
    "    \n",
    "    Args:\n",
    "        model_layers (int, optional): Number of layers within the GNN that will\n",
    "            be explained. This defines the extent of the ground-truth explanations\n",
    "            that are created by the method. (:default: :obj:`3`)\n",
    "        shape (str, optional): Type of shape to be inserted into graph.\n",
    "            Options are `'house'`, `'flag'`, `'circle'`, and `'multiple'`. \n",
    "            If `'multiple'`, random shapes are generated to insert into \n",
    "            the graph.\n",
    "            (:default: :obj:`'house'`)\n",
    "        seed (int, optional): Seed for graph generation. (:default: `None`)\n",
    "\n",
    "        TODO: Ensure seed keeps graph generation constant.\n",
    "\n",
    "        kwargs: Additional arguments\n",
    "\n",
    "            Graph Construction:\n",
    "                variant (int): 0 indicates using the old ShapeGGen method, and 1 indicates\n",
    "                    using the new ShapeGGen method (i.e. one with pref. attachment).   \n",
    "                num_subgraphs (int): Number of individual subgraphs to use in order to build\n",
    "                    the graph. Doesn't guarantee size of graph. (:default: :obj:`10`)\n",
    "                prob_connection (float): Probability of making a connection between any two\n",
    "                    of the original subgraphs. Roughly controls sparsity and number of \n",
    "                    class 0 vs. class 1 nodes. (:default: :obj:`1`)\n",
    "                subgraph_size (int): Expected size of each individual subgraph.\n",
    "                base_graph (str): Base graph structure to use for generating subgraphs.\n",
    "                    Only in effect for variant 0. (:default: :obj:`'ba'`)\n",
    "                verify (bool): Verifies that graph does not have any \"bad\" motifs in it.\n",
    "                    (:default: :obj:`True`)\n",
    "                max_tries_verification (int): Maximum number of tries to re-generate a \n",
    "                    graph that contains bad motifs. (:default: :obj:`5`)\n",
    "\n",
    "            Feature attribution:\n",
    "                n_informative (int): Number of informative features, i.e. those that\n",
    "                    are correlated with label. (:default: :obj:`4`)\n",
    "                class_sep (float):\n",
    "                n_features (int):\n",
    "                n_clusters_per_class (int):\n",
    "                homophily_coef (float):\n",
    "\n",
    "            Sensitive feature:\n",
    "                add_sensitive_feature (bool):  Whether to include a sensitive, discrete \n",
    "                    attribute in the node features. If this is true, the total number of\n",
    "                    features will be `n_features + 1`. (:default: :obj:`True`) \n",
    "                attribute_sensitive_feature (bool): Whether to attribute the sensitive\n",
    "                    feature to the label of the dataset. `False` means to generate\n",
    "                    sensitive features randomly (i.e. uncorrelated). \n",
    "                    (:default: :obj:`False`)\n",
    "                sens_attribution_noise (float):\n",
    "            \n",
    "    Members:\n",
    "        G (nx.Graph): Networkx version of the graph for the dataset\n",
    "            - Contains many values per-node: \n",
    "                1. 'shape': which motif a given node is within\n",
    "                2. 'shapes_in_khop': number of motifs within a (num_hops)-\n",
    "                    hop neighborhood of the given node.\n",
    "                    - Note: only for bound graph\n",
    "    '''\n",
    "\n",
    "    def __init__(self,  \n",
    "        model_layers: int = 3,\n",
    "        shape: Union[str, nx.Graph] = 'house',\n",
    "        seed: Optional[int] = None,\n",
    "        make_explanations: Optional[bool] = True,\n",
    "        **kwargs): # TODO: turn the last three arguments into kwargs\n",
    "\n",
    "        super().__init__(name = 'ShapeGGen', num_hops = model_layers)\n",
    "\n",
    "        self.in_shape = []\n",
    "        self.graph = None\n",
    "        self.model_layers = model_layers\n",
    "        self.make_explanations = make_explanations\n",
    "\n",
    "        # Parse kwargs:\n",
    "        self.variant = 1 if 'variant' not in kwargs else kwargs['variant']\n",
    "            # 0 is old, 1 is preferential attachment one\n",
    "        self.num_subgraphs = 10 if 'num_subgraphs' not in kwargs else kwargs['num_subgraphs']\n",
    "        self.prob_connection = 1 if 'prob_connection' not in kwargs else kwargs['prob_connection']\n",
    "        self.subgraph_size = 13 if 'subgraph_size' not in kwargs else kwargs['subgraph_size']\n",
    "        self.base_graph = 'ba' if 'base_graph' not in kwargs else kwargs['base_graph']\n",
    "        self.verify = True if 'verify' not in kwargs else kwargs['verify']\n",
    "        self.max_tries_verification = 5 if 'max_tries_verification' not in kwargs else kwargs['max_tries_verification']\n",
    "\n",
    "        # Feature args:\n",
    "        self.n_informative = 4 if 'n_informative' not in kwargs else kwargs['n_informative']\n",
    "        self.class_sep = 1.0 if 'class_sep' not in kwargs else kwargs['class_sep']\n",
    "        self.n_features = 10 if 'n_features' not in kwargs else kwargs['n_features']\n",
    "        # Note: n_clusters_per_class assumed to be 2 for the publication\n",
    "        self.n_clusters_per_class = 2 if 'n_clusters_per_class' not in kwargs else kwargs['n_clusters_per_class']\n",
    "        self.homophily_coef = None if 'homophily_coef' not in kwargs else kwargs['homophily_coef']\n",
    "\n",
    "        # Sensitive feature:\n",
    "        self.add_sensitive_feature = True if 'add_sensitive_feature' not in kwargs else kwargs['add_sensitive_feature']\n",
    "        self.attribute_sensitive_feature = False if 'attribute_sensitive_feature' not in kwargs else kwargs['attribute_sensitive_feature']\n",
    "        self.sens_attribution_noise = 0.25 if 'sens_attribution_noise' not in kwargs else kwargs['sens_attribution_noise']\n",
    "\n",
    "        self.seed = seed\n",
    "\n",
    "        # Get shape:\n",
    "        self.shape_method = ''\n",
    "        if isinstance(shape, nx.Graph):\n",
    "            self.insert_shape = shape\n",
    "        else:\n",
    "            self.insert_shape = None\n",
    "            shape = shape.lower()\n",
    "            self.shape_method = shape\n",
    "            if shape == 'house':\n",
    "                self.insert_shape = house\n",
    "            elif shape == 'flag':\n",
    "                pass\n",
    "            elif shape == 'circle':\n",
    "                self.insert_shape = pentagon # 5-member ring\n",
    "            assert shape != 'random', 'Multiple shapes not yet supported for bounded graph'\n",
    "\n",
    "        # Build graph:\n",
    "\n",
    "        if self.verify and shape != 'random':\n",
    "            for i in range(self.max_tries_verification):\n",
    "                if self.variant == 0:\n",
    "                    self.G = BBG_old(\n",
    "                        shape = self.insert_shape, \n",
    "                        num_subgraphs = self.num_subgraphs, \n",
    "                        inter_sg_connections = 1,\n",
    "                        prob_connection = self.prob_connection,\n",
    "                        subgraph_size = self.subgraph_size,\n",
    "                        num_hops = 1,\n",
    "                        base_graph = self.base_graph,\n",
    "                        seed = self.seed,\n",
    "                        )\n",
    "\n",
    "                elif self.variant == 1:\n",
    "                    self.G = BBG_PA(\n",
    "                        shape = self.insert_shape, \n",
    "                        num_subgraphs = self.num_subgraphs, \n",
    "                        inter_sg_connections = 1,\n",
    "                        prob_connection = self.prob_connection,\n",
    "                        subgraph_size = self.subgraph_size,\n",
    "                        num_hops = 1,\n",
    "                        seed = self.seed\n",
    "                        )\n",
    "\n",
    "                if verify_motifs(self.G, self.insert_shape):\n",
    "                    # If the motif verification passes\n",
    "                    break\n",
    "            else:\n",
    "                # Raise error if we couldn't generate a valid graph\n",
    "                raise RuntimeError(f'Could not build a valid graph in {self.max_tries_verification} attempts. \\\n",
    "                    \\n Try using different parameters for graph generation or increasing max_tries_verification argument value.')\n",
    "            \n",
    "        else:\n",
    "            if self.variant == 0:\n",
    "                self.G = BBG_old(\n",
    "                        shape = self.insert_shape, \n",
    "                        num_subgraphs = self.num_subgraphs, \n",
    "                        inter_sg_connections = 1,\n",
    "                        prob_connection = self.prob_connection,\n",
    "                        subgraph_size = self.subgraph_size,\n",
    "                        num_hops = 1,\n",
    "                        base_graph = self.base_graph,\n",
    "                        seed = self.seed,\n",
    "                        )\n",
    "            elif self.variant == 1:\n",
    "                self.G = BBG_PA(\n",
    "                    shape = self.insert_shape, \n",
    "                    num_subgraphs = self.num_subgraphs, \n",
    "                    inter_sg_connections = 1,\n",
    "                    prob_connection = self.prob_connection,\n",
    "                    subgraph_size = self.subgraph_size,\n",
    "                    num_hops = 1,\n",
    "                    seed = self.seed\n",
    "                    )\n",
    "\n",
    "        self.num_nodes = self.G.number_of_nodes() # Number of nodes in graph\n",
    "        self.generate_shape_graph() # Performs planting, augmenting, etc.\n",
    "\n",
    "        # Set random splits for size n graph:\n",
    "        range_set = list(range(self.num_nodes))\n",
    "        random.seed(1234) # Seed random before making splits\n",
    "        train_nodes = random.sample(range_set, int(self.num_nodes * 0.7))\n",
    "        test_nodes  = random.sample(range_set, int(self.num_nodes * 0.25))\n",
    "        valid_nodes = random.sample(range_set, int(self.num_nodes * 0.05))\n",
    "\n",
    "        self.fixed_train_mask = torch.tensor([s in train_nodes for s in range_set], dtype=torch.bool)\n",
    "        self.fixed_test_mask = torch.tensor([s in test_nodes for s in range_set], dtype=torch.bool)\n",
    "        self.fixed_valid_mask = torch.tensor([s in valid_nodes for s in range_set], dtype=torch.bool)\n",
    "\n",
    "    def generate_shape_graph(self):\n",
    "        '''\n",
    "        Generates the full graph with the given insertion and planting policies.\n",
    "\n",
    "        :rtype: :obj:`torch_geometric.Data`\n",
    "        Returns:\n",
    "            data (torch_geometric.Data): Entire generated graph.\n",
    "        '''\n",
    "\n",
    "        gen_labels = bound_graph_label(self.G)\n",
    "        y = torch.tensor([gen_labels(i) for i in self.G.nodes], dtype=torch.long)\n",
    "        self.yvals = y.detach().clone() # MUST COPY TO AVOID MAJOR BUGS\n",
    "\n",
    "        gen_features, self.feature_imp_true = gaussian_lv_generator(\n",
    "            self.G, self.yvals, seed = self.seed,\n",
    "            n_features = self.n_features,\n",
    "            class_sep = self.class_sep,\n",
    "            n_informative = self.n_informative,\n",
    "            n_clusters_per_class=self.n_clusters_per_class,\n",
    "        )\n",
    "        x = torch.stack([gen_features(i) for i in self.G.nodes]).float()\n",
    "\n",
    "        if self.add_sensitive_feature:\n",
    "\n",
    "            # Choose sensitive feature randomly\n",
    "            if self.seed is not None:\n",
    "                torch.manual_seed(self.seed)\n",
    "\n",
    "            if self.attribute_sensitive_feature:\n",
    "                print('Adding sensitive attr')\n",
    "                prob_change = (torch.rand((y.shape[0],)) < self.sens_attribution_noise)\n",
    "                sensitive = torch.where(prob_change, torch.logical_not(y.bool()).long(), y).float()\n",
    "            else:\n",
    "                sensitive = torch.randint(low=0, high=2, size = (x.shape[0],)).float()\n",
    "\n",
    "            # Add sensitive attribute to last dimension on x\n",
    "            x = torch.cat([x, sensitive.unsqueeze(1)], dim = 1)\n",
    "            # Expand feature importance and mark last dimension as negative\n",
    "            self.feature_imp_true = torch.cat([self.feature_imp_true, torch.zeros((1,))])\n",
    "\n",
    "            # Shuffle to mix in x:\n",
    "            shuffle_ind = torch.randperm(x.shape[1])\n",
    "            x[:,shuffle_ind] = x.clone()\n",
    "            self.feature_imp_true[shuffle_ind] = self.feature_imp_true.clone()\n",
    "\n",
    "            # Sensitive feature is in the location where the last index was:\n",
    "            self.sensitive_feature = shuffle_ind[-1].item()\n",
    "\n",
    "        else:\n",
    "            self.sensitive_feature = None\n",
    "\n",
    "        edge_index = to_undirected(torch.tensor(list(self.G.edges), dtype=torch.long).t().contiguous())\n",
    "\n",
    "        if self.homophily_coef is not None:\n",
    "            feat_mask = torch.logical_not(self.feature_imp_true)\n",
    "            if self.sensitive_feature is not None:\n",
    "                feat_mask[self.sensitive_feature] = False\n",
    "\n",
    "            x = optimize_homophily(\n",
    "                x = x,\n",
    "                edge_index = edge_index,\n",
    "                label = y,\n",
    "                feature_mask = feat_mask,\n",
    "                homophily_coef = self.homophily_coef,\n",
    "                epochs = 1000,\n",
    "                connected_batch_size = (edge_index.shape[1] // 2),\n",
    "                disconnected_batch_size = math.comb(self.num_nodes, 2) // self.num_nodes\n",
    "            )\n",
    "\n",
    "        for i in sorted(self.G.nodes):\n",
    "            self.G.nodes[i]['x'] = x[i,:].detach().clone() #gen_features(i)\n",
    "\n",
    "        self.graph = Data(\n",
    "            x=x, \n",
    "            y=y,\n",
    "            edge_index = edge_index, \n",
    "            shape = torch.tensor(list(nx.get_node_attributes(self.G, 'shape').values()))\n",
    "        )\n",
    "\n",
    "        # Generate explanations:\n",
    "        if self.make_explanations:\n",
    "            self.explanations = [self.explanation_generator(n) for n in sorted(self.G.nodes)]\n",
    "        else:\n",
    "            self.explanations = None\n",
    "\n",
    "    def explanation_generator(self, node_idx):\n",
    "\n",
    "        # Label node and edge imp based off of each node's proximity to a house\n",
    "\n",
    "        # Find nodes in num_hops\n",
    "        original_in_num_hop = set([self.G.nodes[n]['shape'] for n in khop_subgraph_nx(node_idx, 1, self.G) if self.G.nodes[n]['shape'] != 0])\n",
    "\n",
    "        # Tag all nodes in houses in the neighborhood:\n",
    "        khop_nodes = khop_subgraph_nx(node_idx, self.model_layers, self.G)\n",
    "        node_imp_map = {i:(self.G.nodes[i]['shape'] in original_in_num_hop) for i in khop_nodes}\n",
    "            # Make map between node importance in networkx and in pytorch data\n",
    "\n",
    "        khop_info = k_hop_subgraph(\n",
    "            node_idx,\n",
    "            num_hops = self.model_layers,\n",
    "            edge_index = to_undirected(self.graph.edge_index)\n",
    "        )\n",
    "\n",
    "        node_imp = torch.tensor([node_imp_map[i.item()] for i in khop_info[0]], dtype=torch.double)\n",
    "\n",
    "        # Get edge importance based on edges between any two nodes in motif\n",
    "        in_motif = khop_info[0][node_imp.bool()] # Get nodes in the motif\n",
    "        edge_imp = torch.zeros(khop_info[1].shape[1], dtype=torch.double)\n",
    "        for i in range(khop_info[1].shape[1]):\n",
    "            # Highlight edge connecting two nodes in a motif\n",
    "            if (khop_info[1][0,i] in in_motif) and (khop_info[1][1,i] in in_motif):\n",
    "                edge_imp[i] = 1\n",
    "                continue\n",
    "            \n",
    "            # Make sure that we highlight edges connecting to the source node if that\n",
    "            #   node is not in a motif:\n",
    "            one_edge_in_motif = ((khop_info[1][0,i] in in_motif) or (khop_info[1][1,i] in in_motif))\n",
    "            node_idx_in_motif = (node_idx in in_motif)\n",
    "            one_end_of_edge_is_nidx = ((khop_info[1][0,i] == node_idx) or (khop_info[1][1,i] == node_idx))\n",
    "\n",
    "            if (one_edge_in_motif and one_end_of_edge_is_nidx) and (not node_idx_in_motif):\n",
    "                edge_imp[i] = 1\n",
    "\n",
    "        exp = Explanation(\n",
    "            feature_imp=self.feature_imp_true,\n",
    "            node_imp = node_imp,\n",
    "            edge_imp = edge_imp,\n",
    "            node_idx = node_idx\n",
    "        )\n",
    "        exp.set_enclosing_subgraph(khop_info)\n",
    "\n",
    "        # Return list of single element since ShapeGGen produces unique explanations\n",
    "        return exp\n",
    "\n",
    "\n",
    "    def visualize(self, shape_label = False, ax = None, show = False):\n",
    "        '''\n",
    "        Args:\n",
    "            shape_label (bool, optional): If `True`, labels each node according to whether\n",
    "            it is a member of an inserted motif or not. If `False`, labels each node \n",
    "            according to its y-value. (:default: :obj:`True`)\n",
    "        '''\n",
    "\n",
    "        ax = ax if ax is not None else plt.gca()\n",
    "\n",
    "        Gitems = list(self.G.nodes.items())\n",
    "        node_map = {Gitems[i][0]:i for i in range(self.G.number_of_nodes())}\n",
    "\n",
    "        if shape_label:\n",
    "            y = [int(self.G.nodes[i]['shape'] > 0) for i in range(self.num_nodes)]\n",
    "        else:\n",
    "            ylist = self.graph.y.tolist()\n",
    "            y = [ylist[node_map[i]] for i in self.G.nodes]\n",
    "\n",
    "        node_weights = {i:node_map[i] for i in self.G.nodes}\n",
    "\n",
    "        #pos = nx.kamada_kawai_layout(self.G)\n",
    "        pos = nx.spring_layout(self.G, seed = 1234) # Seed to always be consistent in output\n",
    "        #_, ax = plt.subplots()\n",
    "        nx.draw(self.G, pos, node_color = y, labels = node_weights, ax=ax)\n",
    "        #ax.set_title('ShapeGGen')\n",
    "        #plt.tight_layout()\n",
    "\n",
    "        if show:\n",
    "            plt.show()\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, input_feat, classes):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_feat, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, classes)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "def faithfulness(model, X, G, edge_mask):\n",
    "    # For this metric, smaller = better\n",
    "    # Smaller implies model results on just the explainer graph are closer to the original results\n",
    "    org_vec = model(X, G)\n",
    "    lst = []\n",
    "    for i in range(0, edge_mask.shape[0]):\n",
    "        if edge_mask[i] >= 0.5:\n",
    "            lst.append(i)\n",
    "    g = G[:, lst]\n",
    "    pert_vec = model(X, g)\n",
    "    org_softmax = F.softmax(org_vec, dim=-1)\n",
    "    pert_softmax = F.softmax(pert_vec, dim=-1)\n",
    "    res = 1 - torch.exp(-F.kl_div(org_softmax.log(), pert_softmax, None, None, 'sum')).item()\n",
    "    return res\n",
    "\n",
    "class GNNExplainer(_BaseExplainer):\n",
    "    \"\"\"\n",
    "    GNNExplainer: node only\n",
    "    \"\"\"\n",
    "    def __init__(self, model: torch.nn.Module, coeff: dict = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model (torch.nn.Module): model on which to make predictions\n",
    "                The output of the model should be unnormalized class score.\n",
    "                For example, last layer = CNConv or Linear.\n",
    "            coeff (dict, optional): coefficient of the entropy term and the size term\n",
    "                for learning edge mask and node feature mask\n",
    "                Default setting:\n",
    "                    coeff = {'edge': {'entropy': 1.0, 'size': 0.005},\n",
    "                             'feature': {'entropy': 0.1, 'size': 1.0}}\n",
    "        \"\"\"\n",
    "        super().__init__(model)\n",
    "        if coeff is not None:\n",
    "            self.coeff = coeff\n",
    "        else:\n",
    "            self.coeff = {'edge': {'entropy': 1.0, 'size': 0.005},\n",
    "                          'feature': {'entropy': 0.1, 'size': 1.0}}\n",
    "\n",
    "    def get_explanation_node(self, \n",
    "            node_idx: int, \n",
    "            x: torch.Tensor,                 \n",
    "            edge_index: torch.Tensor,\n",
    "            label: torch.Tensor = None,\n",
    "            num_hops: int = None,\n",
    "            explain_feature: bool = True,\n",
    "            y = None,\n",
    "            forward_kwargs: dict = {}):\n",
    "        \"\"\"\n",
    "        Explain a node prediction.\n",
    "\n",
    "        Args:\n",
    "            node_idx (int): index of the node to be explained\n",
    "            edge_index (torch.Tensor, [2 x m]): edge index of the graph\n",
    "            x (torch.Tensor, [n x d]): node features\n",
    "            label (torch.Tensor, optional, [n x ...]): labels to explain\n",
    "                If not provided, we use the output of the model.\n",
    "            num_hops (int, optional): number of hops to consider\n",
    "                If not provided, we use the number of graph layers of the GNN.\n",
    "            explain_feature (bool): whether to compute the feature mask or not\n",
    "                (:default: :obj:`True`)\n",
    "            forward_kwargs (dict, optional): additional arguments to model.forward\n",
    "                beyond x and edge_index\n",
    "\n",
    "        Returns:\n",
    "            exp (dict):\n",
    "                exp['feature_imp'] (torch.Tensor, [d]): feature mask explanation\n",
    "                exp['edge_imp'] (torch.Tensor): k-hop edge importance\n",
    "            khop_info (4-tuple of torch.Tensor):\n",
    "                0. the nodes involved in the subgraph\n",
    "                1. the filtered `edge_index`\n",
    "                2. the mapping from node indices in `node_idx` to their new location\n",
    "                3. the `edge_index` mask indicating which edges were preserved\n",
    "        \"\"\"\n",
    "        label = self._predict(x.to(device), edge_index.to(device),\n",
    "                              forward_kwargs=forward_kwargs)# if label is None else label\n",
    "        num_hops = self.L if num_hops is None else num_hops\n",
    "\n",
    "        org_eidx = edge_index.clone().to(device)\n",
    "\n",
    "        khop_info = subset, sub_edge_index, mapping, hard_edge_mask = \\\n",
    "            k_hop_subgraph(node_idx, num_hops, edge_index,\n",
    "                           relabel_nodes=True) #num_nodes=x.shape[0])\n",
    "        sub_x = x[subset].to(device)\n",
    "        \n",
    "        self._set_masks(sub_x.to(device), sub_edge_index.to(device), explain_feature=explain_feature)\n",
    "\n",
    "        self.model.eval()\n",
    "        num_epochs = 200\n",
    "\n",
    "        # Loss function for GNNExplainer's objective\n",
    "        def loss_fn(log_prob, mask, mask_type):\n",
    "            # Select the log prob and the label of node_idx\n",
    "            node_log_prob = log_prob[torch.where(subset==node_idx)].squeeze()\n",
    "            node_label = label[mapping]\n",
    "            # Maximize the probability of predicting the label (cross entropy)\n",
    "            loss = -node_log_prob[node_label].item()\n",
    "            a = mask.sigmoid()\n",
    "            # Size regularization\n",
    "            loss += self.coeff[mask_type]['size'] * torch.sum(a)\n",
    "            # Element-wise entropy regularization\n",
    "            # Low entropy implies the mask is close to binary\n",
    "            entropy = -a * torch.log(a + 1e-15) - (1-a) * torch.log(1-a + 1e-15)\n",
    "            loss += self.coeff[mask_type]['entropy'] * entropy.mean()\n",
    "            return loss\n",
    "\n",
    "        def train(mask, mask_type):\n",
    "            optimizer = torch.optim.Adam([mask], lr=0.01)\n",
    "            for epoch in range(1, num_epochs+1):\n",
    "                optimizer.zero_grad()\n",
    "                if mask_type == 'feature':\n",
    "                    h = sub_x.to(device) * mask.view(1, -1).sigmoid().to(device)\n",
    "                else:\n",
    "                    h = sub_x.to(device)\n",
    "                log_prob = self._predict(h.to(device), sub_edge_index.to(device), return_type='log_prob')\n",
    "                loss = loss_fn(log_prob, mask, mask_type)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        feat_imp = None\n",
    "        if explain_feature: # Get a feature mask\n",
    "            train(self.feature_mask, 'feature')\n",
    "            feat_imp = self.feature_mask.data.sigmoid()\n",
    "\n",
    "        train(self.edge_mask, 'edge')\n",
    "        edge_imp = self.edge_mask.data.sigmoid().to(device)\n",
    "\n",
    "        # print('pre activation edge_imp:', edge_imp)\n",
    "\n",
    "        # print('IN GNNEXPLAINER')\n",
    "        # print('edge imp shape', edge_imp.shape)\n",
    "\n",
    "        self._clear_masks()\n",
    "\n",
    "        discrete_edge_mask = (edge_imp > 0.5) # Turn into bool activation because of sigmoid\n",
    "\n",
    "        khop_info = (subset, org_eidx[:,hard_edge_mask], mapping, hard_edge_mask)\n",
    "\n",
    "        exp = Explanation(\n",
    "            feature_imp = feat_imp,\n",
    "            node_imp = node_mask_from_edge_mask(khop_info[0], khop_info[1], edge_mask = discrete_edge_mask),\n",
    "            edge_imp = discrete_edge_mask.float(),\n",
    "            node_idx = node_idx\n",
    "        )\n",
    "\n",
    "        exp.set_enclosing_subgraph(khop_info)\n",
    "\n",
    "        return exp\n",
    "\n",
    "    def get_explanation_graph(self, x, edge_index, forward_kwargs = {}, lr = 0.01, ep=300):\n",
    "        r\"\"\"Learns and returns a node feature mask and an edge mask that play a\n",
    "        crucial role to explain the prediction made by the GNN for a graph.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): The node feature matrix.\n",
    "            edge_index (LongTensor): The edge indices.\n",
    "            **kwargs (optional): Additional arguments passed to the GNN module.\n",
    "\n",
    "        :rtype: (:class:`Tensor`, :class:`Tensor`)\n",
    "        \"\"\"\n",
    "\n",
    "        self.model.eval()\n",
    "        self._clear_masks()\n",
    "\n",
    "        # all nodes belong to same graph\n",
    "        # batch = torch.zeros(x.shape[0], dtype=int, device=x.device)\n",
    "\n",
    "        # Get the initial prediction.\n",
    "        with torch.no_grad():\n",
    "            log_logits = self._predict(x.to(device), edge_index.to(device), forward_kwargs = forward_kwargs, return_type='log_prob')\n",
    "            pred_label = log_logits.argmax(dim=-1)\n",
    "\n",
    "        self._set_masks(x, edge_index, edge_mask = None, explain_feature = True, device = x.device)\n",
    "        parameters = [self.feature_mask, self.edge_mask]\n",
    "        optimizer = torch.optim.Adam(parameters, lr=lr)\n",
    "\n",
    "        def loss_fn(node_idx, log_logits, pred_label):\n",
    "            if node_idx != -1:\n",
    "                loss = -log_logits[node_idx, pred_label[node_idx]]\n",
    "            else:\n",
    "                loss = -log_logits[0, pred_label[0]]\n",
    "\n",
    "            m = self.edge_mask.sigmoid()\n",
    "            #edge_reduce = getattr(torch, self.coeffs['edge_reduction'])\n",
    "            loss = loss + self.coeff['edge']['size'] * torch.sum(m)\n",
    "            ent = -m * torch.log(m + EPS) - (1 - m) * torch.log(1 - m + EPS)\n",
    "            #loss = loss + self.coeffs['edge_ent'] * ent.mean()\n",
    "            loss = loss + self.coeff['edge']['entropy'] * ent.mean()\n",
    "\n",
    "            m = self.feature_mask.sigmoid()\n",
    "            loss = loss + self.coeff['feature']['size'] * torch.sum(m)\n",
    "            ent = -m * torch.log(m + EPS) - (1 - m) * torch.log(1 - m + EPS)\n",
    "            #loss = loss + self.coeffs['node_feat_ent'] * ent.mean()\n",
    "            loss = loss + self.coeff['feature']['entropy'] * ent.mean()\n",
    "\n",
    "            return loss\n",
    "\n",
    "        num_epochs = ep # TODO: make more general\n",
    "        for epoch in range(1, num_epochs + 1):\n",
    "            optimizer.zero_grad()\n",
    "            h = x * self.feature_mask.sigmoid()\n",
    "\n",
    "            log_logits = self._predict(h.to(device), edge_index.to(device), forward_kwargs = forward_kwargs, return_type='log_prob')\n",
    "\n",
    "            loss = loss_fn(-1, log_logits, pred_label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        feature_mask = self.feature_mask.detach().sigmoid().squeeze()\n",
    "        edge_mask = self.edge_mask.detach().sigmoid()\n",
    "\n",
    "        self._clear_masks()\n",
    "\n",
    "        node_imp = node_mask_from_edge_mask(\n",
    "            torch.arange(x.shape[0]).to(x.device), \n",
    "            edge_index, \n",
    "            (edge_mask > 0.5)) # Make edge mask into discrete and convert to node mask\n",
    "        print(node_imp)\n",
    "        print(edge_mask)\n",
    "        exp = Explanation(\n",
    "            feature_imp = feature_mask,\n",
    "            node_imp = node_imp.float(),\n",
    "            edge_imp = edge_mask \n",
    "        )\n",
    "\n",
    "        exp.set_whole_graph(Data(x=x, edge_index=edge_index))\n",
    "\n",
    "        return exp\n",
    "\n",
    "    def get_explanation_link(self):\n",
    "        \"\"\"\n",
    "        Explain a link prediction.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3f08c46-7840-4fcd-882a-9a4c20f0359a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base\n",
      "Random seed set as 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 8697/8697 [01:27<00:00, 98.92it/s] \n"
     ]
    }
   ],
   "source": [
    "data = 'base'\n",
    "m = data\n",
    "lst = []\n",
    "print(data)\n",
    "if data == 'base':\n",
    "    seed = 1\n",
    "    set_seed(seed)\n",
    "    Gr = ShapeGGen(shape = 'house', num_subgraphs = 1200, prob_connection = 0.006, subgraph_size = 11, n_features = 11, n_informative = 4, class_sep = 0.6, \n",
    "                        n_clusters_per_class = 2, sens_attribution_noise = 0.5, homophily_coef = 1)\n",
    "    lr = 0.16\n",
    "    wd = 0.0001\n",
    "elif data == 'hetero':\n",
    "    seed = 1\n",
    "    set_seed(seed)\n",
    "    Gr = ShapeGGen(shape = 'house', num_subgraphs = 1200, prob_connection = 0.006, subgraph_size = 11, n_features = 11, n_informative = 4, class_sep = 0.6, \n",
    "                        n_clusters_per_class = 2, sens_attribution_noise = 0.5, homophily_coef = -1)\n",
    "    lr = 0.1\n",
    "    wd = 5e-5\n",
    "elif data == 'unfair':\n",
    "    seed = 4\n",
    "    set_seed(seed)\n",
    "    Gr = ShapeGGen(shape = 'house', num_subgraphs = 1200, prob_connection = 0.006, subgraph_size = 11, n_features = 11, n_informative = 4, class_sep = 0.6, \n",
    "                        n_clusters_per_class = 2, sens_attribution_noise = 0.75, homophily_coef = 1)\n",
    "    lr = 0.15\n",
    "    wd = 0.0001\n",
    "elif data == 'moreinform':\n",
    "    seed = 400\n",
    "    set_seed(seed)\n",
    "    Gr = ShapeGGen(shape = 'house', num_subgraphs = 1200, prob_connection = 0.006, subgraph_size = 11, n_features = 11, n_informative = 8, class_sep = 0.6, \n",
    "                        n_clusters_per_class = 2, sens_attribution_noise = 0.5, homophily_coef = 1)\n",
    "    lr = 0.05\n",
    "    wd = 0.001\n",
    "elif data == 'lessinform':\n",
    "    seed = 1000\n",
    "    set_seed(seed)\n",
    "    Gr = ShapeGGen(shape = 'house', num_subgraphs = 1200, prob_connection = 0.006, subgraph_size = 11, n_features = 21, n_informative = 4, class_sep = 0.6, \n",
    "                        n_clusters_per_class = 2, sens_attribution_noise = 0.5, homophily_coef = 1)\n",
    "    lr = 0.05\n",
    "    wd = 0.001\n",
    "elif data == 'test':\n",
    "    seed = 1000\n",
    "    set_seed(seed)\n",
    "    Gr = ShapeGGen(shape = 'house', num_subgraphs = 100, prob_connection = 0.006, subgraph_size = 11, n_features = 11, n_informative = 8, class_sep = 0.6, n_clusters_per_class = 2, sens_attribution_noise = 0.5, homophily_coef = 1)\n",
    "    lr = 0.05\n",
    "    wd = 0.001\n",
    "else:\n",
    "    print('Not a dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f1cc4d1-8393-4285-8676-dc4db66b4947",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "graph = Gr.generate_shape_graph()\n",
    "data = Gr.graph\n",
    "train_mask = Gr.fixed_train_mask\n",
    "test_mask = Gr.fixed_test_mask\n",
    "val_mask = Gr.fixed_valid_mask\n",
    "num_classes = len(np.unique(data.y))\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c66333d-d870-483f-bd75-c8318efa744e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (conv1): GCNConv(12, 16)\n",
      "  (conv2): GCNConv(16, 2)\n",
      ")\n",
      "Epoch: 001, Train Acc: 0.6590, Test Acc: 0.6695, Val Acc: 0.6611, Loss: 1.3524\n",
      "Epoch: 002, Train Acc: 0.6591, Test Acc: 0.6698, Val Acc: 0.6611, Loss: 1.2912\n",
      "Epoch: 003, Train Acc: 0.6592, Test Acc: 0.6698, Val Acc: 0.6611, Loss: 1.1078\n",
      "Epoch: 004, Train Acc: 0.6532, Test Acc: 0.6628, Val Acc: 0.6581, Loss: 0.7890\n",
      "Epoch: 005, Train Acc: 0.6585, Test Acc: 0.6668, Val Acc: 0.6641, Loss: 0.6467\n",
      "Epoch: 006, Train Acc: 0.6809, Test Acc: 0.6846, Val Acc: 0.6929, Loss: 0.6271\n",
      "Epoch: 007, Train Acc: 0.6778, Test Acc: 0.6840, Val Acc: 0.6959, Loss: 0.6190\n",
      "Epoch: 008, Train Acc: 0.6763, Test Acc: 0.6846, Val Acc: 0.6959, Loss: 0.6366\n",
      "Epoch: 009, Train Acc: 0.6793, Test Acc: 0.6870, Val Acc: 0.6959, Loss: 0.6491\n",
      "Epoch: 010, Train Acc: 0.6782, Test Acc: 0.6861, Val Acc: 0.6884, Loss: 0.6527\n",
      "Epoch: 011, Train Acc: 0.6871, Test Acc: 0.6882, Val Acc: 0.6914, Loss: 0.6452\n",
      "Epoch: 012, Train Acc: 0.6878, Test Acc: 0.6840, Val Acc: 0.6929, Loss: 0.6438\n",
      "Epoch: 013, Train Acc: 0.6893, Test Acc: 0.6858, Val Acc: 0.6793, Loss: 0.6364\n",
      "Epoch: 014, Train Acc: 0.6951, Test Acc: 0.6934, Val Acc: 0.6944, Loss: 0.6367\n",
      "Epoch: 015, Train Acc: 0.7018, Test Acc: 0.7024, Val Acc: 0.7065, Loss: 0.6331\n",
      "Epoch: 016, Train Acc: 0.7052, Test Acc: 0.7028, Val Acc: 0.7186, Loss: 0.6341\n",
      "Epoch: 017, Train Acc: 0.7054, Test Acc: 0.6964, Val Acc: 0.7110, Loss: 0.6166\n",
      "Epoch: 018, Train Acc: 0.7074, Test Acc: 0.7031, Val Acc: 0.7080, Loss: 0.6109\n",
      "Epoch: 019, Train Acc: 0.7069, Test Acc: 0.7037, Val Acc: 0.7035, Loss: 0.6032\n",
      "Epoch: 020, Train Acc: 0.7096, Test Acc: 0.7145, Val Acc: 0.7171, Loss: 0.6005\n",
      "Epoch: 021, Train Acc: 0.7105, Test Acc: 0.7100, Val Acc: 0.7065, Loss: 0.5957\n",
      "Epoch: 022, Train Acc: 0.7146, Test Acc: 0.7155, Val Acc: 0.7156, Loss: 0.5948\n",
      "Epoch: 023, Train Acc: 0.7164, Test Acc: 0.7203, Val Acc: 0.7247, Loss: 0.5924\n",
      "Epoch: 024, Train Acc: 0.7178, Test Acc: 0.7206, Val Acc: 0.7292, Loss: 0.5914\n",
      "Epoch: 025, Train Acc: 0.7187, Test Acc: 0.7218, Val Acc: 0.7247, Loss: 0.5860\n",
      "Epoch: 026, Train Acc: 0.7166, Test Acc: 0.7206, Val Acc: 0.7247, Loss: 0.5809\n",
      "Epoch: 027, Train Acc: 0.7128, Test Acc: 0.7203, Val Acc: 0.7292, Loss: 0.5846\n",
      "Epoch: 028, Train Acc: 0.7106, Test Acc: 0.7188, Val Acc: 0.7277, Loss: 0.5800\n",
      "Epoch: 029, Train Acc: 0.7117, Test Acc: 0.7212, Val Acc: 0.7231, Loss: 0.5856\n",
      "Epoch: 030, Train Acc: 0.7119, Test Acc: 0.7236, Val Acc: 0.7231, Loss: 0.5834\n",
      "Epoch: 031, Train Acc: 0.7148, Test Acc: 0.7260, Val Acc: 0.7247, Loss: 0.5772\n",
      "Epoch: 032, Train Acc: 0.7161, Test Acc: 0.7272, Val Acc: 0.7292, Loss: 0.5772\n",
      "Epoch: 033, Train Acc: 0.7168, Test Acc: 0.7282, Val Acc: 0.7307, Loss: 0.5746\n",
      "Epoch: 034, Train Acc: 0.7142, Test Acc: 0.7291, Val Acc: 0.7231, Loss: 0.5753\n",
      "Epoch: 035, Train Acc: 0.7156, Test Acc: 0.7312, Val Acc: 0.7231, Loss: 0.5724\n",
      "Epoch: 036, Train Acc: 0.7172, Test Acc: 0.7315, Val Acc: 0.7247, Loss: 0.5679\n",
      "Epoch: 037, Train Acc: 0.7187, Test Acc: 0.7318, Val Acc: 0.7262, Loss: 0.5646\n",
      "Epoch: 038, Train Acc: 0.7188, Test Acc: 0.7303, Val Acc: 0.7277, Loss: 0.5667\n",
      "Epoch: 039, Train Acc: 0.7212, Test Acc: 0.7309, Val Acc: 0.7322, Loss: 0.5622\n",
      "Epoch: 040, Train Acc: 0.7277, Test Acc: 0.7351, Val Acc: 0.7413, Loss: 0.5638\n",
      "Epoch: 041, Train Acc: 0.7322, Test Acc: 0.7406, Val Acc: 0.7443, Loss: 0.5595\n",
      "Epoch: 042, Train Acc: 0.7330, Test Acc: 0.7421, Val Acc: 0.7458, Loss: 0.5614\n",
      "Epoch: 043, Train Acc: 0.7351, Test Acc: 0.7475, Val Acc: 0.7428, Loss: 0.5601\n",
      "Epoch: 044, Train Acc: 0.7374, Test Acc: 0.7514, Val Acc: 0.7458, Loss: 0.5573\n",
      "Epoch: 045, Train Acc: 0.7390, Test Acc: 0.7505, Val Acc: 0.7458, Loss: 0.5633\n",
      "Epoch: 046, Train Acc: 0.7390, Test Acc: 0.7502, Val Acc: 0.7534, Loss: 0.5596\n",
      "Epoch: 047, Train Acc: 0.7385, Test Acc: 0.7514, Val Acc: 0.7549, Loss: 0.5543\n",
      "Epoch: 048, Train Acc: 0.7384, Test Acc: 0.7505, Val Acc: 0.7595, Loss: 0.5542\n",
      "Epoch: 049, Train Acc: 0.7371, Test Acc: 0.7511, Val Acc: 0.7549, Loss: 0.5474\n",
      "Epoch: 050, Train Acc: 0.7375, Test Acc: 0.7511, Val Acc: 0.7579, Loss: 0.5530\n",
      "Epoch: 051, Train Acc: 0.7373, Test Acc: 0.7490, Val Acc: 0.7504, Loss: 0.5515\n",
      "Epoch: 052, Train Acc: 0.7383, Test Acc: 0.7481, Val Acc: 0.7534, Loss: 0.5495\n",
      "Epoch: 053, Train Acc: 0.7398, Test Acc: 0.7478, Val Acc: 0.7519, Loss: 0.5492\n",
      "Epoch: 054, Train Acc: 0.7406, Test Acc: 0.7466, Val Acc: 0.7534, Loss: 0.5491\n",
      "Epoch: 055, Train Acc: 0.7403, Test Acc: 0.7466, Val Acc: 0.7579, Loss: 0.5543\n",
      "Epoch: 056, Train Acc: 0.7427, Test Acc: 0.7511, Val Acc: 0.7610, Loss: 0.5471\n",
      "Epoch: 057, Train Acc: 0.7440, Test Acc: 0.7508, Val Acc: 0.7595, Loss: 0.5419\n",
      "Epoch: 058, Train Acc: 0.7434, Test Acc: 0.7499, Val Acc: 0.7564, Loss: 0.5407\n",
      "Epoch: 059, Train Acc: 0.7448, Test Acc: 0.7502, Val Acc: 0.7625, Loss: 0.5390\n",
      "Epoch: 060, Train Acc: 0.7448, Test Acc: 0.7499, Val Acc: 0.7625, Loss: 0.5461\n",
      "Epoch: 061, Train Acc: 0.7460, Test Acc: 0.7511, Val Acc: 0.7655, Loss: 0.5471\n",
      "Epoch: 062, Train Acc: 0.7482, Test Acc: 0.7548, Val Acc: 0.7655, Loss: 0.5398\n",
      "Epoch: 063, Train Acc: 0.7468, Test Acc: 0.7533, Val Acc: 0.7640, Loss: 0.5453\n",
      "Epoch: 064, Train Acc: 0.7469, Test Acc: 0.7554, Val Acc: 0.7640, Loss: 0.5451\n",
      "Epoch: 065, Train Acc: 0.7461, Test Acc: 0.7569, Val Acc: 0.7595, Loss: 0.5425\n",
      "Epoch: 066, Train Acc: 0.7465, Test Acc: 0.7575, Val Acc: 0.7610, Loss: 0.5438\n",
      "Epoch: 067, Train Acc: 0.7461, Test Acc: 0.7563, Val Acc: 0.7595, Loss: 0.5395\n",
      "Epoch: 068, Train Acc: 0.7489, Test Acc: 0.7590, Val Acc: 0.7595, Loss: 0.5397\n",
      "Epoch: 069, Train Acc: 0.7492, Test Acc: 0.7569, Val Acc: 0.7595, Loss: 0.5393\n",
      "Epoch: 070, Train Acc: 0.7491, Test Acc: 0.7560, Val Acc: 0.7610, Loss: 0.5411\n",
      "Epoch: 071, Train Acc: 0.7489, Test Acc: 0.7566, Val Acc: 0.7595, Loss: 0.5356\n",
      "Epoch: 072, Train Acc: 0.7482, Test Acc: 0.7529, Val Acc: 0.7610, Loss: 0.5356\n",
      "Epoch: 073, Train Acc: 0.7484, Test Acc: 0.7560, Val Acc: 0.7579, Loss: 0.5326\n",
      "Epoch: 074, Train Acc: 0.7510, Test Acc: 0.7608, Val Acc: 0.7625, Loss: 0.5417\n",
      "Epoch: 075, Train Acc: 0.7514, Test Acc: 0.7611, Val Acc: 0.7595, Loss: 0.5369\n",
      "Epoch: 076, Train Acc: 0.7516, Test Acc: 0.7605, Val Acc: 0.7595, Loss: 0.5405\n",
      "Epoch: 077, Train Acc: 0.7520, Test Acc: 0.7632, Val Acc: 0.7579, Loss: 0.5314\n",
      "Epoch: 078, Train Acc: 0.7491, Test Acc: 0.7611, Val Acc: 0.7595, Loss: 0.5312\n",
      "Epoch: 079, Train Acc: 0.7491, Test Acc: 0.7617, Val Acc: 0.7655, Loss: 0.5386\n",
      "Epoch: 080, Train Acc: 0.7492, Test Acc: 0.7593, Val Acc: 0.7610, Loss: 0.5385\n",
      "Epoch: 081, Train Acc: 0.7508, Test Acc: 0.7623, Val Acc: 0.7595, Loss: 0.5323\n",
      "Epoch: 082, Train Acc: 0.7497, Test Acc: 0.7614, Val Acc: 0.7610, Loss: 0.5396\n",
      "Epoch: 083, Train Acc: 0.7498, Test Acc: 0.7608, Val Acc: 0.7610, Loss: 0.5293\n",
      "Epoch: 084, Train Acc: 0.7507, Test Acc: 0.7611, Val Acc: 0.7625, Loss: 0.5329\n",
      "Epoch: 085, Train Acc: 0.7508, Test Acc: 0.7605, Val Acc: 0.7625, Loss: 0.5337\n",
      "Epoch: 086, Train Acc: 0.7561, Test Acc: 0.7596, Val Acc: 0.7595, Loss: 0.5361\n",
      "Epoch: 087, Train Acc: 0.7575, Test Acc: 0.7608, Val Acc: 0.7625, Loss: 0.5297\n",
      "Epoch: 088, Train Acc: 0.7556, Test Acc: 0.7599, Val Acc: 0.7610, Loss: 0.5314\n",
      "Epoch: 089, Train Acc: 0.7541, Test Acc: 0.7617, Val Acc: 0.7625, Loss: 0.5314\n",
      "Epoch: 090, Train Acc: 0.7526, Test Acc: 0.7626, Val Acc: 0.7610, Loss: 0.5357\n",
      "Epoch: 091, Train Acc: 0.7538, Test Acc: 0.7626, Val Acc: 0.7610, Loss: 0.5328\n",
      "Epoch: 092, Train Acc: 0.7564, Test Acc: 0.7663, Val Acc: 0.7640, Loss: 0.5342\n",
      "Epoch: 093, Train Acc: 0.7559, Test Acc: 0.7672, Val Acc: 0.7655, Loss: 0.5286\n",
      "Epoch: 094, Train Acc: 0.7552, Test Acc: 0.7672, Val Acc: 0.7640, Loss: 0.5303\n",
      "Epoch: 095, Train Acc: 0.7541, Test Acc: 0.7656, Val Acc: 0.7640, Loss: 0.5254\n",
      "Epoch: 096, Train Acc: 0.7563, Test Acc: 0.7660, Val Acc: 0.7579, Loss: 0.5272\n",
      "Epoch: 097, Train Acc: 0.7593, Test Acc: 0.7690, Val Acc: 0.7670, Loss: 0.5339\n",
      "Epoch: 098, Train Acc: 0.7607, Test Acc: 0.7693, Val Acc: 0.7700, Loss: 0.5314\n",
      "Epoch: 099, Train Acc: 0.7607, Test Acc: 0.7696, Val Acc: 0.7655, Loss: 0.5296\n",
      "Epoch: 100, Train Acc: 0.7589, Test Acc: 0.7711, Val Acc: 0.7595, Loss: 0.5238\n",
      "Epoch: 101, Train Acc: 0.7575, Test Acc: 0.7684, Val Acc: 0.7610, Loss: 0.5298\n",
      "Epoch: 102, Train Acc: 0.7582, Test Acc: 0.7696, Val Acc: 0.7640, Loss: 0.5304\n",
      "Epoch: 103, Train Acc: 0.7599, Test Acc: 0.7687, Val Acc: 0.7655, Loss: 0.5246\n",
      "Epoch: 104, Train Acc: 0.7633, Test Acc: 0.7726, Val Acc: 0.7761, Loss: 0.5201\n",
      "Epoch: 105, Train Acc: 0.7627, Test Acc: 0.7717, Val Acc: 0.7761, Loss: 0.5263\n",
      "Epoch: 106, Train Acc: 0.7604, Test Acc: 0.7702, Val Acc: 0.7716, Loss: 0.5293\n",
      "Epoch: 107, Train Acc: 0.7586, Test Acc: 0.7693, Val Acc: 0.7655, Loss: 0.5301\n",
      "Epoch: 108, Train Acc: 0.7578, Test Acc: 0.7669, Val Acc: 0.7640, Loss: 0.5313\n",
      "Epoch: 109, Train Acc: 0.7576, Test Acc: 0.7681, Val Acc: 0.7655, Loss: 0.5264\n",
      "Epoch: 110, Train Acc: 0.7580, Test Acc: 0.7672, Val Acc: 0.7670, Loss: 0.5270\n",
      "Epoch: 111, Train Acc: 0.7605, Test Acc: 0.7717, Val Acc: 0.7700, Loss: 0.5253\n",
      "Epoch: 112, Train Acc: 0.7622, Test Acc: 0.7723, Val Acc: 0.7731, Loss: 0.5272\n",
      "Epoch: 113, Train Acc: 0.7623, Test Acc: 0.7714, Val Acc: 0.7761, Loss: 0.5257\n",
      "Epoch: 114, Train Acc: 0.7619, Test Acc: 0.7726, Val Acc: 0.7746, Loss: 0.5268\n",
      "Epoch: 115, Train Acc: 0.7605, Test Acc: 0.7717, Val Acc: 0.7746, Loss: 0.5211\n",
      "Epoch: 116, Train Acc: 0.7608, Test Acc: 0.7705, Val Acc: 0.7731, Loss: 0.5270\n",
      "Epoch: 117, Train Acc: 0.7609, Test Acc: 0.7699, Val Acc: 0.7700, Loss: 0.5241\n",
      "Epoch: 118, Train Acc: 0.7626, Test Acc: 0.7714, Val Acc: 0.7791, Loss: 0.5243\n",
      "Epoch: 119, Train Acc: 0.7638, Test Acc: 0.7741, Val Acc: 0.7867, Loss: 0.5223\n",
      "Epoch: 120, Train Acc: 0.7645, Test Acc: 0.7741, Val Acc: 0.7837, Loss: 0.5235\n",
      "Epoch: 121, Train Acc: 0.7622, Test Acc: 0.7702, Val Acc: 0.7806, Loss: 0.5205\n",
      "Epoch: 122, Train Acc: 0.7633, Test Acc: 0.7726, Val Acc: 0.7867, Loss: 0.5248\n",
      "Epoch: 123, Train Acc: 0.7632, Test Acc: 0.7744, Val Acc: 0.7897, Loss: 0.5255\n",
      "Epoch: 124, Train Acc: 0.7648, Test Acc: 0.7732, Val Acc: 0.7897, Loss: 0.5253\n",
      "Epoch: 125, Train Acc: 0.7645, Test Acc: 0.7729, Val Acc: 0.7791, Loss: 0.5249\n",
      "Epoch: 126, Train Acc: 0.7638, Test Acc: 0.7726, Val Acc: 0.7746, Loss: 0.5209\n",
      "Epoch: 127, Train Acc: 0.7624, Test Acc: 0.7708, Val Acc: 0.7716, Loss: 0.5235\n",
      "Epoch: 128, Train Acc: 0.7624, Test Acc: 0.7696, Val Acc: 0.7670, Loss: 0.5248\n",
      "Epoch: 129, Train Acc: 0.7634, Test Acc: 0.7705, Val Acc: 0.7731, Loss: 0.5205\n",
      "Epoch: 130, Train Acc: 0.7653, Test Acc: 0.7720, Val Acc: 0.7746, Loss: 0.5163\n",
      "Epoch: 131, Train Acc: 0.7682, Test Acc: 0.7741, Val Acc: 0.7867, Loss: 0.5252\n",
      "Epoch: 132, Train Acc: 0.7699, Test Acc: 0.7759, Val Acc: 0.7958, Loss: 0.5219\n",
      "Epoch: 133, Train Acc: 0.7699, Test Acc: 0.7774, Val Acc: 0.7912, Loss: 0.5265\n",
      "Epoch: 134, Train Acc: 0.7700, Test Acc: 0.7799, Val Acc: 0.7837, Loss: 0.5232\n",
      "Epoch: 135, Train Acc: 0.7668, Test Acc: 0.7774, Val Acc: 0.7716, Loss: 0.5206\n",
      "Epoch: 136, Train Acc: 0.7648, Test Acc: 0.7756, Val Acc: 0.7685, Loss: 0.5235\n",
      "Epoch: 137, Train Acc: 0.7663, Test Acc: 0.7771, Val Acc: 0.7746, Loss: 0.5255\n",
      "Epoch: 138, Train Acc: 0.7668, Test Acc: 0.7799, Val Acc: 0.7776, Loss: 0.5237\n",
      "Epoch: 139, Train Acc: 0.7660, Test Acc: 0.7777, Val Acc: 0.7821, Loss: 0.5198\n",
      "Epoch: 140, Train Acc: 0.7655, Test Acc: 0.7771, Val Acc: 0.7731, Loss: 0.5245\n",
      "Epoch: 141, Train Acc: 0.7648, Test Acc: 0.7741, Val Acc: 0.7700, Loss: 0.5209\n",
      "Epoch: 142, Train Acc: 0.7674, Test Acc: 0.7759, Val Acc: 0.7791, Loss: 0.5239\n",
      "Epoch: 143, Train Acc: 0.7686, Test Acc: 0.7774, Val Acc: 0.7761, Loss: 0.5223\n",
      "Epoch: 144, Train Acc: 0.7663, Test Acc: 0.7747, Val Acc: 0.7700, Loss: 0.5225\n",
      "Epoch: 145, Train Acc: 0.7624, Test Acc: 0.7729, Val Acc: 0.7670, Loss: 0.5231\n",
      "Epoch: 146, Train Acc: 0.7643, Test Acc: 0.7750, Val Acc: 0.7716, Loss: 0.5233\n",
      "Epoch: 147, Train Acc: 0.7674, Test Acc: 0.7826, Val Acc: 0.7670, Loss: 0.5178\n",
      "Epoch: 148, Train Acc: 0.7699, Test Acc: 0.7811, Val Acc: 0.7761, Loss: 0.5170\n",
      "Epoch: 149, Train Acc: 0.7694, Test Acc: 0.7799, Val Acc: 0.7821, Loss: 0.5192\n",
      "Epoch: 150, Train Acc: 0.7695, Test Acc: 0.7799, Val Acc: 0.7776, Loss: 0.5201\n",
      "Epoch: 151, Train Acc: 0.7668, Test Acc: 0.7787, Val Acc: 0.7776, Loss: 0.5215\n",
      "Epoch: 152, Train Acc: 0.7655, Test Acc: 0.7771, Val Acc: 0.7716, Loss: 0.5236\n",
      "Epoch: 153, Train Acc: 0.7663, Test Acc: 0.7744, Val Acc: 0.7700, Loss: 0.5206\n",
      "Epoch: 154, Train Acc: 0.7659, Test Acc: 0.7741, Val Acc: 0.7700, Loss: 0.5186\n",
      "Epoch: 155, Train Acc: 0.7677, Test Acc: 0.7777, Val Acc: 0.7731, Loss: 0.5206\n",
      "Epoch: 156, Train Acc: 0.7689, Test Acc: 0.7796, Val Acc: 0.7731, Loss: 0.5177\n",
      "Epoch: 157, Train Acc: 0.7664, Test Acc: 0.7771, Val Acc: 0.7685, Loss: 0.5162\n",
      "Epoch: 158, Train Acc: 0.7640, Test Acc: 0.7747, Val Acc: 0.7640, Loss: 0.5213\n",
      "Epoch: 159, Train Acc: 0.7690, Test Acc: 0.7802, Val Acc: 0.7685, Loss: 0.5173\n",
      "Epoch: 160, Train Acc: 0.7712, Test Acc: 0.7814, Val Acc: 0.7655, Loss: 0.5259\n",
      "Epoch: 161, Train Acc: 0.7690, Test Acc: 0.7799, Val Acc: 0.7625, Loss: 0.5237\n",
      "Epoch: 162, Train Acc: 0.7654, Test Acc: 0.7762, Val Acc: 0.7700, Loss: 0.5197\n",
      "Epoch: 163, Train Acc: 0.7646, Test Acc: 0.7741, Val Acc: 0.7685, Loss: 0.5174\n",
      "Epoch: 164, Train Acc: 0.7669, Test Acc: 0.7753, Val Acc: 0.7746, Loss: 0.5239\n",
      "Epoch: 165, Train Acc: 0.7686, Test Acc: 0.7771, Val Acc: 0.7776, Loss: 0.5235\n",
      "Epoch: 166, Train Acc: 0.7672, Test Acc: 0.7768, Val Acc: 0.7776, Loss: 0.5213\n",
      "Epoch: 167, Train Acc: 0.7659, Test Acc: 0.7732, Val Acc: 0.7731, Loss: 0.5224\n",
      "Epoch: 168, Train Acc: 0.7661, Test Acc: 0.7735, Val Acc: 0.7700, Loss: 0.5253\n",
      "Epoch: 169, Train Acc: 0.7658, Test Acc: 0.7723, Val Acc: 0.7610, Loss: 0.5220\n",
      "Epoch: 170, Train Acc: 0.7662, Test Acc: 0.7738, Val Acc: 0.7610, Loss: 0.5222\n",
      "Epoch: 171, Train Acc: 0.7657, Test Acc: 0.7753, Val Acc: 0.7670, Loss: 0.5185\n",
      "Epoch: 172, Train Acc: 0.7670, Test Acc: 0.7777, Val Acc: 0.7655, Loss: 0.5210\n",
      "Epoch: 173, Train Acc: 0.7656, Test Acc: 0.7777, Val Acc: 0.7640, Loss: 0.5221\n",
      "Epoch: 174, Train Acc: 0.7669, Test Acc: 0.7802, Val Acc: 0.7685, Loss: 0.5199\n",
      "Epoch: 175, Train Acc: 0.7684, Test Acc: 0.7787, Val Acc: 0.7700, Loss: 0.5215\n",
      "Epoch: 176, Train Acc: 0.7667, Test Acc: 0.7750, Val Acc: 0.7670, Loss: 0.5229\n",
      "Epoch: 177, Train Acc: 0.7650, Test Acc: 0.7720, Val Acc: 0.7640, Loss: 0.5220\n",
      "Epoch: 178, Train Acc: 0.7638, Test Acc: 0.7720, Val Acc: 0.7625, Loss: 0.5279\n",
      "Epoch: 179, Train Acc: 0.7667, Test Acc: 0.7771, Val Acc: 0.7700, Loss: 0.5186\n",
      "Epoch: 180, Train Acc: 0.7651, Test Acc: 0.7774, Val Acc: 0.7670, Loss: 0.5187\n",
      "Epoch: 181, Train Acc: 0.7657, Test Acc: 0.7787, Val Acc: 0.7670, Loss: 0.5283\n",
      "Epoch: 182, Train Acc: 0.7656, Test Acc: 0.7774, Val Acc: 0.7610, Loss: 0.5121\n",
      "Epoch: 183, Train Acc: 0.7648, Test Acc: 0.7732, Val Acc: 0.7564, Loss: 0.5237\n",
      "Epoch: 184, Train Acc: 0.7663, Test Acc: 0.7741, Val Acc: 0.7595, Loss: 0.5260\n",
      "Epoch: 185, Train Acc: 0.7684, Test Acc: 0.7729, Val Acc: 0.7670, Loss: 0.5179\n",
      "Epoch: 186, Train Acc: 0.7661, Test Acc: 0.7717, Val Acc: 0.7595, Loss: 0.5196\n",
      "Epoch: 187, Train Acc: 0.7657, Test Acc: 0.7708, Val Acc: 0.7534, Loss: 0.5182\n",
      "Epoch: 188, Train Acc: 0.7667, Test Acc: 0.7711, Val Acc: 0.7564, Loss: 0.5228\n",
      "Epoch: 189, Train Acc: 0.7707, Test Acc: 0.7768, Val Acc: 0.7685, Loss: 0.5223\n",
      "Epoch: 190, Train Acc: 0.7682, Test Acc: 0.7747, Val Acc: 0.7731, Loss: 0.5205\n",
      "Epoch: 191, Train Acc: 0.7635, Test Acc: 0.7690, Val Acc: 0.7625, Loss: 0.5185\n",
      "Epoch: 192, Train Acc: 0.7654, Test Acc: 0.7738, Val Acc: 0.7640, Loss: 0.5185\n",
      "Epoch: 193, Train Acc: 0.7678, Test Acc: 0.7799, Val Acc: 0.7685, Loss: 0.5230\n",
      "Epoch: 194, Train Acc: 0.7670, Test Acc: 0.7774, Val Acc: 0.7700, Loss: 0.5248\n",
      "Epoch: 195, Train Acc: 0.7645, Test Acc: 0.7762, Val Acc: 0.7655, Loss: 0.5228\n",
      "Epoch: 196, Train Acc: 0.7641, Test Acc: 0.7729, Val Acc: 0.7655, Loss: 0.5254\n",
      "Epoch: 197, Train Acc: 0.7635, Test Acc: 0.7711, Val Acc: 0.7625, Loss: 0.5197\n",
      "Epoch: 198, Train Acc: 0.7670, Test Acc: 0.7744, Val Acc: 0.7625, Loss: 0.5225\n",
      "Epoch: 199, Train Acc: 0.7684, Test Acc: 0.7768, Val Acc: 0.7655, Loss: 0.5192\n",
      "Epoch: 200, Train Acc: 0.7684, Test Acc: 0.7783, Val Acc: 0.7731, Loss: 0.5187\n",
      "Epoch: 201, Train Acc: 0.7695, Test Acc: 0.7802, Val Acc: 0.7700, Loss: 0.5208\n",
      "Epoch: 202, Train Acc: 0.7692, Test Acc: 0.7808, Val Acc: 0.7640, Loss: 0.5192\n",
      "Epoch: 203, Train Acc: 0.7675, Test Acc: 0.7765, Val Acc: 0.7655, Loss: 0.5212\n",
      "Epoch: 204, Train Acc: 0.7656, Test Acc: 0.7753, Val Acc: 0.7625, Loss: 0.5205\n",
      "Epoch: 205, Train Acc: 0.7655, Test Acc: 0.7744, Val Acc: 0.7685, Loss: 0.5231\n",
      "Epoch: 206, Train Acc: 0.7671, Test Acc: 0.7777, Val Acc: 0.7685, Loss: 0.5236\n",
      "Epoch: 207, Train Acc: 0.7659, Test Acc: 0.7759, Val Acc: 0.7655, Loss: 0.5173\n",
      "Epoch: 208, Train Acc: 0.7648, Test Acc: 0.7732, Val Acc: 0.7670, Loss: 0.5136\n",
      "Epoch: 209, Train Acc: 0.7650, Test Acc: 0.7750, Val Acc: 0.7595, Loss: 0.5228\n",
      "Epoch: 210, Train Acc: 0.7647, Test Acc: 0.7738, Val Acc: 0.7670, Loss: 0.5239\n",
      "Epoch: 211, Train Acc: 0.7635, Test Acc: 0.7741, Val Acc: 0.7655, Loss: 0.5239\n",
      "Epoch: 212, Train Acc: 0.7594, Test Acc: 0.7696, Val Acc: 0.7655, Loss: 0.5205\n",
      "Epoch: 213, Train Acc: 0.7615, Test Acc: 0.7717, Val Acc: 0.7564, Loss: 0.5199\n",
      "Epoch: 214, Train Acc: 0.7698, Test Acc: 0.7774, Val Acc: 0.7685, Loss: 0.5218\n",
      "Epoch: 215, Train Acc: 0.7710, Test Acc: 0.7799, Val Acc: 0.7731, Loss: 0.5199\n",
      "Epoch: 216, Train Acc: 0.7686, Test Acc: 0.7799, Val Acc: 0.7700, Loss: 0.5204\n",
      "Epoch: 217, Train Acc: 0.7670, Test Acc: 0.7765, Val Acc: 0.7640, Loss: 0.5180\n",
      "Epoch: 218, Train Acc: 0.7665, Test Acc: 0.7780, Val Acc: 0.7474, Loss: 0.5208\n",
      "Epoch: 219, Train Acc: 0.7694, Test Acc: 0.7829, Val Acc: 0.7564, Loss: 0.5203\n",
      "Epoch: 220, Train Acc: 0.7691, Test Acc: 0.7847, Val Acc: 0.7670, Loss: 0.5213\n",
      "Epoch: 221, Train Acc: 0.7667, Test Acc: 0.7762, Val Acc: 0.7519, Loss: 0.5166\n",
      "Epoch: 222, Train Acc: 0.7644, Test Acc: 0.7750, Val Acc: 0.7595, Loss: 0.5225\n",
      "Epoch: 223, Train Acc: 0.7702, Test Acc: 0.7787, Val Acc: 0.7670, Loss: 0.5231\n",
      "Epoch: 224, Train Acc: 0.7685, Test Acc: 0.7793, Val Acc: 0.7579, Loss: 0.5219\n",
      "Epoch: 225, Train Acc: 0.7665, Test Acc: 0.7759, Val Acc: 0.7549, Loss: 0.5265\n",
      "Epoch: 226, Train Acc: 0.7653, Test Acc: 0.7729, Val Acc: 0.7504, Loss: 0.5230\n",
      "Epoch: 227, Train Acc: 0.7674, Test Acc: 0.7780, Val Acc: 0.7519, Loss: 0.5219\n",
      "Epoch: 228, Train Acc: 0.7682, Test Acc: 0.7793, Val Acc: 0.7610, Loss: 0.5245\n",
      "Epoch: 229, Train Acc: 0.7694, Test Acc: 0.7817, Val Acc: 0.7685, Loss: 0.5239\n",
      "Epoch: 230, Train Acc: 0.7691, Test Acc: 0.7777, Val Acc: 0.7625, Loss: 0.5185\n",
      "Epoch: 231, Train Acc: 0.7646, Test Acc: 0.7693, Val Acc: 0.7564, Loss: 0.5234\n",
      "Epoch: 232, Train Acc: 0.7637, Test Acc: 0.7702, Val Acc: 0.7610, Loss: 0.5190\n",
      "Epoch: 233, Train Acc: 0.7628, Test Acc: 0.7687, Val Acc: 0.7655, Loss: 0.5170\n",
      "Epoch: 234, Train Acc: 0.7650, Test Acc: 0.7741, Val Acc: 0.7685, Loss: 0.5248\n",
      "Epoch: 235, Train Acc: 0.7650, Test Acc: 0.7705, Val Acc: 0.7685, Loss: 0.5183\n",
      "Epoch: 236, Train Acc: 0.7656, Test Acc: 0.7735, Val Acc: 0.7670, Loss: 0.5153\n",
      "Epoch: 237, Train Acc: 0.7656, Test Acc: 0.7744, Val Acc: 0.7625, Loss: 0.5190\n",
      "Epoch: 238, Train Acc: 0.7669, Test Acc: 0.7750, Val Acc: 0.7579, Loss: 0.5198\n",
      "Epoch: 239, Train Acc: 0.7664, Test Acc: 0.7756, Val Acc: 0.7595, Loss: 0.5247\n",
      "Epoch: 240, Train Acc: 0.7674, Test Acc: 0.7750, Val Acc: 0.7564, Loss: 0.5210\n",
      "Epoch: 241, Train Acc: 0.7650, Test Acc: 0.7738, Val Acc: 0.7655, Loss: 0.5235\n",
      "Epoch: 242, Train Acc: 0.7650, Test Acc: 0.7726, Val Acc: 0.7716, Loss: 0.5197\n",
      "Epoch: 243, Train Acc: 0.7660, Test Acc: 0.7741, Val Acc: 0.7655, Loss: 0.5158\n",
      "Epoch: 244, Train Acc: 0.7647, Test Acc: 0.7720, Val Acc: 0.7610, Loss: 0.5197\n",
      "Epoch: 245, Train Acc: 0.7662, Test Acc: 0.7720, Val Acc: 0.7685, Loss: 0.5134\n",
      "Epoch: 246, Train Acc: 0.7674, Test Acc: 0.7741, Val Acc: 0.7670, Loss: 0.5216\n",
      "Epoch: 247, Train Acc: 0.7680, Test Acc: 0.7771, Val Acc: 0.7670, Loss: 0.5205\n",
      "Epoch: 248, Train Acc: 0.7689, Test Acc: 0.7777, Val Acc: 0.7731, Loss: 0.5229\n",
      "Epoch: 249, Train Acc: 0.7703, Test Acc: 0.7811, Val Acc: 0.7746, Loss: 0.5175\n",
      "Epoch: 250, Train Acc: 0.7684, Test Acc: 0.7780, Val Acc: 0.7685, Loss: 0.5205\n",
      "Epoch: 251, Train Acc: 0.7627, Test Acc: 0.7735, Val Acc: 0.7731, Loss: 0.5132\n",
      "Epoch: 252, Train Acc: 0.7637, Test Acc: 0.7723, Val Acc: 0.7731, Loss: 0.5199\n",
      "Epoch: 253, Train Acc: 0.7674, Test Acc: 0.7753, Val Acc: 0.7700, Loss: 0.5183\n",
      "Epoch: 254, Train Acc: 0.7678, Test Acc: 0.7777, Val Acc: 0.7716, Loss: 0.5236\n",
      "Epoch: 255, Train Acc: 0.7649, Test Acc: 0.7756, Val Acc: 0.7640, Loss: 0.5237\n",
      "Epoch: 256, Train Acc: 0.7638, Test Acc: 0.7738, Val Acc: 0.7595, Loss: 0.5237\n",
      "Epoch: 257, Train Acc: 0.7641, Test Acc: 0.7720, Val Acc: 0.7595, Loss: 0.5204\n",
      "Epoch: 258, Train Acc: 0.7681, Test Acc: 0.7738, Val Acc: 0.7685, Loss: 0.5194\n",
      "Epoch: 259, Train Acc: 0.7684, Test Acc: 0.7741, Val Acc: 0.7700, Loss: 0.5267\n",
      "Epoch: 260, Train Acc: 0.7684, Test Acc: 0.7762, Val Acc: 0.7579, Loss: 0.5240\n",
      "Epoch: 261, Train Acc: 0.7672, Test Acc: 0.7762, Val Acc: 0.7640, Loss: 0.5193\n",
      "Epoch: 262, Train Acc: 0.7658, Test Acc: 0.7762, Val Acc: 0.7776, Loss: 0.5157\n",
      "Epoch: 263, Train Acc: 0.7656, Test Acc: 0.7744, Val Acc: 0.7670, Loss: 0.5171\n",
      "Epoch: 264, Train Acc: 0.7669, Test Acc: 0.7762, Val Acc: 0.7655, Loss: 0.5228\n",
      "Epoch: 265, Train Acc: 0.7669, Test Acc: 0.7774, Val Acc: 0.7746, Loss: 0.5235\n",
      "Epoch: 266, Train Acc: 0.7650, Test Acc: 0.7738, Val Acc: 0.7746, Loss: 0.5128\n",
      "Epoch: 267, Train Acc: 0.7634, Test Acc: 0.7720, Val Acc: 0.7640, Loss: 0.5229\n",
      "Epoch: 268, Train Acc: 0.7678, Test Acc: 0.7732, Val Acc: 0.7716, Loss: 0.5190\n",
      "Epoch: 269, Train Acc: 0.7702, Test Acc: 0.7780, Val Acc: 0.7806, Loss: 0.5176\n",
      "Epoch: 270, Train Acc: 0.7681, Test Acc: 0.7796, Val Acc: 0.7655, Loss: 0.5216\n",
      "Epoch: 271, Train Acc: 0.7671, Test Acc: 0.7771, Val Acc: 0.7625, Loss: 0.5124\n",
      "Epoch: 272, Train Acc: 0.7656, Test Acc: 0.7750, Val Acc: 0.7655, Loss: 0.5225\n",
      "Epoch: 273, Train Acc: 0.7671, Test Acc: 0.7783, Val Acc: 0.7655, Loss: 0.5147\n",
      "Epoch: 274, Train Acc: 0.7655, Test Acc: 0.7759, Val Acc: 0.7610, Loss: 0.5231\n",
      "Epoch: 275, Train Acc: 0.7653, Test Acc: 0.7771, Val Acc: 0.7610, Loss: 0.5201\n",
      "Epoch: 276, Train Acc: 0.7658, Test Acc: 0.7805, Val Acc: 0.7625, Loss: 0.5209\n",
      "Epoch: 277, Train Acc: 0.7671, Test Acc: 0.7802, Val Acc: 0.7640, Loss: 0.5142\n",
      "Epoch: 278, Train Acc: 0.7642, Test Acc: 0.7750, Val Acc: 0.7625, Loss: 0.5214\n",
      "Epoch: 279, Train Acc: 0.7646, Test Acc: 0.7738, Val Acc: 0.7504, Loss: 0.5175\n",
      "Epoch: 280, Train Acc: 0.7699, Test Acc: 0.7777, Val Acc: 0.7670, Loss: 0.5195\n",
      "Epoch: 281, Train Acc: 0.7700, Test Acc: 0.7777, Val Acc: 0.7595, Loss: 0.5190\n",
      "Epoch: 282, Train Acc: 0.7674, Test Acc: 0.7780, Val Acc: 0.7579, Loss: 0.5201\n",
      "Epoch: 283, Train Acc: 0.7650, Test Acc: 0.7771, Val Acc: 0.7519, Loss: 0.5187\n",
      "Epoch: 284, Train Acc: 0.7672, Test Acc: 0.7768, Val Acc: 0.7519, Loss: 0.5198\n",
      "Epoch: 285, Train Acc: 0.7696, Test Acc: 0.7805, Val Acc: 0.7685, Loss: 0.5192\n",
      "Epoch: 286, Train Acc: 0.7718, Test Acc: 0.7814, Val Acc: 0.7776, Loss: 0.5185\n",
      "Epoch: 287, Train Acc: 0.7664, Test Acc: 0.7762, Val Acc: 0.7746, Loss: 0.5209\n",
      "Epoch: 288, Train Acc: 0.7611, Test Acc: 0.7729, Val Acc: 0.7640, Loss: 0.5180\n",
      "Epoch: 289, Train Acc: 0.7641, Test Acc: 0.7735, Val Acc: 0.7625, Loss: 0.5138\n",
      "Epoch: 290, Train Acc: 0.7709, Test Acc: 0.7765, Val Acc: 0.7731, Loss: 0.5199\n",
      "Epoch: 291, Train Acc: 0.7705, Test Acc: 0.7771, Val Acc: 0.7716, Loss: 0.5202\n",
      "Epoch: 292, Train Acc: 0.7660, Test Acc: 0.7738, Val Acc: 0.7716, Loss: 0.5207\n",
      "Epoch: 293, Train Acc: 0.7627, Test Acc: 0.7714, Val Acc: 0.7610, Loss: 0.5226\n",
      "Epoch: 294, Train Acc: 0.7694, Test Acc: 0.7774, Val Acc: 0.7625, Loss: 0.5192\n",
      "Epoch: 295, Train Acc: 0.7703, Test Acc: 0.7780, Val Acc: 0.7625, Loss: 0.5221\n",
      "Epoch: 296, Train Acc: 0.7647, Test Acc: 0.7693, Val Acc: 0.7579, Loss: 0.5178\n",
      "Epoch: 297, Train Acc: 0.7626, Test Acc: 0.7678, Val Acc: 0.7579, Loss: 0.5210\n",
      "Epoch: 298, Train Acc: 0.7662, Test Acc: 0.7735, Val Acc: 0.7685, Loss: 0.5185\n",
      "Epoch: 299, Train Acc: 0.7713, Test Acc: 0.7771, Val Acc: 0.7685, Loss: 0.5191\n",
      "Epoch: 300, Train Acc: 0.7714, Test Acc: 0.7756, Val Acc: 0.7731, Loss: 0.5167\n",
      "Epoch: 301, Train Acc: 0.7669, Test Acc: 0.7756, Val Acc: 0.7685, Loss: 0.5203\n",
      "Epoch: 302, Train Acc: 0.7645, Test Acc: 0.7735, Val Acc: 0.7776, Loss: 0.5242\n",
      "Epoch: 303, Train Acc: 0.7671, Test Acc: 0.7771, Val Acc: 0.7867, Loss: 0.5215\n",
      "Epoch: 304, Train Acc: 0.7668, Test Acc: 0.7783, Val Acc: 0.7791, Loss: 0.5213\n",
      "Epoch: 305, Train Acc: 0.7658, Test Acc: 0.7777, Val Acc: 0.7776, Loss: 0.5160\n",
      "Epoch: 306, Train Acc: 0.7645, Test Acc: 0.7768, Val Acc: 0.7746, Loss: 0.5212\n",
      "Epoch: 307, Train Acc: 0.7692, Test Acc: 0.7814, Val Acc: 0.7761, Loss: 0.5234\n",
      "Epoch: 308, Train Acc: 0.7728, Test Acc: 0.7838, Val Acc: 0.7700, Loss: 0.5190\n",
      "Epoch: 309, Train Acc: 0.7671, Test Acc: 0.7759, Val Acc: 0.7564, Loss: 0.5233\n",
      "Epoch: 310, Train Acc: 0.7624, Test Acc: 0.7741, Val Acc: 0.7504, Loss: 0.5201\n",
      "Epoch: 311, Train Acc: 0.7667, Test Acc: 0.7753, Val Acc: 0.7595, Loss: 0.5190\n",
      "Epoch: 312, Train Acc: 0.7675, Test Acc: 0.7768, Val Acc: 0.7549, Loss: 0.5198\n",
      "Epoch: 313, Train Acc: 0.7694, Test Acc: 0.7790, Val Acc: 0.7549, Loss: 0.5242\n",
      "Epoch: 314, Train Acc: 0.7646, Test Acc: 0.7729, Val Acc: 0.7595, Loss: 0.5242\n",
      "Epoch: 315, Train Acc: 0.7641, Test Acc: 0.7750, Val Acc: 0.7640, Loss: 0.5222\n",
      "Epoch: 316, Train Acc: 0.7685, Test Acc: 0.7771, Val Acc: 0.7625, Loss: 0.5195\n",
      "Epoch: 317, Train Acc: 0.7726, Test Acc: 0.7805, Val Acc: 0.7700, Loss: 0.5185\n",
      "Epoch: 318, Train Acc: 0.7638, Test Acc: 0.7756, Val Acc: 0.7640, Loss: 0.5192\n",
      "Epoch: 319, Train Acc: 0.7626, Test Acc: 0.7744, Val Acc: 0.7716, Loss: 0.5235\n",
      "Epoch: 320, Train Acc: 0.7676, Test Acc: 0.7802, Val Acc: 0.7655, Loss: 0.5202\n",
      "Epoch: 321, Train Acc: 0.7687, Test Acc: 0.7805, Val Acc: 0.7700, Loss: 0.5226\n",
      "Epoch: 322, Train Acc: 0.7684, Test Acc: 0.7808, Val Acc: 0.7716, Loss: 0.5198\n",
      "Epoch: 323, Train Acc: 0.7688, Test Acc: 0.7817, Val Acc: 0.7746, Loss: 0.5214\n",
      "Epoch: 324, Train Acc: 0.7691, Test Acc: 0.7796, Val Acc: 0.7731, Loss: 0.5142\n",
      "Epoch: 325, Train Acc: 0.7684, Test Acc: 0.7802, Val Acc: 0.7685, Loss: 0.5203\n",
      "Epoch: 326, Train Acc: 0.7694, Test Acc: 0.7802, Val Acc: 0.7655, Loss: 0.5162\n",
      "Epoch: 327, Train Acc: 0.7683, Test Acc: 0.7796, Val Acc: 0.7685, Loss: 0.5197\n",
      "Epoch: 328, Train Acc: 0.7668, Test Acc: 0.7790, Val Acc: 0.7700, Loss: 0.5237\n",
      "Epoch: 329, Train Acc: 0.7686, Test Acc: 0.7790, Val Acc: 0.7685, Loss: 0.5229\n",
      "Epoch: 330, Train Acc: 0.7719, Test Acc: 0.7793, Val Acc: 0.7716, Loss: 0.5153\n",
      "Epoch: 331, Train Acc: 0.7713, Test Acc: 0.7756, Val Acc: 0.7685, Loss: 0.5230\n",
      "Epoch: 332, Train Acc: 0.7687, Test Acc: 0.7777, Val Acc: 0.7700, Loss: 0.5220\n",
      "Epoch: 333, Train Acc: 0.7658, Test Acc: 0.7738, Val Acc: 0.7579, Loss: 0.5187\n",
      "Epoch: 334, Train Acc: 0.7662, Test Acc: 0.7790, Val Acc: 0.7549, Loss: 0.5188\n",
      "Epoch: 335, Train Acc: 0.7695, Test Acc: 0.7820, Val Acc: 0.7655, Loss: 0.5169\n",
      "Epoch: 336, Train Acc: 0.7686, Test Acc: 0.7802, Val Acc: 0.7685, Loss: 0.5245\n",
      "Epoch: 337, Train Acc: 0.7647, Test Acc: 0.7817, Val Acc: 0.7716, Loss: 0.5202\n",
      "Epoch: 338, Train Acc: 0.7654, Test Acc: 0.7817, Val Acc: 0.7625, Loss: 0.5191\n",
      "Epoch: 339, Train Acc: 0.7644, Test Acc: 0.7799, Val Acc: 0.7610, Loss: 0.5157\n",
      "Epoch: 340, Train Acc: 0.7668, Test Acc: 0.7747, Val Acc: 0.7595, Loss: 0.5223\n",
      "Epoch: 341, Train Acc: 0.7660, Test Acc: 0.7708, Val Acc: 0.7685, Loss: 0.5211\n",
      "Epoch: 342, Train Acc: 0.7641, Test Acc: 0.7723, Val Acc: 0.7610, Loss: 0.5188\n",
      "Epoch: 343, Train Acc: 0.7664, Test Acc: 0.7765, Val Acc: 0.7625, Loss: 0.5156\n",
      "Epoch: 344, Train Acc: 0.7663, Test Acc: 0.7744, Val Acc: 0.7579, Loss: 0.5211\n",
      "Epoch: 345, Train Acc: 0.7653, Test Acc: 0.7738, Val Acc: 0.7579, Loss: 0.5144\n",
      "Epoch: 346, Train Acc: 0.7671, Test Acc: 0.7771, Val Acc: 0.7549, Loss: 0.5200\n",
      "Epoch: 347, Train Acc: 0.7661, Test Acc: 0.7790, Val Acc: 0.7579, Loss: 0.5190\n",
      "Epoch: 348, Train Acc: 0.7657, Test Acc: 0.7759, Val Acc: 0.7625, Loss: 0.5216\n",
      "Epoch: 349, Train Acc: 0.7637, Test Acc: 0.7747, Val Acc: 0.7670, Loss: 0.5217\n",
      "Epoch: 350, Train Acc: 0.7645, Test Acc: 0.7741, Val Acc: 0.7716, Loss: 0.5196\n",
      "Epoch: 351, Train Acc: 0.7678, Test Acc: 0.7783, Val Acc: 0.7685, Loss: 0.5184\n",
      "Epoch: 352, Train Acc: 0.7685, Test Acc: 0.7729, Val Acc: 0.7655, Loss: 0.5183\n",
      "Epoch: 353, Train Acc: 0.7677, Test Acc: 0.7768, Val Acc: 0.7716, Loss: 0.5183\n",
      "Epoch: 354, Train Acc: 0.7656, Test Acc: 0.7768, Val Acc: 0.7610, Loss: 0.5156\n",
      "Epoch: 355, Train Acc: 0.7672, Test Acc: 0.7753, Val Acc: 0.7625, Loss: 0.5238\n",
      "Epoch: 356, Train Acc: 0.7705, Test Acc: 0.7771, Val Acc: 0.7685, Loss: 0.5221\n",
      "Epoch: 357, Train Acc: 0.7713, Test Acc: 0.7768, Val Acc: 0.7716, Loss: 0.5194\n",
      "Epoch: 358, Train Acc: 0.7646, Test Acc: 0.7747, Val Acc: 0.7700, Loss: 0.5191\n",
      "Epoch: 359, Train Acc: 0.7632, Test Acc: 0.7690, Val Acc: 0.7761, Loss: 0.5255\n",
      "Epoch: 360, Train Acc: 0.7669, Test Acc: 0.7726, Val Acc: 0.7776, Loss: 0.5184\n",
      "Epoch: 361, Train Acc: 0.7671, Test Acc: 0.7696, Val Acc: 0.7821, Loss: 0.5219\n",
      "Epoch: 362, Train Acc: 0.7633, Test Acc: 0.7687, Val Acc: 0.7837, Loss: 0.5190\n",
      "Epoch: 363, Train Acc: 0.7631, Test Acc: 0.7690, Val Acc: 0.7852, Loss: 0.5154\n",
      "Epoch: 364, Train Acc: 0.7637, Test Acc: 0.7711, Val Acc: 0.7852, Loss: 0.5206\n",
      "Epoch: 365, Train Acc: 0.7663, Test Acc: 0.7717, Val Acc: 0.7746, Loss: 0.5171\n",
      "Epoch: 366, Train Acc: 0.7701, Test Acc: 0.7780, Val Acc: 0.7806, Loss: 0.5227\n",
      "Epoch: 367, Train Acc: 0.7682, Test Acc: 0.7768, Val Acc: 0.7655, Loss: 0.5200\n",
      "Epoch: 368, Train Acc: 0.7669, Test Acc: 0.7762, Val Acc: 0.7700, Loss: 0.5158\n",
      "Epoch: 369, Train Acc: 0.7668, Test Acc: 0.7777, Val Acc: 0.7685, Loss: 0.5179\n",
      "Epoch: 370, Train Acc: 0.7700, Test Acc: 0.7793, Val Acc: 0.7700, Loss: 0.5184\n",
      "Epoch: 371, Train Acc: 0.7680, Test Acc: 0.7811, Val Acc: 0.7731, Loss: 0.5146\n",
      "Epoch: 372, Train Acc: 0.7670, Test Acc: 0.7768, Val Acc: 0.7791, Loss: 0.5237\n",
      "Epoch: 373, Train Acc: 0.7649, Test Acc: 0.7759, Val Acc: 0.7640, Loss: 0.5145\n",
      "Epoch: 374, Train Acc: 0.7653, Test Acc: 0.7774, Val Acc: 0.7640, Loss: 0.5236\n",
      "Epoch: 375, Train Acc: 0.7663, Test Acc: 0.7783, Val Acc: 0.7640, Loss: 0.5213\n",
      "Epoch: 376, Train Acc: 0.7680, Test Acc: 0.7762, Val Acc: 0.7655, Loss: 0.5232\n",
      "Epoch: 377, Train Acc: 0.7665, Test Acc: 0.7765, Val Acc: 0.7776, Loss: 0.5191\n",
      "Epoch: 378, Train Acc: 0.7662, Test Acc: 0.7735, Val Acc: 0.7731, Loss: 0.5176\n",
      "Epoch: 379, Train Acc: 0.7695, Test Acc: 0.7759, Val Acc: 0.7670, Loss: 0.5265\n",
      "Epoch: 380, Train Acc: 0.7700, Test Acc: 0.7780, Val Acc: 0.7685, Loss: 0.5234\n",
      "Epoch: 381, Train Acc: 0.7629, Test Acc: 0.7765, Val Acc: 0.7625, Loss: 0.5218\n",
      "Epoch: 382, Train Acc: 0.7614, Test Acc: 0.7747, Val Acc: 0.7640, Loss: 0.5186\n",
      "Epoch: 383, Train Acc: 0.7673, Test Acc: 0.7780, Val Acc: 0.7761, Loss: 0.5209\n",
      "Epoch: 384, Train Acc: 0.7711, Test Acc: 0.7765, Val Acc: 0.7821, Loss: 0.5125\n",
      "Epoch: 385, Train Acc: 0.7662, Test Acc: 0.7693, Val Acc: 0.7716, Loss: 0.5240\n",
      "Epoch: 386, Train Acc: 0.7645, Test Acc: 0.7717, Val Acc: 0.7670, Loss: 0.5242\n",
      "Epoch: 387, Train Acc: 0.7659, Test Acc: 0.7765, Val Acc: 0.7610, Loss: 0.5247\n",
      "Epoch: 388, Train Acc: 0.7704, Test Acc: 0.7787, Val Acc: 0.7685, Loss: 0.5223\n",
      "Epoch: 389, Train Acc: 0.7685, Test Acc: 0.7771, Val Acc: 0.7655, Loss: 0.5192\n",
      "Epoch: 390, Train Acc: 0.7637, Test Acc: 0.7744, Val Acc: 0.7670, Loss: 0.5171\n",
      "Epoch: 391, Train Acc: 0.7675, Test Acc: 0.7771, Val Acc: 0.7716, Loss: 0.5191\n",
      "Epoch: 392, Train Acc: 0.7690, Test Acc: 0.7787, Val Acc: 0.7700, Loss: 0.5190\n",
      "Epoch: 393, Train Acc: 0.7690, Test Acc: 0.7780, Val Acc: 0.7761, Loss: 0.5230\n",
      "Epoch: 394, Train Acc: 0.7683, Test Acc: 0.7802, Val Acc: 0.7791, Loss: 0.5266\n",
      "Epoch: 395, Train Acc: 0.7672, Test Acc: 0.7777, Val Acc: 0.7655, Loss: 0.5206\n",
      "Epoch: 396, Train Acc: 0.7659, Test Acc: 0.7793, Val Acc: 0.7579, Loss: 0.5282\n",
      "Epoch: 397, Train Acc: 0.7674, Test Acc: 0.7780, Val Acc: 0.7625, Loss: 0.5200\n",
      "Epoch: 398, Train Acc: 0.7653, Test Acc: 0.7756, Val Acc: 0.7640, Loss: 0.5252\n",
      "Epoch: 399, Train Acc: 0.7578, Test Acc: 0.7690, Val Acc: 0.7610, Loss: 0.5137\n",
      "Epoch: 400, Train Acc: 0.7685, Test Acc: 0.7765, Val Acc: 0.7716, Loss: 0.5191\n",
      "Epoch: 401, Train Acc: 0.7677, Test Acc: 0.7793, Val Acc: 0.7776, Loss: 0.5189\n",
      "Epoch: 402, Train Acc: 0.7675, Test Acc: 0.7777, Val Acc: 0.7700, Loss: 0.5241\n",
      "Epoch: 403, Train Acc: 0.7614, Test Acc: 0.7684, Val Acc: 0.7595, Loss: 0.5174\n",
      "Epoch: 404, Train Acc: 0.7651, Test Acc: 0.7726, Val Acc: 0.7610, Loss: 0.5173\n",
      "Epoch: 405, Train Acc: 0.7718, Test Acc: 0.7771, Val Acc: 0.7610, Loss: 0.5241\n",
      "Epoch: 406, Train Acc: 0.7734, Test Acc: 0.7820, Val Acc: 0.7685, Loss: 0.5211\n",
      "Epoch: 407, Train Acc: 0.7687, Test Acc: 0.7805, Val Acc: 0.7685, Loss: 0.5201\n",
      "Epoch: 408, Train Acc: 0.7667, Test Acc: 0.7756, Val Acc: 0.7655, Loss: 0.5156\n",
      "Epoch: 409, Train Acc: 0.7728, Test Acc: 0.7808, Val Acc: 0.7731, Loss: 0.5210\n",
      "Epoch: 410, Train Acc: 0.7744, Test Acc: 0.7844, Val Acc: 0.7791, Loss: 0.5204\n",
      "Epoch: 411, Train Acc: 0.7714, Test Acc: 0.7829, Val Acc: 0.7700, Loss: 0.5207\n",
      "Epoch: 412, Train Acc: 0.7637, Test Acc: 0.7814, Val Acc: 0.7474, Loss: 0.5177\n",
      "Epoch: 413, Train Acc: 0.7643, Test Acc: 0.7796, Val Acc: 0.7534, Loss: 0.5261\n",
      "Epoch: 414, Train Acc: 0.7673, Test Acc: 0.7823, Val Acc: 0.7534, Loss: 0.5188\n",
      "Epoch: 415, Train Acc: 0.7676, Test Acc: 0.7811, Val Acc: 0.7625, Loss: 0.5193\n",
      "Epoch: 416, Train Acc: 0.7660, Test Acc: 0.7793, Val Acc: 0.7685, Loss: 0.5206\n",
      "Epoch: 417, Train Acc: 0.7669, Test Acc: 0.7780, Val Acc: 0.7655, Loss: 0.5191\n",
      "Epoch: 418, Train Acc: 0.7699, Test Acc: 0.7811, Val Acc: 0.7700, Loss: 0.5165\n",
      "Epoch: 419, Train Acc: 0.7694, Test Acc: 0.7774, Val Acc: 0.7700, Loss: 0.5236\n",
      "Epoch: 420, Train Acc: 0.7667, Test Acc: 0.7796, Val Acc: 0.7670, Loss: 0.5170\n",
      "Epoch: 421, Train Acc: 0.7623, Test Acc: 0.7726, Val Acc: 0.7700, Loss: 0.5208\n",
      "Epoch: 422, Train Acc: 0.7626, Test Acc: 0.7744, Val Acc: 0.7670, Loss: 0.5195\n",
      "Epoch: 423, Train Acc: 0.7687, Test Acc: 0.7820, Val Acc: 0.7806, Loss: 0.5208\n",
      "Epoch: 424, Train Acc: 0.7687, Test Acc: 0.7780, Val Acc: 0.7806, Loss: 0.5159\n",
      "Epoch: 425, Train Acc: 0.7636, Test Acc: 0.7732, Val Acc: 0.7595, Loss: 0.5181\n",
      "Epoch: 426, Train Acc: 0.7640, Test Acc: 0.7756, Val Acc: 0.7595, Loss: 0.5169\n",
      "Epoch: 427, Train Acc: 0.7717, Test Acc: 0.7823, Val Acc: 0.7655, Loss: 0.5211\n",
      "Epoch: 428, Train Acc: 0.7734, Test Acc: 0.7805, Val Acc: 0.7655, Loss: 0.5200\n",
      "Epoch: 429, Train Acc: 0.7725, Test Acc: 0.7783, Val Acc: 0.7595, Loss: 0.5168\n",
      "Epoch: 430, Train Acc: 0.7683, Test Acc: 0.7744, Val Acc: 0.7564, Loss: 0.5245\n",
      "Epoch: 431, Train Acc: 0.7686, Test Acc: 0.7771, Val Acc: 0.7655, Loss: 0.5223\n",
      "Epoch: 432, Train Acc: 0.7668, Test Acc: 0.7732, Val Acc: 0.7670, Loss: 0.5244\n",
      "Epoch: 433, Train Acc: 0.7674, Test Acc: 0.7744, Val Acc: 0.7655, Loss: 0.5235\n",
      "Epoch: 434, Train Acc: 0.7656, Test Acc: 0.7741, Val Acc: 0.7579, Loss: 0.5163\n",
      "Epoch: 435, Train Acc: 0.7642, Test Acc: 0.7726, Val Acc: 0.7610, Loss: 0.5222\n",
      "Epoch: 436, Train Acc: 0.7651, Test Acc: 0.7759, Val Acc: 0.7595, Loss: 0.5201\n",
      "Epoch: 437, Train Acc: 0.7683, Test Acc: 0.7793, Val Acc: 0.7595, Loss: 0.5160\n",
      "Epoch: 438, Train Acc: 0.7686, Test Acc: 0.7768, Val Acc: 0.7655, Loss: 0.5200\n",
      "Epoch: 439, Train Acc: 0.7663, Test Acc: 0.7747, Val Acc: 0.7685, Loss: 0.5211\n",
      "Epoch: 440, Train Acc: 0.7659, Test Acc: 0.7738, Val Acc: 0.7716, Loss: 0.5217\n",
      "Epoch: 441, Train Acc: 0.7643, Test Acc: 0.7744, Val Acc: 0.7655, Loss: 0.5140\n",
      "Epoch: 442, Train Acc: 0.7689, Test Acc: 0.7832, Val Acc: 0.7579, Loss: 0.5244\n",
      "Epoch: 443, Train Acc: 0.7688, Test Acc: 0.7829, Val Acc: 0.7519, Loss: 0.5188\n",
      "Epoch: 444, Train Acc: 0.7662, Test Acc: 0.7793, Val Acc: 0.7564, Loss: 0.5180\n",
      "Epoch: 445, Train Acc: 0.7681, Test Acc: 0.7847, Val Acc: 0.7700, Loss: 0.5186\n",
      "Epoch: 446, Train Acc: 0.7708, Test Acc: 0.7871, Val Acc: 0.7776, Loss: 0.5105\n",
      "Epoch: 447, Train Acc: 0.7709, Test Acc: 0.7871, Val Acc: 0.7821, Loss: 0.5218\n",
      "Epoch: 448, Train Acc: 0.7698, Test Acc: 0.7850, Val Acc: 0.7791, Loss: 0.5206\n",
      "Epoch: 449, Train Acc: 0.7691, Test Acc: 0.7820, Val Acc: 0.7746, Loss: 0.5204\n",
      "Epoch: 450, Train Acc: 0.7692, Test Acc: 0.7811, Val Acc: 0.7716, Loss: 0.5226\n",
      "Epoch: 451, Train Acc: 0.7707, Test Acc: 0.7826, Val Acc: 0.7761, Loss: 0.5259\n",
      "Epoch: 452, Train Acc: 0.7703, Test Acc: 0.7787, Val Acc: 0.7761, Loss: 0.5215\n",
      "Epoch: 453, Train Acc: 0.7635, Test Acc: 0.7738, Val Acc: 0.7685, Loss: 0.5181\n",
      "Epoch: 454, Train Acc: 0.7643, Test Acc: 0.7741, Val Acc: 0.7670, Loss: 0.5236\n",
      "Epoch: 455, Train Acc: 0.7676, Test Acc: 0.7768, Val Acc: 0.7700, Loss: 0.5204\n",
      "Epoch: 456, Train Acc: 0.7682, Test Acc: 0.7796, Val Acc: 0.7625, Loss: 0.5214\n",
      "Epoch: 457, Train Acc: 0.7683, Test Acc: 0.7780, Val Acc: 0.7655, Loss: 0.5192\n",
      "Epoch: 458, Train Acc: 0.7632, Test Acc: 0.7726, Val Acc: 0.7504, Loss: 0.5165\n",
      "Epoch: 459, Train Acc: 0.7662, Test Acc: 0.7777, Val Acc: 0.7564, Loss: 0.5207\n",
      "Epoch: 460, Train Acc: 0.7684, Test Acc: 0.7774, Val Acc: 0.7625, Loss: 0.5179\n",
      "Epoch: 461, Train Acc: 0.7646, Test Acc: 0.7747, Val Acc: 0.7564, Loss: 0.5249\n",
      "Epoch: 462, Train Acc: 0.7667, Test Acc: 0.7777, Val Acc: 0.7685, Loss: 0.5305\n",
      "Epoch: 463, Train Acc: 0.7650, Test Acc: 0.7753, Val Acc: 0.7700, Loss: 0.5216\n",
      "Epoch: 464, Train Acc: 0.7649, Test Acc: 0.7793, Val Acc: 0.7746, Loss: 0.5226\n",
      "Epoch: 465, Train Acc: 0.7654, Test Acc: 0.7799, Val Acc: 0.7685, Loss: 0.5199\n",
      "Epoch: 466, Train Acc: 0.7640, Test Acc: 0.7753, Val Acc: 0.7655, Loss: 0.5193\n",
      "Epoch: 467, Train Acc: 0.7655, Test Acc: 0.7790, Val Acc: 0.7655, Loss: 0.5191\n",
      "Epoch: 468, Train Acc: 0.7681, Test Acc: 0.7808, Val Acc: 0.7670, Loss: 0.5200\n",
      "Epoch: 469, Train Acc: 0.7713, Test Acc: 0.7829, Val Acc: 0.7776, Loss: 0.5210\n",
      "Epoch: 470, Train Acc: 0.7699, Test Acc: 0.7829, Val Acc: 0.7806, Loss: 0.5255\n",
      "Epoch: 471, Train Acc: 0.7668, Test Acc: 0.7790, Val Acc: 0.7670, Loss: 0.5207\n",
      "Epoch: 472, Train Acc: 0.7663, Test Acc: 0.7768, Val Acc: 0.7595, Loss: 0.5209\n",
      "Epoch: 473, Train Acc: 0.7686, Test Acc: 0.7793, Val Acc: 0.7640, Loss: 0.5250\n",
      "Epoch: 474, Train Acc: 0.7691, Test Acc: 0.7811, Val Acc: 0.7640, Loss: 0.5233\n",
      "Epoch: 475, Train Acc: 0.7660, Test Acc: 0.7768, Val Acc: 0.7685, Loss: 0.5222\n",
      "Epoch: 476, Train Acc: 0.7638, Test Acc: 0.7738, Val Acc: 0.7610, Loss: 0.5144\n",
      "Epoch: 477, Train Acc: 0.7702, Test Acc: 0.7780, Val Acc: 0.7640, Loss: 0.5209\n",
      "Epoch: 478, Train Acc: 0.7711, Test Acc: 0.7790, Val Acc: 0.7685, Loss: 0.5190\n",
      "Epoch: 479, Train Acc: 0.7697, Test Acc: 0.7783, Val Acc: 0.7700, Loss: 0.5180\n",
      "Epoch: 480, Train Acc: 0.7689, Test Acc: 0.7808, Val Acc: 0.7700, Loss: 0.5190\n",
      "Epoch: 481, Train Acc: 0.7708, Test Acc: 0.7814, Val Acc: 0.7655, Loss: 0.5232\n",
      "Epoch: 482, Train Acc: 0.7705, Test Acc: 0.7802, Val Acc: 0.7640, Loss: 0.5174\n",
      "Epoch: 483, Train Acc: 0.7677, Test Acc: 0.7808, Val Acc: 0.7685, Loss: 0.5159\n",
      "Epoch: 484, Train Acc: 0.7701, Test Acc: 0.7823, Val Acc: 0.7670, Loss: 0.5268\n",
      "Epoch: 485, Train Acc: 0.7715, Test Acc: 0.7799, Val Acc: 0.7625, Loss: 0.5180\n",
      "Epoch: 486, Train Acc: 0.7742, Test Acc: 0.7814, Val Acc: 0.7655, Loss: 0.5198\n",
      "Epoch: 487, Train Acc: 0.7688, Test Acc: 0.7780, Val Acc: 0.7625, Loss: 0.5152\n",
      "Epoch: 488, Train Acc: 0.7660, Test Acc: 0.7750, Val Acc: 0.7640, Loss: 0.5135\n",
      "Epoch: 489, Train Acc: 0.7678, Test Acc: 0.7756, Val Acc: 0.7625, Loss: 0.5224\n",
      "Epoch: 490, Train Acc: 0.7711, Test Acc: 0.7805, Val Acc: 0.7595, Loss: 0.5182\n",
      "Epoch: 491, Train Acc: 0.7709, Test Acc: 0.7799, Val Acc: 0.7625, Loss: 0.5226\n",
      "Epoch: 492, Train Acc: 0.7655, Test Acc: 0.7747, Val Acc: 0.7564, Loss: 0.5146\n",
      "Epoch: 493, Train Acc: 0.7697, Test Acc: 0.7811, Val Acc: 0.7746, Loss: 0.5222\n",
      "Epoch: 494, Train Acc: 0.7705, Test Acc: 0.7838, Val Acc: 0.7700, Loss: 0.5191\n",
      "Epoch: 495, Train Acc: 0.7651, Test Acc: 0.7802, Val Acc: 0.7731, Loss: 0.5246\n",
      "Epoch: 496, Train Acc: 0.7629, Test Acc: 0.7750, Val Acc: 0.7670, Loss: 0.5200\n",
      "Epoch: 497, Train Acc: 0.7650, Test Acc: 0.7777, Val Acc: 0.7700, Loss: 0.5206\n",
      "Epoch: 498, Train Acc: 0.7634, Test Acc: 0.7753, Val Acc: 0.7700, Loss: 0.5181\n",
      "Epoch: 499, Train Acc: 0.7641, Test Acc: 0.7741, Val Acc: 0.7700, Loss: 0.5207\n",
      "Epoch: 500, Train Acc: 0.7655, Test Acc: 0.7753, Val Acc: 0.7670, Loss: 0.5205\n",
      "Epoch: 501, Train Acc: 0.7670, Test Acc: 0.7765, Val Acc: 0.7716, Loss: 0.5231\n",
      "Epoch: 502, Train Acc: 0.7699, Test Acc: 0.7796, Val Acc: 0.7776, Loss: 0.5196\n",
      "Epoch: 503, Train Acc: 0.7689, Test Acc: 0.7805, Val Acc: 0.7716, Loss: 0.5197\n",
      "Epoch: 504, Train Acc: 0.7689, Test Acc: 0.7799, Val Acc: 0.7685, Loss: 0.5165\n",
      "Epoch: 505, Train Acc: 0.7717, Test Acc: 0.7787, Val Acc: 0.7655, Loss: 0.5243\n",
      "Epoch: 506, Train Acc: 0.7719, Test Acc: 0.7793, Val Acc: 0.7655, Loss: 0.5185\n",
      "Epoch: 507, Train Acc: 0.7669, Test Acc: 0.7799, Val Acc: 0.7731, Loss: 0.5209\n",
      "Epoch: 508, Train Acc: 0.7643, Test Acc: 0.7759, Val Acc: 0.7655, Loss: 0.5230\n",
      "Epoch: 509, Train Acc: 0.7675, Test Acc: 0.7738, Val Acc: 0.7595, Loss: 0.5187\n",
      "Epoch: 510, Train Acc: 0.7669, Test Acc: 0.7723, Val Acc: 0.7595, Loss: 0.5238\n",
      "Epoch: 511, Train Acc: 0.7622, Test Acc: 0.7714, Val Acc: 0.7549, Loss: 0.5141\n",
      "Epoch: 512, Train Acc: 0.7673, Test Acc: 0.7726, Val Acc: 0.7625, Loss: 0.5205\n",
      "Epoch: 513, Train Acc: 0.7683, Test Acc: 0.7750, Val Acc: 0.7670, Loss: 0.5219\n",
      "Epoch: 514, Train Acc: 0.7686, Test Acc: 0.7735, Val Acc: 0.7655, Loss: 0.5223\n",
      "Epoch: 515, Train Acc: 0.7651, Test Acc: 0.7735, Val Acc: 0.7625, Loss: 0.5223\n",
      "Epoch: 516, Train Acc: 0.7681, Test Acc: 0.7750, Val Acc: 0.7685, Loss: 0.5221\n",
      "Epoch: 517, Train Acc: 0.7697, Test Acc: 0.7799, Val Acc: 0.7685, Loss: 0.5257\n",
      "Epoch: 518, Train Acc: 0.7653, Test Acc: 0.7790, Val Acc: 0.7640, Loss: 0.5207\n",
      "Epoch: 519, Train Acc: 0.7674, Test Acc: 0.7811, Val Acc: 0.7731, Loss: 0.5260\n",
      "Epoch: 520, Train Acc: 0.7714, Test Acc: 0.7880, Val Acc: 0.7655, Loss: 0.5186\n",
      "Epoch: 521, Train Acc: 0.7707, Test Acc: 0.7850, Val Acc: 0.7670, Loss: 0.5196\n",
      "Epoch: 522, Train Acc: 0.7686, Test Acc: 0.7844, Val Acc: 0.7640, Loss: 0.5243\n",
      "Epoch: 523, Train Acc: 0.7675, Test Acc: 0.7796, Val Acc: 0.7625, Loss: 0.5128\n",
      "Epoch: 524, Train Acc: 0.7716, Test Acc: 0.7853, Val Acc: 0.7700, Loss: 0.5137\n",
      "Epoch: 525, Train Acc: 0.7726, Test Acc: 0.7847, Val Acc: 0.7685, Loss: 0.5256\n",
      "Epoch: 526, Train Acc: 0.7669, Test Acc: 0.7790, Val Acc: 0.7595, Loss: 0.5218\n",
      "Epoch: 527, Train Acc: 0.7671, Test Acc: 0.7787, Val Acc: 0.7625, Loss: 0.5193\n",
      "Epoch: 528, Train Acc: 0.7695, Test Acc: 0.7774, Val Acc: 0.7595, Loss: 0.5207\n",
      "Epoch: 529, Train Acc: 0.7719, Test Acc: 0.7787, Val Acc: 0.7579, Loss: 0.5250\n",
      "Epoch: 530, Train Acc: 0.7677, Test Acc: 0.7793, Val Acc: 0.7610, Loss: 0.5189\n",
      "Epoch: 531, Train Acc: 0.7662, Test Acc: 0.7729, Val Acc: 0.7655, Loss: 0.5209\n",
      "Epoch: 532, Train Acc: 0.7683, Test Acc: 0.7732, Val Acc: 0.7610, Loss: 0.5276\n",
      "Epoch: 533, Train Acc: 0.7709, Test Acc: 0.7802, Val Acc: 0.7700, Loss: 0.5212\n",
      "Epoch: 534, Train Acc: 0.7684, Test Acc: 0.7753, Val Acc: 0.7595, Loss: 0.5161\n",
      "Epoch: 535, Train Acc: 0.7653, Test Acc: 0.7732, Val Acc: 0.7625, Loss: 0.5123\n",
      "Epoch: 536, Train Acc: 0.7686, Test Acc: 0.7759, Val Acc: 0.7595, Loss: 0.5168\n",
      "Epoch: 537, Train Acc: 0.7714, Test Acc: 0.7814, Val Acc: 0.7610, Loss: 0.5155\n",
      "Epoch: 538, Train Acc: 0.7727, Test Acc: 0.7856, Val Acc: 0.7595, Loss: 0.5170\n",
      "Epoch: 539, Train Acc: 0.7710, Test Acc: 0.7847, Val Acc: 0.7670, Loss: 0.5144\n",
      "Epoch: 540, Train Acc: 0.7692, Test Acc: 0.7859, Val Acc: 0.7731, Loss: 0.5164\n",
      "Epoch: 541, Train Acc: 0.7727, Test Acc: 0.7904, Val Acc: 0.7685, Loss: 0.5179\n",
      "Epoch: 542, Train Acc: 0.7737, Test Acc: 0.7880, Val Acc: 0.7700, Loss: 0.5190\n",
      "Epoch: 543, Train Acc: 0.7632, Test Acc: 0.7762, Val Acc: 0.7670, Loss: 0.5126\n",
      "Epoch: 544, Train Acc: 0.7677, Test Acc: 0.7811, Val Acc: 0.7685, Loss: 0.5228\n",
      "Epoch: 545, Train Acc: 0.7744, Test Acc: 0.7871, Val Acc: 0.7670, Loss: 0.5200\n",
      "Epoch: 546, Train Acc: 0.7691, Test Acc: 0.7780, Val Acc: 0.7610, Loss: 0.5169\n",
      "Epoch: 547, Train Acc: 0.7631, Test Acc: 0.7738, Val Acc: 0.7716, Loss: 0.5160\n",
      "Epoch: 548, Train Acc: 0.7685, Test Acc: 0.7802, Val Acc: 0.7625, Loss: 0.5194\n",
      "Epoch: 549, Train Acc: 0.7726, Test Acc: 0.7811, Val Acc: 0.7655, Loss: 0.5257\n",
      "Epoch: 550, Train Acc: 0.7672, Test Acc: 0.7838, Val Acc: 0.7685, Loss: 0.5231\n",
      "Epoch: 551, Train Acc: 0.7578, Test Acc: 0.7690, Val Acc: 0.7413, Loss: 0.5232\n",
      "Epoch: 552, Train Acc: 0.7636, Test Acc: 0.7799, Val Acc: 0.7625, Loss: 0.5261\n",
      "Epoch: 553, Train Acc: 0.7710, Test Acc: 0.7847, Val Acc: 0.7655, Loss: 0.5227\n",
      "Epoch: 554, Train Acc: 0.7672, Test Acc: 0.7783, Val Acc: 0.7640, Loss: 0.5230\n",
      "Epoch: 555, Train Acc: 0.7614, Test Acc: 0.7744, Val Acc: 0.7534, Loss: 0.5191\n",
      "Epoch: 556, Train Acc: 0.7670, Test Acc: 0.7762, Val Acc: 0.7670, Loss: 0.5243\n",
      "Epoch: 557, Train Acc: 0.7698, Test Acc: 0.7802, Val Acc: 0.7640, Loss: 0.5235\n",
      "Epoch: 558, Train Acc: 0.7688, Test Acc: 0.7829, Val Acc: 0.7625, Loss: 0.5215\n",
      "Epoch: 559, Train Acc: 0.7636, Test Acc: 0.7780, Val Acc: 0.7564, Loss: 0.5192\n",
      "Epoch: 560, Train Acc: 0.7590, Test Acc: 0.7771, Val Acc: 0.7534, Loss: 0.5202\n",
      "Epoch: 561, Train Acc: 0.7682, Test Acc: 0.7805, Val Acc: 0.7640, Loss: 0.5206\n",
      "Epoch: 562, Train Acc: 0.7688, Test Acc: 0.7805, Val Acc: 0.7655, Loss: 0.5199\n",
      "Epoch: 563, Train Acc: 0.7653, Test Acc: 0.7793, Val Acc: 0.7731, Loss: 0.5197\n",
      "Epoch: 564, Train Acc: 0.7673, Test Acc: 0.7814, Val Acc: 0.7595, Loss: 0.5174\n",
      "Epoch: 565, Train Acc: 0.7710, Test Acc: 0.7850, Val Acc: 0.7731, Loss: 0.5187\n",
      "Epoch: 566, Train Acc: 0.7685, Test Acc: 0.7832, Val Acc: 0.7625, Loss: 0.5222\n",
      "Epoch: 567, Train Acc: 0.7634, Test Acc: 0.7756, Val Acc: 0.7564, Loss: 0.5210\n",
      "Epoch: 568, Train Acc: 0.7672, Test Acc: 0.7780, Val Acc: 0.7700, Loss: 0.5171\n",
      "Epoch: 569, Train Acc: 0.7716, Test Acc: 0.7808, Val Acc: 0.7700, Loss: 0.5256\n",
      "Epoch: 570, Train Acc: 0.7684, Test Acc: 0.7762, Val Acc: 0.7595, Loss: 0.5232\n",
      "Epoch: 571, Train Acc: 0.7641, Test Acc: 0.7759, Val Acc: 0.7595, Loss: 0.5184\n",
      "Epoch: 572, Train Acc: 0.7629, Test Acc: 0.7714, Val Acc: 0.7595, Loss: 0.5129\n",
      "Epoch: 573, Train Acc: 0.7676, Test Acc: 0.7768, Val Acc: 0.7685, Loss: 0.5234\n",
      "Epoch: 574, Train Acc: 0.7690, Test Acc: 0.7790, Val Acc: 0.7640, Loss: 0.5205\n",
      "Epoch: 575, Train Acc: 0.7675, Test Acc: 0.7753, Val Acc: 0.7610, Loss: 0.5167\n",
      "Epoch: 576, Train Acc: 0.7658, Test Acc: 0.7738, Val Acc: 0.7640, Loss: 0.5208\n",
      "Epoch: 577, Train Acc: 0.7689, Test Acc: 0.7759, Val Acc: 0.7670, Loss: 0.5212\n",
      "Epoch: 578, Train Acc: 0.7672, Test Acc: 0.7765, Val Acc: 0.7640, Loss: 0.5106\n",
      "Epoch: 579, Train Acc: 0.7644, Test Acc: 0.7768, Val Acc: 0.7534, Loss: 0.5170\n",
      "Epoch: 580, Train Acc: 0.7669, Test Acc: 0.7799, Val Acc: 0.7776, Loss: 0.5165\n",
      "Epoch: 581, Train Acc: 0.7681, Test Acc: 0.7787, Val Acc: 0.7761, Loss: 0.5243\n",
      "Epoch: 582, Train Acc: 0.7695, Test Acc: 0.7777, Val Acc: 0.7716, Loss: 0.5197\n",
      "Epoch: 583, Train Acc: 0.7674, Test Acc: 0.7774, Val Acc: 0.7640, Loss: 0.5158\n",
      "Epoch: 584, Train Acc: 0.7632, Test Acc: 0.7702, Val Acc: 0.7564, Loss: 0.5240\n",
      "Epoch: 585, Train Acc: 0.7694, Test Acc: 0.7720, Val Acc: 0.7640, Loss: 0.5198\n",
      "Epoch: 586, Train Acc: 0.7700, Test Acc: 0.7729, Val Acc: 0.7746, Loss: 0.5171\n",
      "Epoch: 587, Train Acc: 0.7705, Test Acc: 0.7765, Val Acc: 0.7670, Loss: 0.5214\n",
      "Epoch: 588, Train Acc: 0.7700, Test Acc: 0.7777, Val Acc: 0.7655, Loss: 0.5160\n",
      "Epoch: 589, Train Acc: 0.7728, Test Acc: 0.7805, Val Acc: 0.7655, Loss: 0.5123\n",
      "Epoch: 590, Train Acc: 0.7728, Test Acc: 0.7826, Val Acc: 0.7610, Loss: 0.5211\n",
      "Epoch: 591, Train Acc: 0.7700, Test Acc: 0.7835, Val Acc: 0.7549, Loss: 0.5186\n",
      "Epoch: 592, Train Acc: 0.7662, Test Acc: 0.7787, Val Acc: 0.7610, Loss: 0.5244\n",
      "Epoch: 593, Train Acc: 0.7691, Test Acc: 0.7790, Val Acc: 0.7549, Loss: 0.5248\n",
      "Epoch: 594, Train Acc: 0.7719, Test Acc: 0.7838, Val Acc: 0.7670, Loss: 0.5247\n",
      "Epoch: 595, Train Acc: 0.7685, Test Acc: 0.7823, Val Acc: 0.7685, Loss: 0.5171\n",
      "Epoch: 596, Train Acc: 0.7668, Test Acc: 0.7805, Val Acc: 0.7579, Loss: 0.5211\n",
      "Epoch: 597, Train Acc: 0.7696, Test Acc: 0.7835, Val Acc: 0.7610, Loss: 0.5186\n",
      "Epoch: 598, Train Acc: 0.7695, Test Acc: 0.7814, Val Acc: 0.7655, Loss: 0.5239\n",
      "Epoch: 599, Train Acc: 0.7699, Test Acc: 0.7820, Val Acc: 0.7746, Loss: 0.5187\n",
      "Epoch: 600, Train Acc: 0.7719, Test Acc: 0.7823, Val Acc: 0.7746, Loss: 0.5220\n",
      "Epoch: 601, Train Acc: 0.7716, Test Acc: 0.7820, Val Acc: 0.7716, Loss: 0.5095\n",
      "Epoch: 602, Train Acc: 0.7690, Test Acc: 0.7777, Val Acc: 0.7700, Loss: 0.5182\n",
      "Epoch: 603, Train Acc: 0.7663, Test Acc: 0.7768, Val Acc: 0.7655, Loss: 0.5116\n",
      "Epoch: 604, Train Acc: 0.7665, Test Acc: 0.7774, Val Acc: 0.7670, Loss: 0.5219\n",
      "Epoch: 605, Train Acc: 0.7677, Test Acc: 0.7783, Val Acc: 0.7716, Loss: 0.5170\n",
      "Epoch: 606, Train Acc: 0.7701, Test Acc: 0.7823, Val Acc: 0.7716, Loss: 0.5200\n",
      "Epoch: 607, Train Acc: 0.7699, Test Acc: 0.7811, Val Acc: 0.7595, Loss: 0.5197\n",
      "Epoch: 608, Train Acc: 0.7676, Test Acc: 0.7817, Val Acc: 0.7519, Loss: 0.5181\n",
      "Epoch: 609, Train Acc: 0.7709, Test Acc: 0.7826, Val Acc: 0.7595, Loss: 0.5276\n",
      "Epoch: 610, Train Acc: 0.7711, Test Acc: 0.7823, Val Acc: 0.7549, Loss: 0.5206\n",
      "Epoch: 611, Train Acc: 0.7688, Test Acc: 0.7771, Val Acc: 0.7579, Loss: 0.5206\n",
      "Epoch: 612, Train Acc: 0.7676, Test Acc: 0.7777, Val Acc: 0.7595, Loss: 0.5200\n",
      "Epoch: 613, Train Acc: 0.7715, Test Acc: 0.7847, Val Acc: 0.7640, Loss: 0.5178\n",
      "Epoch: 614, Train Acc: 0.7723, Test Acc: 0.7847, Val Acc: 0.7640, Loss: 0.5175\n",
      "Epoch: 615, Train Acc: 0.7710, Test Acc: 0.7817, Val Acc: 0.7610, Loss: 0.5211\n",
      "Epoch: 616, Train Acc: 0.7645, Test Acc: 0.7756, Val Acc: 0.7443, Loss: 0.5159\n",
      "Epoch: 617, Train Acc: 0.7696, Test Acc: 0.7790, Val Acc: 0.7474, Loss: 0.5217\n",
      "Epoch: 618, Train Acc: 0.7710, Test Acc: 0.7826, Val Acc: 0.7519, Loss: 0.5231\n",
      "Epoch: 619, Train Acc: 0.7688, Test Acc: 0.7783, Val Acc: 0.7625, Loss: 0.5231\n",
      "Epoch: 620, Train Acc: 0.7690, Test Acc: 0.7826, Val Acc: 0.7625, Loss: 0.5156\n",
      "Epoch: 621, Train Acc: 0.7704, Test Acc: 0.7823, Val Acc: 0.7685, Loss: 0.5173\n",
      "Epoch: 622, Train Acc: 0.7685, Test Acc: 0.7793, Val Acc: 0.7640, Loss: 0.5163\n",
      "Epoch: 623, Train Acc: 0.7685, Test Acc: 0.7750, Val Acc: 0.7655, Loss: 0.5251\n",
      "Epoch: 624, Train Acc: 0.7676, Test Acc: 0.7768, Val Acc: 0.7685, Loss: 0.5238\n",
      "Epoch: 625, Train Acc: 0.7692, Test Acc: 0.7793, Val Acc: 0.7685, Loss: 0.5203\n",
      "Epoch: 626, Train Acc: 0.7659, Test Acc: 0.7777, Val Acc: 0.7776, Loss: 0.5213\n",
      "Epoch: 627, Train Acc: 0.7616, Test Acc: 0.7735, Val Acc: 0.7655, Loss: 0.5241\n",
      "Epoch: 628, Train Acc: 0.7623, Test Acc: 0.7756, Val Acc: 0.7640, Loss: 0.5194\n",
      "Epoch: 629, Train Acc: 0.7631, Test Acc: 0.7723, Val Acc: 0.7625, Loss: 0.5235\n",
      "Epoch: 630, Train Acc: 0.7650, Test Acc: 0.7747, Val Acc: 0.7564, Loss: 0.5196\n",
      "Epoch: 631, Train Acc: 0.7607, Test Acc: 0.7720, Val Acc: 0.7610, Loss: 0.5202\n",
      "Epoch: 632, Train Acc: 0.7632, Test Acc: 0.7756, Val Acc: 0.7655, Loss: 0.5164\n",
      "Epoch: 633, Train Acc: 0.7707, Test Acc: 0.7826, Val Acc: 0.7761, Loss: 0.5261\n",
      "Epoch: 634, Train Acc: 0.7718, Test Acc: 0.7823, Val Acc: 0.7685, Loss: 0.5195\n",
      "Epoch: 635, Train Acc: 0.7664, Test Acc: 0.7759, Val Acc: 0.7716, Loss: 0.5201\n",
      "Epoch: 636, Train Acc: 0.7665, Test Acc: 0.7738, Val Acc: 0.7716, Loss: 0.5185\n",
      "Epoch: 637, Train Acc: 0.7680, Test Acc: 0.7738, Val Acc: 0.7761, Loss: 0.5232\n",
      "Epoch: 638, Train Acc: 0.7673, Test Acc: 0.7738, Val Acc: 0.7716, Loss: 0.5238\n",
      "Epoch: 639, Train Acc: 0.7656, Test Acc: 0.7765, Val Acc: 0.7610, Loss: 0.5228\n",
      "Epoch: 640, Train Acc: 0.7653, Test Acc: 0.7756, Val Acc: 0.7670, Loss: 0.5230\n",
      "Epoch: 641, Train Acc: 0.7676, Test Acc: 0.7783, Val Acc: 0.7685, Loss: 0.5184\n",
      "Epoch: 642, Train Acc: 0.7685, Test Acc: 0.7780, Val Acc: 0.7731, Loss: 0.5191\n",
      "Epoch: 643, Train Acc: 0.7671, Test Acc: 0.7756, Val Acc: 0.7700, Loss: 0.5220\n",
      "Epoch: 644, Train Acc: 0.7712, Test Acc: 0.7808, Val Acc: 0.7670, Loss: 0.5178\n",
      "Epoch: 645, Train Acc: 0.7698, Test Acc: 0.7805, Val Acc: 0.7625, Loss: 0.5191\n",
      "Epoch: 646, Train Acc: 0.7658, Test Acc: 0.7826, Val Acc: 0.7625, Loss: 0.5226\n",
      "Epoch: 647, Train Acc: 0.7686, Test Acc: 0.7838, Val Acc: 0.7655, Loss: 0.5249\n",
      "Epoch: 648, Train Acc: 0.7696, Test Acc: 0.7796, Val Acc: 0.7685, Loss: 0.5200\n",
      "Epoch: 649, Train Acc: 0.7668, Test Acc: 0.7753, Val Acc: 0.7685, Loss: 0.5196\n",
      "Epoch: 650, Train Acc: 0.7640, Test Acc: 0.7729, Val Acc: 0.7670, Loss: 0.5242\n",
      "Epoch: 651, Train Acc: 0.7677, Test Acc: 0.7750, Val Acc: 0.7731, Loss: 0.5210\n",
      "Epoch: 652, Train Acc: 0.7712, Test Acc: 0.7783, Val Acc: 0.7700, Loss: 0.5188\n",
      "Epoch: 653, Train Acc: 0.7643, Test Acc: 0.7735, Val Acc: 0.7655, Loss: 0.5245\n",
      "Epoch: 654, Train Acc: 0.7629, Test Acc: 0.7726, Val Acc: 0.7564, Loss: 0.5217\n",
      "Epoch: 655, Train Acc: 0.7680, Test Acc: 0.7777, Val Acc: 0.7625, Loss: 0.5200\n",
      "Epoch: 656, Train Acc: 0.7719, Test Acc: 0.7823, Val Acc: 0.7731, Loss: 0.5216\n",
      "Epoch: 657, Train Acc: 0.7676, Test Acc: 0.7783, Val Acc: 0.7610, Loss: 0.5236\n",
      "Epoch: 658, Train Acc: 0.7632, Test Acc: 0.7741, Val Acc: 0.7610, Loss: 0.5217\n",
      "Epoch: 659, Train Acc: 0.7691, Test Acc: 0.7765, Val Acc: 0.7670, Loss: 0.5157\n",
      "Epoch: 660, Train Acc: 0.7722, Test Acc: 0.7783, Val Acc: 0.7625, Loss: 0.5180\n",
      "Epoch: 661, Train Acc: 0.7670, Test Acc: 0.7774, Val Acc: 0.7640, Loss: 0.5174\n",
      "Epoch: 662, Train Acc: 0.7646, Test Acc: 0.7747, Val Acc: 0.7579, Loss: 0.5200\n",
      "Epoch: 663, Train Acc: 0.7739, Test Acc: 0.7826, Val Acc: 0.7746, Loss: 0.5204\n",
      "Epoch: 664, Train Acc: 0.7737, Test Acc: 0.7829, Val Acc: 0.7640, Loss: 0.5194\n",
      "Epoch: 665, Train Acc: 0.7641, Test Acc: 0.7759, Val Acc: 0.7564, Loss: 0.5256\n",
      "Epoch: 666, Train Acc: 0.7632, Test Acc: 0.7747, Val Acc: 0.7595, Loss: 0.5271\n",
      "Epoch: 667, Train Acc: 0.7683, Test Acc: 0.7732, Val Acc: 0.7731, Loss: 0.5251\n",
      "Epoch: 668, Train Acc: 0.7689, Test Acc: 0.7750, Val Acc: 0.7761, Loss: 0.5169\n",
      "Epoch: 669, Train Acc: 0.7674, Test Acc: 0.7768, Val Acc: 0.7685, Loss: 0.5176\n",
      "Epoch: 670, Train Acc: 0.7661, Test Acc: 0.7774, Val Acc: 0.7670, Loss: 0.5221\n",
      "Epoch: 671, Train Acc: 0.7667, Test Acc: 0.7768, Val Acc: 0.7716, Loss: 0.5234\n",
      "Epoch: 672, Train Acc: 0.7670, Test Acc: 0.7814, Val Acc: 0.7776, Loss: 0.5206\n",
      "Epoch: 673, Train Acc: 0.7650, Test Acc: 0.7756, Val Acc: 0.7685, Loss: 0.5216\n",
      "Epoch: 674, Train Acc: 0.7629, Test Acc: 0.7756, Val Acc: 0.7716, Loss: 0.5269\n",
      "Epoch: 675, Train Acc: 0.7667, Test Acc: 0.7829, Val Acc: 0.7761, Loss: 0.5172\n",
      "Epoch: 676, Train Acc: 0.7688, Test Acc: 0.7838, Val Acc: 0.7716, Loss: 0.5229\n",
      "Epoch: 677, Train Acc: 0.7673, Test Acc: 0.7832, Val Acc: 0.7670, Loss: 0.5228\n",
      "Epoch: 678, Train Acc: 0.7674, Test Acc: 0.7814, Val Acc: 0.7595, Loss: 0.5274\n",
      "Epoch: 679, Train Acc: 0.7642, Test Acc: 0.7783, Val Acc: 0.7504, Loss: 0.5205\n",
      "Epoch: 680, Train Acc: 0.7655, Test Acc: 0.7771, Val Acc: 0.7579, Loss: 0.5150\n",
      "Epoch: 681, Train Acc: 0.7688, Test Acc: 0.7768, Val Acc: 0.7610, Loss: 0.5253\n",
      "Epoch: 682, Train Acc: 0.7690, Test Acc: 0.7783, Val Acc: 0.7746, Loss: 0.5216\n",
      "Epoch: 683, Train Acc: 0.7670, Test Acc: 0.7787, Val Acc: 0.7670, Loss: 0.5177\n",
      "Epoch: 684, Train Acc: 0.7672, Test Acc: 0.7783, Val Acc: 0.7640, Loss: 0.5227\n",
      "Epoch: 685, Train Acc: 0.7682, Test Acc: 0.7814, Val Acc: 0.7595, Loss: 0.5217\n",
      "Epoch: 686, Train Acc: 0.7645, Test Acc: 0.7790, Val Acc: 0.7595, Loss: 0.5189\n",
      "Epoch: 687, Train Acc: 0.7671, Test Acc: 0.7805, Val Acc: 0.7625, Loss: 0.5256\n",
      "Epoch: 688, Train Acc: 0.7659, Test Acc: 0.7790, Val Acc: 0.7579, Loss: 0.5243\n",
      "Epoch: 689, Train Acc: 0.7629, Test Acc: 0.7729, Val Acc: 0.7534, Loss: 0.5189\n",
      "Epoch: 690, Train Acc: 0.7597, Test Acc: 0.7705, Val Acc: 0.7534, Loss: 0.5247\n",
      "Epoch: 691, Train Acc: 0.7605, Test Acc: 0.7672, Val Acc: 0.7595, Loss: 0.5159\n",
      "Epoch: 692, Train Acc: 0.7664, Test Acc: 0.7741, Val Acc: 0.7595, Loss: 0.5255\n",
      "Epoch: 693, Train Acc: 0.7662, Test Acc: 0.7759, Val Acc: 0.7625, Loss: 0.5213\n",
      "Epoch: 694, Train Acc: 0.7680, Test Acc: 0.7777, Val Acc: 0.7579, Loss: 0.5217\n",
      "Epoch: 695, Train Acc: 0.7687, Test Acc: 0.7796, Val Acc: 0.7640, Loss: 0.5216\n",
      "Epoch: 696, Train Acc: 0.7722, Test Acc: 0.7790, Val Acc: 0.7670, Loss: 0.5190\n",
      "Epoch: 697, Train Acc: 0.7743, Test Acc: 0.7823, Val Acc: 0.7655, Loss: 0.5205\n",
      "Epoch: 698, Train Acc: 0.7689, Test Acc: 0.7787, Val Acc: 0.7625, Loss: 0.5211\n",
      "Epoch: 699, Train Acc: 0.7648, Test Acc: 0.7741, Val Acc: 0.7579, Loss: 0.5140\n",
      "Epoch: 700, Train Acc: 0.7719, Test Acc: 0.7808, Val Acc: 0.7731, Loss: 0.5297\n",
      "Epoch: 701, Train Acc: 0.7692, Test Acc: 0.7793, Val Acc: 0.7564, Loss: 0.5213\n",
      "Epoch: 702, Train Acc: 0.7642, Test Acc: 0.7747, Val Acc: 0.7549, Loss: 0.5183\n",
      "Epoch: 703, Train Acc: 0.7655, Test Acc: 0.7759, Val Acc: 0.7549, Loss: 0.5191\n",
      "Epoch: 704, Train Acc: 0.7717, Test Acc: 0.7844, Val Acc: 0.7655, Loss: 0.5233\n",
      "Epoch: 705, Train Acc: 0.7708, Test Acc: 0.7844, Val Acc: 0.7595, Loss: 0.5250\n",
      "Epoch: 706, Train Acc: 0.7630, Test Acc: 0.7759, Val Acc: 0.7579, Loss: 0.5258\n",
      "Epoch: 707, Train Acc: 0.7602, Test Acc: 0.7744, Val Acc: 0.7670, Loss: 0.5164\n",
      "Epoch: 708, Train Acc: 0.7691, Test Acc: 0.7793, Val Acc: 0.7564, Loss: 0.5182\n",
      "Epoch: 709, Train Acc: 0.7677, Test Acc: 0.7783, Val Acc: 0.7625, Loss: 0.5194\n",
      "Epoch: 710, Train Acc: 0.7637, Test Acc: 0.7796, Val Acc: 0.7610, Loss: 0.5196\n",
      "Epoch: 711, Train Acc: 0.7691, Test Acc: 0.7832, Val Acc: 0.7610, Loss: 0.5239\n",
      "Epoch: 712, Train Acc: 0.7682, Test Acc: 0.7859, Val Acc: 0.7700, Loss: 0.5264\n",
      "Epoch: 713, Train Acc: 0.7686, Test Acc: 0.7892, Val Acc: 0.7716, Loss: 0.5219\n",
      "Epoch: 714, Train Acc: 0.7665, Test Acc: 0.7856, Val Acc: 0.7640, Loss: 0.5175\n",
      "Epoch: 715, Train Acc: 0.7684, Test Acc: 0.7829, Val Acc: 0.7549, Loss: 0.5172\n",
      "Epoch: 716, Train Acc: 0.7691, Test Acc: 0.7841, Val Acc: 0.7610, Loss: 0.5223\n",
      "Epoch: 717, Train Acc: 0.7713, Test Acc: 0.7814, Val Acc: 0.7685, Loss: 0.5237\n",
      "Epoch: 718, Train Acc: 0.7734, Test Acc: 0.7823, Val Acc: 0.7776, Loss: 0.5229\n",
      "Epoch: 719, Train Acc: 0.7704, Test Acc: 0.7799, Val Acc: 0.7806, Loss: 0.5202\n",
      "Epoch: 720, Train Acc: 0.7696, Test Acc: 0.7793, Val Acc: 0.7731, Loss: 0.5210\n",
      "Epoch: 721, Train Acc: 0.7714, Test Acc: 0.7817, Val Acc: 0.7746, Loss: 0.5188\n",
      "Epoch: 722, Train Acc: 0.7684, Test Acc: 0.7790, Val Acc: 0.7685, Loss: 0.5207\n",
      "Epoch: 723, Train Acc: 0.7656, Test Acc: 0.7768, Val Acc: 0.7670, Loss: 0.5224\n",
      "Epoch: 724, Train Acc: 0.7663, Test Acc: 0.7768, Val Acc: 0.7685, Loss: 0.5225\n",
      "Epoch: 725, Train Acc: 0.7649, Test Acc: 0.7777, Val Acc: 0.7670, Loss: 0.5235\n",
      "Epoch: 726, Train Acc: 0.7615, Test Acc: 0.7750, Val Acc: 0.7716, Loss: 0.5146\n",
      "Epoch: 727, Train Acc: 0.7683, Test Acc: 0.7790, Val Acc: 0.7791, Loss: 0.5234\n",
      "Epoch: 728, Train Acc: 0.7698, Test Acc: 0.7771, Val Acc: 0.7791, Loss: 0.5205\n",
      "Epoch: 729, Train Acc: 0.7640, Test Acc: 0.7738, Val Acc: 0.7685, Loss: 0.5196\n",
      "Epoch: 730, Train Acc: 0.7645, Test Acc: 0.7732, Val Acc: 0.7595, Loss: 0.5298\n",
      "Epoch: 731, Train Acc: 0.7678, Test Acc: 0.7759, Val Acc: 0.7640, Loss: 0.5171\n",
      "Epoch: 732, Train Acc: 0.7683, Test Acc: 0.7787, Val Acc: 0.7761, Loss: 0.5244\n",
      "Epoch: 733, Train Acc: 0.7658, Test Acc: 0.7783, Val Acc: 0.7640, Loss: 0.5215\n",
      "Epoch: 734, Train Acc: 0.7649, Test Acc: 0.7762, Val Acc: 0.7670, Loss: 0.5200\n",
      "Epoch: 735, Train Acc: 0.7674, Test Acc: 0.7793, Val Acc: 0.7700, Loss: 0.5226\n",
      "Epoch: 736, Train Acc: 0.7672, Test Acc: 0.7756, Val Acc: 0.7579, Loss: 0.5234\n",
      "Epoch: 737, Train Acc: 0.7673, Test Acc: 0.7774, Val Acc: 0.7534, Loss: 0.5192\n",
      "Epoch: 738, Train Acc: 0.7712, Test Acc: 0.7799, Val Acc: 0.7640, Loss: 0.5235\n",
      "Epoch: 739, Train Acc: 0.7707, Test Acc: 0.7787, Val Acc: 0.7640, Loss: 0.5258\n",
      "Epoch: 740, Train Acc: 0.7702, Test Acc: 0.7783, Val Acc: 0.7700, Loss: 0.5193\n",
      "Epoch: 741, Train Acc: 0.7683, Test Acc: 0.7771, Val Acc: 0.7625, Loss: 0.5211\n",
      "Epoch: 742, Train Acc: 0.7707, Test Acc: 0.7793, Val Acc: 0.7700, Loss: 0.5159\n",
      "Epoch: 743, Train Acc: 0.7731, Test Acc: 0.7850, Val Acc: 0.7685, Loss: 0.5190\n",
      "Epoch: 744, Train Acc: 0.7658, Test Acc: 0.7783, Val Acc: 0.7746, Loss: 0.5239\n",
      "Epoch: 745, Train Acc: 0.7627, Test Acc: 0.7756, Val Acc: 0.7685, Loss: 0.5244\n",
      "Epoch: 746, Train Acc: 0.7637, Test Acc: 0.7790, Val Acc: 0.7746, Loss: 0.5214\n",
      "Epoch: 747, Train Acc: 0.7633, Test Acc: 0.7780, Val Acc: 0.7655, Loss: 0.5137\n",
      "Epoch: 748, Train Acc: 0.7634, Test Acc: 0.7753, Val Acc: 0.7700, Loss: 0.5233\n",
      "Epoch: 749, Train Acc: 0.7653, Test Acc: 0.7720, Val Acc: 0.7746, Loss: 0.5190\n",
      "Epoch: 750, Train Acc: 0.7680, Test Acc: 0.7768, Val Acc: 0.7685, Loss: 0.5210\n",
      "Epoch: 751, Train Acc: 0.7670, Test Acc: 0.7762, Val Acc: 0.7640, Loss: 0.5227\n",
      "Epoch: 752, Train Acc: 0.7647, Test Acc: 0.7750, Val Acc: 0.7595, Loss: 0.5241\n",
      "Epoch: 753, Train Acc: 0.7640, Test Acc: 0.7753, Val Acc: 0.7610, Loss: 0.5225\n",
      "Epoch: 754, Train Acc: 0.7661, Test Acc: 0.7802, Val Acc: 0.7685, Loss: 0.5182\n",
      "Epoch: 755, Train Acc: 0.7646, Test Acc: 0.7768, Val Acc: 0.7595, Loss: 0.5185\n",
      "Epoch: 756, Train Acc: 0.7643, Test Acc: 0.7735, Val Acc: 0.7716, Loss: 0.5200\n",
      "Epoch: 757, Train Acc: 0.7660, Test Acc: 0.7759, Val Acc: 0.7716, Loss: 0.5192\n",
      "Epoch: 758, Train Acc: 0.7678, Test Acc: 0.7799, Val Acc: 0.7640, Loss: 0.5209\n",
      "Epoch: 759, Train Acc: 0.7690, Test Acc: 0.7777, Val Acc: 0.7716, Loss: 0.5150\n",
      "Epoch: 760, Train Acc: 0.7665, Test Acc: 0.7774, Val Acc: 0.7579, Loss: 0.5149\n",
      "Epoch: 761, Train Acc: 0.7681, Test Acc: 0.7768, Val Acc: 0.7625, Loss: 0.5202\n",
      "Epoch: 762, Train Acc: 0.7704, Test Acc: 0.7796, Val Acc: 0.7685, Loss: 0.5206\n",
      "Epoch: 763, Train Acc: 0.7691, Test Acc: 0.7796, Val Acc: 0.7670, Loss: 0.5174\n",
      "Epoch: 764, Train Acc: 0.7660, Test Acc: 0.7750, Val Acc: 0.7625, Loss: 0.5206\n",
      "Epoch: 765, Train Acc: 0.7643, Test Acc: 0.7750, Val Acc: 0.7579, Loss: 0.5192\n",
      "Epoch: 766, Train Acc: 0.7708, Test Acc: 0.7768, Val Acc: 0.7655, Loss: 0.5231\n",
      "Epoch: 767, Train Acc: 0.7695, Test Acc: 0.7796, Val Acc: 0.7670, Loss: 0.5148\n",
      "Epoch: 768, Train Acc: 0.7659, Test Acc: 0.7741, Val Acc: 0.7670, Loss: 0.5189\n",
      "Epoch: 769, Train Acc: 0.7636, Test Acc: 0.7762, Val Acc: 0.7716, Loss: 0.5143\n",
      "Epoch: 770, Train Acc: 0.7680, Test Acc: 0.7811, Val Acc: 0.7746, Loss: 0.5192\n",
      "Epoch: 771, Train Acc: 0.7683, Test Acc: 0.7805, Val Acc: 0.7731, Loss: 0.5263\n",
      "Epoch: 772, Train Acc: 0.7674, Test Acc: 0.7771, Val Acc: 0.7685, Loss: 0.5206\n",
      "Epoch: 773, Train Acc: 0.7674, Test Acc: 0.7771, Val Acc: 0.7731, Loss: 0.5227\n",
      "Epoch: 774, Train Acc: 0.7668, Test Acc: 0.7753, Val Acc: 0.7700, Loss: 0.5215\n",
      "Epoch: 775, Train Acc: 0.7684, Test Acc: 0.7759, Val Acc: 0.7837, Loss: 0.5284\n",
      "Epoch: 776, Train Acc: 0.7718, Test Acc: 0.7802, Val Acc: 0.7791, Loss: 0.5209\n",
      "Epoch: 777, Train Acc: 0.7688, Test Acc: 0.7799, Val Acc: 0.7716, Loss: 0.5165\n",
      "Epoch: 778, Train Acc: 0.7682, Test Acc: 0.7817, Val Acc: 0.7731, Loss: 0.5176\n",
      "Epoch: 779, Train Acc: 0.7704, Test Acc: 0.7865, Val Acc: 0.7731, Loss: 0.5247\n",
      "Epoch: 780, Train Acc: 0.7726, Test Acc: 0.7868, Val Acc: 0.7685, Loss: 0.5158\n",
      "Epoch: 781, Train Acc: 0.7678, Test Acc: 0.7823, Val Acc: 0.7655, Loss: 0.5153\n",
      "Epoch: 782, Train Acc: 0.7674, Test Acc: 0.7817, Val Acc: 0.7655, Loss: 0.5197\n",
      "Epoch: 783, Train Acc: 0.7690, Test Acc: 0.7814, Val Acc: 0.7640, Loss: 0.5207\n",
      "Epoch: 784, Train Acc: 0.7691, Test Acc: 0.7787, Val Acc: 0.7655, Loss: 0.5205\n",
      "Epoch: 785, Train Acc: 0.7725, Test Acc: 0.7771, Val Acc: 0.7716, Loss: 0.5219\n",
      "Epoch: 786, Train Acc: 0.7673, Test Acc: 0.7741, Val Acc: 0.7610, Loss: 0.5200\n",
      "Epoch: 787, Train Acc: 0.7636, Test Acc: 0.7714, Val Acc: 0.7549, Loss: 0.5190\n",
      "Epoch: 788, Train Acc: 0.7686, Test Acc: 0.7741, Val Acc: 0.7655, Loss: 0.5180\n",
      "Epoch: 789, Train Acc: 0.7703, Test Acc: 0.7750, Val Acc: 0.7640, Loss: 0.5227\n",
      "Epoch: 790, Train Acc: 0.7663, Test Acc: 0.7750, Val Acc: 0.7700, Loss: 0.5197\n",
      "Epoch: 791, Train Acc: 0.7672, Test Acc: 0.7762, Val Acc: 0.7670, Loss: 0.5191\n",
      "Epoch: 792, Train Acc: 0.7736, Test Acc: 0.7850, Val Acc: 0.7746, Loss: 0.5190\n",
      "Epoch: 793, Train Acc: 0.7697, Test Acc: 0.7817, Val Acc: 0.7731, Loss: 0.5187\n",
      "Epoch: 794, Train Acc: 0.7682, Test Acc: 0.7783, Val Acc: 0.7731, Loss: 0.5250\n",
      "Epoch: 795, Train Acc: 0.7700, Test Acc: 0.7799, Val Acc: 0.7579, Loss: 0.5210\n",
      "Epoch: 796, Train Acc: 0.7655, Test Acc: 0.7762, Val Acc: 0.7519, Loss: 0.5189\n",
      "Epoch: 797, Train Acc: 0.7599, Test Acc: 0.7744, Val Acc: 0.7489, Loss: 0.5137\n",
      "Epoch: 798, Train Acc: 0.7663, Test Acc: 0.7741, Val Acc: 0.7610, Loss: 0.5183\n",
      "Epoch: 799, Train Acc: 0.7674, Test Acc: 0.7747, Val Acc: 0.7655, Loss: 0.5223\n",
      "Epoch: 800, Train Acc: 0.7607, Test Acc: 0.7714, Val Acc: 0.7534, Loss: 0.5167\n",
      "Epoch: 801, Train Acc: 0.7619, Test Acc: 0.7732, Val Acc: 0.7564, Loss: 0.5235\n",
      "Epoch: 802, Train Acc: 0.7656, Test Acc: 0.7780, Val Acc: 0.7625, Loss: 0.5162\n",
      "Epoch: 803, Train Acc: 0.7710, Test Acc: 0.7850, Val Acc: 0.7716, Loss: 0.5161\n",
      "Epoch: 804, Train Acc: 0.7715, Test Acc: 0.7835, Val Acc: 0.7806, Loss: 0.5216\n",
      "Epoch: 805, Train Acc: 0.7683, Test Acc: 0.7774, Val Acc: 0.7716, Loss: 0.5220\n",
      "Epoch: 806, Train Acc: 0.7695, Test Acc: 0.7802, Val Acc: 0.7685, Loss: 0.5211\n",
      "Epoch: 807, Train Acc: 0.7699, Test Acc: 0.7805, Val Acc: 0.7640, Loss: 0.5197\n",
      "Epoch: 808, Train Acc: 0.7674, Test Acc: 0.7762, Val Acc: 0.7595, Loss: 0.5207\n",
      "Epoch: 809, Train Acc: 0.7674, Test Acc: 0.7777, Val Acc: 0.7610, Loss: 0.5223\n",
      "Epoch: 810, Train Acc: 0.7699, Test Acc: 0.7811, Val Acc: 0.7595, Loss: 0.5156\n",
      "Epoch: 811, Train Acc: 0.7686, Test Acc: 0.7796, Val Acc: 0.7579, Loss: 0.5196\n",
      "Epoch: 812, Train Acc: 0.7698, Test Acc: 0.7787, Val Acc: 0.7685, Loss: 0.5218\n",
      "Epoch: 813, Train Acc: 0.7683, Test Acc: 0.7777, Val Acc: 0.7670, Loss: 0.5241\n",
      "Epoch: 814, Train Acc: 0.7696, Test Acc: 0.7814, Val Acc: 0.7761, Loss: 0.5195\n",
      "Epoch: 815, Train Acc: 0.7690, Test Acc: 0.7814, Val Acc: 0.7761, Loss: 0.5177\n",
      "Epoch: 816, Train Acc: 0.7712, Test Acc: 0.7826, Val Acc: 0.7806, Loss: 0.5193\n",
      "Epoch: 817, Train Acc: 0.7701, Test Acc: 0.7790, Val Acc: 0.7746, Loss: 0.5237\n",
      "Epoch: 818, Train Acc: 0.7648, Test Acc: 0.7747, Val Acc: 0.7534, Loss: 0.5174\n",
      "Epoch: 819, Train Acc: 0.7695, Test Acc: 0.7799, Val Acc: 0.7579, Loss: 0.5185\n",
      "Epoch: 820, Train Acc: 0.7707, Test Acc: 0.7817, Val Acc: 0.7640, Loss: 0.5215\n",
      "Epoch: 821, Train Acc: 0.7671, Test Acc: 0.7738, Val Acc: 0.7564, Loss: 0.5156\n",
      "Epoch: 822, Train Acc: 0.7649, Test Acc: 0.7738, Val Acc: 0.7534, Loss: 0.5216\n",
      "Epoch: 823, Train Acc: 0.7675, Test Acc: 0.7787, Val Acc: 0.7640, Loss: 0.5184\n",
      "Epoch: 824, Train Acc: 0.7686, Test Acc: 0.7811, Val Acc: 0.7716, Loss: 0.5251\n",
      "Epoch: 825, Train Acc: 0.7677, Test Acc: 0.7790, Val Acc: 0.7625, Loss: 0.5224\n",
      "Epoch: 826, Train Acc: 0.7677, Test Acc: 0.7790, Val Acc: 0.7549, Loss: 0.5153\n",
      "Epoch: 827, Train Acc: 0.7695, Test Acc: 0.7811, Val Acc: 0.7579, Loss: 0.5213\n",
      "Epoch: 828, Train Acc: 0.7687, Test Acc: 0.7802, Val Acc: 0.7670, Loss: 0.5234\n",
      "Epoch: 829, Train Acc: 0.7616, Test Acc: 0.7768, Val Acc: 0.7549, Loss: 0.5227\n",
      "Epoch: 830, Train Acc: 0.7635, Test Acc: 0.7771, Val Acc: 0.7579, Loss: 0.5220\n",
      "Epoch: 831, Train Acc: 0.7677, Test Acc: 0.7796, Val Acc: 0.7700, Loss: 0.5199\n",
      "Epoch: 832, Train Acc: 0.7629, Test Acc: 0.7753, Val Acc: 0.7579, Loss: 0.5165\n",
      "Epoch: 833, Train Acc: 0.7649, Test Acc: 0.7759, Val Acc: 0.7640, Loss: 0.5196\n",
      "Epoch: 834, Train Acc: 0.7673, Test Acc: 0.7765, Val Acc: 0.7595, Loss: 0.5199\n",
      "Epoch: 835, Train Acc: 0.7663, Test Acc: 0.7765, Val Acc: 0.7670, Loss: 0.5175\n",
      "Epoch: 836, Train Acc: 0.7657, Test Acc: 0.7762, Val Acc: 0.7716, Loss: 0.5168\n",
      "Epoch: 837, Train Acc: 0.7696, Test Acc: 0.7820, Val Acc: 0.7595, Loss: 0.5206\n",
      "Epoch: 838, Train Acc: 0.7712, Test Acc: 0.7826, Val Acc: 0.7625, Loss: 0.5197\n",
      "Epoch: 839, Train Acc: 0.7628, Test Acc: 0.7805, Val Acc: 0.7610, Loss: 0.5188\n",
      "Epoch: 840, Train Acc: 0.7583, Test Acc: 0.7723, Val Acc: 0.7564, Loss: 0.5177\n",
      "Epoch: 841, Train Acc: 0.7750, Test Acc: 0.7832, Val Acc: 0.7549, Loss: 0.5285\n",
      "Epoch: 842, Train Acc: 0.7751, Test Acc: 0.7835, Val Acc: 0.7700, Loss: 0.5220\n",
      "Epoch: 843, Train Acc: 0.7602, Test Acc: 0.7717, Val Acc: 0.7474, Loss: 0.5207\n",
      "Epoch: 844, Train Acc: 0.7626, Test Acc: 0.7729, Val Acc: 0.7579, Loss: 0.5211\n",
      "Epoch: 845, Train Acc: 0.7724, Test Acc: 0.7793, Val Acc: 0.7731, Loss: 0.5189\n",
      "Epoch: 846, Train Acc: 0.7686, Test Acc: 0.7741, Val Acc: 0.7655, Loss: 0.5145\n",
      "Epoch: 847, Train Acc: 0.7644, Test Acc: 0.7717, Val Acc: 0.7685, Loss: 0.5235\n",
      "Epoch: 848, Train Acc: 0.7661, Test Acc: 0.7753, Val Acc: 0.7685, Loss: 0.5271\n",
      "Epoch: 849, Train Acc: 0.7714, Test Acc: 0.7793, Val Acc: 0.7731, Loss: 0.5242\n",
      "Epoch: 850, Train Acc: 0.7713, Test Acc: 0.7823, Val Acc: 0.7761, Loss: 0.5159\n",
      "Epoch: 851, Train Acc: 0.7701, Test Acc: 0.7832, Val Acc: 0.7655, Loss: 0.5192\n",
      "Epoch: 852, Train Acc: 0.7640, Test Acc: 0.7750, Val Acc: 0.7625, Loss: 0.5201\n",
      "Epoch: 853, Train Acc: 0.7657, Test Acc: 0.7790, Val Acc: 0.7625, Loss: 0.5237\n",
      "Epoch: 854, Train Acc: 0.7707, Test Acc: 0.7820, Val Acc: 0.7640, Loss: 0.5183\n",
      "Epoch: 855, Train Acc: 0.7653, Test Acc: 0.7787, Val Acc: 0.7579, Loss: 0.5195\n",
      "Epoch: 856, Train Acc: 0.7613, Test Acc: 0.7711, Val Acc: 0.7610, Loss: 0.5171\n",
      "Epoch: 857, Train Acc: 0.7662, Test Acc: 0.7774, Val Acc: 0.7670, Loss: 0.5182\n",
      "Epoch: 858, Train Acc: 0.7689, Test Acc: 0.7741, Val Acc: 0.7700, Loss: 0.5207\n",
      "Epoch: 859, Train Acc: 0.7629, Test Acc: 0.7684, Val Acc: 0.7685, Loss: 0.5220\n",
      "Epoch: 860, Train Acc: 0.7660, Test Acc: 0.7726, Val Acc: 0.7716, Loss: 0.5271\n",
      "Epoch: 861, Train Acc: 0.7634, Test Acc: 0.7796, Val Acc: 0.7670, Loss: 0.5146\n",
      "Epoch: 862, Train Acc: 0.7660, Test Acc: 0.7790, Val Acc: 0.7821, Loss: 0.5189\n",
      "Epoch: 863, Train Acc: 0.7668, Test Acc: 0.7753, Val Acc: 0.7761, Loss: 0.5215\n",
      "Epoch: 864, Train Acc: 0.7664, Test Acc: 0.7744, Val Acc: 0.7655, Loss: 0.5211\n",
      "Epoch: 865, Train Acc: 0.7682, Test Acc: 0.7774, Val Acc: 0.7670, Loss: 0.5211\n",
      "Epoch: 866, Train Acc: 0.7664, Test Acc: 0.7783, Val Acc: 0.7670, Loss: 0.5138\n",
      "Epoch: 867, Train Acc: 0.7690, Test Acc: 0.7777, Val Acc: 0.7640, Loss: 0.5175\n",
      "Epoch: 868, Train Acc: 0.7681, Test Acc: 0.7759, Val Acc: 0.7716, Loss: 0.5220\n",
      "Epoch: 869, Train Acc: 0.7664, Test Acc: 0.7793, Val Acc: 0.7776, Loss: 0.5222\n",
      "Epoch: 870, Train Acc: 0.7624, Test Acc: 0.7780, Val Acc: 0.7670, Loss: 0.5221\n",
      "Epoch: 871, Train Acc: 0.7644, Test Acc: 0.7744, Val Acc: 0.7731, Loss: 0.5237\n",
      "Epoch: 872, Train Acc: 0.7653, Test Acc: 0.7753, Val Acc: 0.7685, Loss: 0.5212\n",
      "Epoch: 873, Train Acc: 0.7657, Test Acc: 0.7768, Val Acc: 0.7670, Loss: 0.5227\n",
      "Epoch: 874, Train Acc: 0.7623, Test Acc: 0.7753, Val Acc: 0.7655, Loss: 0.5255\n",
      "Epoch: 875, Train Acc: 0.7617, Test Acc: 0.7780, Val Acc: 0.7640, Loss: 0.5195\n",
      "Epoch: 876, Train Acc: 0.7643, Test Acc: 0.7802, Val Acc: 0.7655, Loss: 0.5248\n",
      "Epoch: 877, Train Acc: 0.7648, Test Acc: 0.7759, Val Acc: 0.7564, Loss: 0.5226\n",
      "Epoch: 878, Train Acc: 0.7633, Test Acc: 0.7738, Val Acc: 0.7625, Loss: 0.5165\n",
      "Epoch: 879, Train Acc: 0.7610, Test Acc: 0.7750, Val Acc: 0.7564, Loss: 0.5202\n",
      "Epoch: 880, Train Acc: 0.7642, Test Acc: 0.7771, Val Acc: 0.7564, Loss: 0.5188\n",
      "Epoch: 881, Train Acc: 0.7691, Test Acc: 0.7793, Val Acc: 0.7595, Loss: 0.5180\n",
      "Epoch: 882, Train Acc: 0.7659, Test Acc: 0.7759, Val Acc: 0.7640, Loss: 0.5164\n",
      "Epoch: 883, Train Acc: 0.7663, Test Acc: 0.7756, Val Acc: 0.7700, Loss: 0.5252\n",
      "Epoch: 884, Train Acc: 0.7672, Test Acc: 0.7768, Val Acc: 0.7716, Loss: 0.5252\n",
      "Epoch: 885, Train Acc: 0.7673, Test Acc: 0.7774, Val Acc: 0.7821, Loss: 0.5222\n",
      "Epoch: 886, Train Acc: 0.7634, Test Acc: 0.7741, Val Acc: 0.7716, Loss: 0.5201\n",
      "Epoch: 887, Train Acc: 0.7624, Test Acc: 0.7735, Val Acc: 0.7716, Loss: 0.5186\n",
      "Epoch: 888, Train Acc: 0.7663, Test Acc: 0.7805, Val Acc: 0.7716, Loss: 0.5172\n",
      "Epoch: 889, Train Acc: 0.7675, Test Acc: 0.7802, Val Acc: 0.7716, Loss: 0.5215\n",
      "Epoch: 890, Train Acc: 0.7651, Test Acc: 0.7726, Val Acc: 0.7670, Loss: 0.5164\n",
      "Epoch: 891, Train Acc: 0.7665, Test Acc: 0.7747, Val Acc: 0.7776, Loss: 0.5153\n",
      "Epoch: 892, Train Acc: 0.7690, Test Acc: 0.7814, Val Acc: 0.7806, Loss: 0.5158\n",
      "Epoch: 893, Train Acc: 0.7678, Test Acc: 0.7805, Val Acc: 0.7761, Loss: 0.5194\n",
      "Epoch: 894, Train Acc: 0.7680, Test Acc: 0.7771, Val Acc: 0.7716, Loss: 0.5194\n",
      "Epoch: 895, Train Acc: 0.7697, Test Acc: 0.7771, Val Acc: 0.7670, Loss: 0.5171\n",
      "Epoch: 896, Train Acc: 0.7655, Test Acc: 0.7750, Val Acc: 0.7610, Loss: 0.5208\n",
      "Epoch: 897, Train Acc: 0.7668, Test Acc: 0.7753, Val Acc: 0.7610, Loss: 0.5239\n",
      "Epoch: 898, Train Acc: 0.7638, Test Acc: 0.7726, Val Acc: 0.7564, Loss: 0.5122\n",
      "Epoch: 899, Train Acc: 0.7682, Test Acc: 0.7777, Val Acc: 0.7716, Loss: 0.5202\n",
      "Epoch: 900, Train Acc: 0.7678, Test Acc: 0.7790, Val Acc: 0.7670, Loss: 0.5207\n",
      "Epoch: 901, Train Acc: 0.7694, Test Acc: 0.7777, Val Acc: 0.7655, Loss: 0.5261\n",
      "Epoch: 902, Train Acc: 0.7665, Test Acc: 0.7759, Val Acc: 0.7595, Loss: 0.5216\n",
      "Epoch: 903, Train Acc: 0.7592, Test Acc: 0.7711, Val Acc: 0.7625, Loss: 0.5213\n",
      "Epoch: 904, Train Acc: 0.7623, Test Acc: 0.7702, Val Acc: 0.7670, Loss: 0.5177\n",
      "Epoch: 905, Train Acc: 0.7692, Test Acc: 0.7762, Val Acc: 0.7806, Loss: 0.5242\n",
      "Epoch: 906, Train Acc: 0.7663, Test Acc: 0.7756, Val Acc: 0.7806, Loss: 0.5195\n",
      "Epoch: 907, Train Acc: 0.7504, Test Acc: 0.7638, Val Acc: 0.7579, Loss: 0.5197\n",
      "Epoch: 908, Train Acc: 0.7618, Test Acc: 0.7765, Val Acc: 0.7716, Loss: 0.5205\n",
      "Epoch: 909, Train Acc: 0.7683, Test Acc: 0.7853, Val Acc: 0.7640, Loss: 0.5199\n",
      "Epoch: 910, Train Acc: 0.7638, Test Acc: 0.7771, Val Acc: 0.7640, Loss: 0.5232\n",
      "Epoch: 911, Train Acc: 0.7555, Test Acc: 0.7711, Val Acc: 0.7549, Loss: 0.5199\n",
      "Epoch: 912, Train Acc: 0.7624, Test Acc: 0.7735, Val Acc: 0.7579, Loss: 0.5224\n",
      "Epoch: 913, Train Acc: 0.7659, Test Acc: 0.7759, Val Acc: 0.7700, Loss: 0.5253\n",
      "Epoch: 914, Train Acc: 0.7663, Test Acc: 0.7750, Val Acc: 0.7716, Loss: 0.5189\n",
      "Epoch: 915, Train Acc: 0.7643, Test Acc: 0.7741, Val Acc: 0.7746, Loss: 0.5226\n",
      "Epoch: 916, Train Acc: 0.7698, Test Acc: 0.7814, Val Acc: 0.7716, Loss: 0.5203\n",
      "Epoch: 917, Train Acc: 0.7698, Test Acc: 0.7823, Val Acc: 0.7776, Loss: 0.5194\n",
      "Epoch: 918, Train Acc: 0.7644, Test Acc: 0.7771, Val Acc: 0.7610, Loss: 0.5190\n",
      "Epoch: 919, Train Acc: 0.7609, Test Acc: 0.7750, Val Acc: 0.7610, Loss: 0.5242\n",
      "Epoch: 920, Train Acc: 0.7638, Test Acc: 0.7756, Val Acc: 0.7670, Loss: 0.5203\n",
      "Epoch: 921, Train Acc: 0.7681, Test Acc: 0.7768, Val Acc: 0.7610, Loss: 0.5224\n",
      "Epoch: 922, Train Acc: 0.7665, Test Acc: 0.7753, Val Acc: 0.7595, Loss: 0.5200\n",
      "Epoch: 923, Train Acc: 0.7664, Test Acc: 0.7790, Val Acc: 0.7640, Loss: 0.5244\n",
      "Epoch: 924, Train Acc: 0.7649, Test Acc: 0.7805, Val Acc: 0.7700, Loss: 0.5192\n",
      "Epoch: 925, Train Acc: 0.7674, Test Acc: 0.7787, Val Acc: 0.7700, Loss: 0.5250\n",
      "Epoch: 926, Train Acc: 0.7690, Test Acc: 0.7774, Val Acc: 0.7655, Loss: 0.5244\n",
      "Epoch: 927, Train Acc: 0.7629, Test Acc: 0.7744, Val Acc: 0.7564, Loss: 0.5196\n",
      "Epoch: 928, Train Acc: 0.7615, Test Acc: 0.7747, Val Acc: 0.7489, Loss: 0.5230\n",
      "Epoch: 929, Train Acc: 0.7701, Test Acc: 0.7814, Val Acc: 0.7625, Loss: 0.5201\n",
      "Epoch: 930, Train Acc: 0.7685, Test Acc: 0.7793, Val Acc: 0.7700, Loss: 0.5162\n",
      "Epoch: 931, Train Acc: 0.7638, Test Acc: 0.7790, Val Acc: 0.7746, Loss: 0.5199\n",
      "Epoch: 932, Train Acc: 0.7609, Test Acc: 0.7765, Val Acc: 0.7640, Loss: 0.5180\n",
      "Epoch: 933, Train Acc: 0.7672, Test Acc: 0.7780, Val Acc: 0.7564, Loss: 0.5239\n",
      "Epoch: 934, Train Acc: 0.7655, Test Acc: 0.7756, Val Acc: 0.7595, Loss: 0.5190\n",
      "Epoch: 935, Train Acc: 0.7655, Test Acc: 0.7796, Val Acc: 0.7595, Loss: 0.5132\n",
      "Epoch: 936, Train Acc: 0.7660, Test Acc: 0.7783, Val Acc: 0.7655, Loss: 0.5136\n",
      "Epoch: 937, Train Acc: 0.7689, Test Acc: 0.7780, Val Acc: 0.7700, Loss: 0.5205\n",
      "Epoch: 938, Train Acc: 0.7676, Test Acc: 0.7787, Val Acc: 0.7700, Loss: 0.5127\n",
      "Epoch: 939, Train Acc: 0.7691, Test Acc: 0.7793, Val Acc: 0.7625, Loss: 0.5235\n",
      "Epoch: 940, Train Acc: 0.7638, Test Acc: 0.7787, Val Acc: 0.7625, Loss: 0.5223\n",
      "Epoch: 941, Train Acc: 0.7647, Test Acc: 0.7774, Val Acc: 0.7685, Loss: 0.5209\n",
      "Epoch: 942, Train Acc: 0.7694, Test Acc: 0.7796, Val Acc: 0.7731, Loss: 0.5232\n",
      "Epoch: 943, Train Acc: 0.7660, Test Acc: 0.7793, Val Acc: 0.7716, Loss: 0.5215\n",
      "Epoch: 944, Train Acc: 0.7656, Test Acc: 0.7780, Val Acc: 0.7700, Loss: 0.5290\n",
      "Epoch: 945, Train Acc: 0.7689, Test Acc: 0.7841, Val Acc: 0.7791, Loss: 0.5175\n",
      "Epoch: 946, Train Acc: 0.7662, Test Acc: 0.7768, Val Acc: 0.7670, Loss: 0.5209\n",
      "Epoch: 947, Train Acc: 0.7635, Test Acc: 0.7720, Val Acc: 0.7595, Loss: 0.5203\n",
      "Epoch: 948, Train Acc: 0.7660, Test Acc: 0.7765, Val Acc: 0.7640, Loss: 0.5182\n",
      "Epoch: 949, Train Acc: 0.7658, Test Acc: 0.7765, Val Acc: 0.7625, Loss: 0.5236\n",
      "Epoch: 950, Train Acc: 0.7657, Test Acc: 0.7774, Val Acc: 0.7685, Loss: 0.5200\n",
      "Epoch: 951, Train Acc: 0.7663, Test Acc: 0.7765, Val Acc: 0.7746, Loss: 0.5213\n",
      "Epoch: 952, Train Acc: 0.7658, Test Acc: 0.7759, Val Acc: 0.7700, Loss: 0.5216\n",
      "Epoch: 953, Train Acc: 0.7607, Test Acc: 0.7723, Val Acc: 0.7700, Loss: 0.5193\n",
      "Epoch: 954, Train Acc: 0.7622, Test Acc: 0.7765, Val Acc: 0.7640, Loss: 0.5260\n",
      "Epoch: 955, Train Acc: 0.7634, Test Acc: 0.7765, Val Acc: 0.7655, Loss: 0.5248\n",
      "Epoch: 956, Train Acc: 0.7574, Test Acc: 0.7702, Val Acc: 0.7519, Loss: 0.5191\n",
      "Epoch: 957, Train Acc: 0.7590, Test Acc: 0.7714, Val Acc: 0.7595, Loss: 0.5215\n",
      "Epoch: 958, Train Acc: 0.7675, Test Acc: 0.7732, Val Acc: 0.7655, Loss: 0.5209\n",
      "Epoch: 959, Train Acc: 0.7682, Test Acc: 0.7768, Val Acc: 0.7700, Loss: 0.5188\n",
      "Epoch: 960, Train Acc: 0.7651, Test Acc: 0.7759, Val Acc: 0.7655, Loss: 0.5215\n",
      "Epoch: 961, Train Acc: 0.7656, Test Acc: 0.7768, Val Acc: 0.7655, Loss: 0.5204\n",
      "Epoch: 962, Train Acc: 0.7678, Test Acc: 0.7787, Val Acc: 0.7625, Loss: 0.5158\n",
      "Epoch: 963, Train Acc: 0.7675, Test Acc: 0.7765, Val Acc: 0.7489, Loss: 0.5237\n",
      "Epoch: 964, Train Acc: 0.7659, Test Acc: 0.7762, Val Acc: 0.7489, Loss: 0.5188\n",
      "Epoch: 965, Train Acc: 0.7647, Test Acc: 0.7747, Val Acc: 0.7549, Loss: 0.5215\n",
      "Epoch: 966, Train Acc: 0.7700, Test Acc: 0.7768, Val Acc: 0.7579, Loss: 0.5160\n",
      "Epoch: 967, Train Acc: 0.7686, Test Acc: 0.7783, Val Acc: 0.7579, Loss: 0.5188\n",
      "Epoch: 968, Train Acc: 0.7658, Test Acc: 0.7747, Val Acc: 0.7640, Loss: 0.5236\n",
      "Epoch: 969, Train Acc: 0.7647, Test Acc: 0.7777, Val Acc: 0.7489, Loss: 0.5192\n",
      "Epoch: 970, Train Acc: 0.7673, Test Acc: 0.7774, Val Acc: 0.7579, Loss: 0.5268\n",
      "Epoch: 971, Train Acc: 0.7646, Test Acc: 0.7765, Val Acc: 0.7670, Loss: 0.5234\n",
      "Epoch: 972, Train Acc: 0.7616, Test Acc: 0.7759, Val Acc: 0.7640, Loss: 0.5193\n",
      "Epoch: 973, Train Acc: 0.7642, Test Acc: 0.7765, Val Acc: 0.7700, Loss: 0.5224\n",
      "Epoch: 974, Train Acc: 0.7685, Test Acc: 0.7817, Val Acc: 0.7731, Loss: 0.5223\n",
      "Epoch: 975, Train Acc: 0.7678, Test Acc: 0.7783, Val Acc: 0.7761, Loss: 0.5195\n",
      "Epoch: 976, Train Acc: 0.7616, Test Acc: 0.7747, Val Acc: 0.7731, Loss: 0.5173\n",
      "Epoch: 977, Train Acc: 0.7632, Test Acc: 0.7738, Val Acc: 0.7821, Loss: 0.5175\n",
      "Epoch: 978, Train Acc: 0.7687, Test Acc: 0.7765, Val Acc: 0.7776, Loss: 0.5179\n",
      "Epoch: 979, Train Acc: 0.7653, Test Acc: 0.7771, Val Acc: 0.7610, Loss: 0.5148\n",
      "Epoch: 980, Train Acc: 0.7615, Test Acc: 0.7711, Val Acc: 0.7579, Loss: 0.5216\n",
      "Epoch: 981, Train Acc: 0.7632, Test Acc: 0.7723, Val Acc: 0.7519, Loss: 0.5226\n",
      "Epoch: 982, Train Acc: 0.7661, Test Acc: 0.7732, Val Acc: 0.7549, Loss: 0.5209\n",
      "Epoch: 983, Train Acc: 0.7668, Test Acc: 0.7780, Val Acc: 0.7595, Loss: 0.5119\n",
      "Epoch: 984, Train Acc: 0.7705, Test Acc: 0.7823, Val Acc: 0.7595, Loss: 0.5228\n",
      "Epoch: 985, Train Acc: 0.7701, Test Acc: 0.7796, Val Acc: 0.7595, Loss: 0.5217\n",
      "Epoch: 986, Train Acc: 0.7674, Test Acc: 0.7744, Val Acc: 0.7610, Loss: 0.5180\n",
      "Epoch: 987, Train Acc: 0.7686, Test Acc: 0.7768, Val Acc: 0.7595, Loss: 0.5218\n",
      "Epoch: 988, Train Acc: 0.7688, Test Acc: 0.7765, Val Acc: 0.7625, Loss: 0.5184\n",
      "Epoch: 989, Train Acc: 0.7655, Test Acc: 0.7723, Val Acc: 0.7625, Loss: 0.5223\n",
      "Epoch: 990, Train Acc: 0.7649, Test Acc: 0.7732, Val Acc: 0.7655, Loss: 0.5217\n",
      "Epoch: 991, Train Acc: 0.7656, Test Acc: 0.7729, Val Acc: 0.7746, Loss: 0.5158\n",
      "Epoch: 992, Train Acc: 0.7715, Test Acc: 0.7771, Val Acc: 0.7716, Loss: 0.5203\n",
      "Epoch: 993, Train Acc: 0.7659, Test Acc: 0.7738, Val Acc: 0.7640, Loss: 0.5174\n",
      "Epoch: 994, Train Acc: 0.7606, Test Acc: 0.7711, Val Acc: 0.7595, Loss: 0.5226\n",
      "Epoch: 995, Train Acc: 0.7661, Test Acc: 0.7768, Val Acc: 0.7640, Loss: 0.5236\n",
      "Epoch: 996, Train Acc: 0.7714, Test Acc: 0.7793, Val Acc: 0.7731, Loss: 0.5200\n",
      "Epoch: 997, Train Acc: 0.7631, Test Acc: 0.7750, Val Acc: 0.7716, Loss: 0.5160\n",
      "Epoch: 998, Train Acc: 0.7609, Test Acc: 0.7720, Val Acc: 0.7746, Loss: 0.5158\n",
      "Epoch: 999, Train Acc: 0.7698, Test Acc: 0.7765, Val Acc: 0.7731, Loss: 0.5224\n",
      "Epoch: 1000, Train Acc: 0.7700, Test Acc: 0.7811, Val Acc: 0.7746, Loss: 0.5193\n",
      "Epoch: 1001, Train Acc: 0.7628, Test Acc: 0.7738, Val Acc: 0.7685, Loss: 0.5221\n",
      "Epoch: 1002, Train Acc: 0.7620, Test Acc: 0.7729, Val Acc: 0.7655, Loss: 0.5205\n",
      "Epoch: 1003, Train Acc: 0.7657, Test Acc: 0.7735, Val Acc: 0.7655, Loss: 0.5246\n",
      "Epoch: 1004, Train Acc: 0.7588, Test Acc: 0.7690, Val Acc: 0.7640, Loss: 0.5186\n",
      "Epoch: 1005, Train Acc: 0.7597, Test Acc: 0.7702, Val Acc: 0.7685, Loss: 0.5193\n",
      "Epoch: 1006, Train Acc: 0.7658, Test Acc: 0.7777, Val Acc: 0.7655, Loss: 0.5236\n",
      "Epoch: 1007, Train Acc: 0.7660, Test Acc: 0.7780, Val Acc: 0.7716, Loss: 0.5195\n",
      "Epoch: 1008, Train Acc: 0.7622, Test Acc: 0.7708, Val Acc: 0.7716, Loss: 0.5246\n",
      "Epoch: 1009, Train Acc: 0.7596, Test Acc: 0.7699, Val Acc: 0.7640, Loss: 0.5224\n",
      "Epoch: 1010, Train Acc: 0.7646, Test Acc: 0.7720, Val Acc: 0.7700, Loss: 0.5248\n",
      "Epoch: 1011, Train Acc: 0.7670, Test Acc: 0.7744, Val Acc: 0.7670, Loss: 0.5281\n",
      "Epoch: 1012, Train Acc: 0.7645, Test Acc: 0.7765, Val Acc: 0.7731, Loss: 0.5190\n",
      "Epoch: 1013, Train Acc: 0.7606, Test Acc: 0.7708, Val Acc: 0.7640, Loss: 0.5196\n",
      "Epoch: 1014, Train Acc: 0.7624, Test Acc: 0.7732, Val Acc: 0.7655, Loss: 0.5223\n",
      "Epoch: 1015, Train Acc: 0.7636, Test Acc: 0.7747, Val Acc: 0.7610, Loss: 0.5246\n",
      "Epoch: 1016, Train Acc: 0.7586, Test Acc: 0.7729, Val Acc: 0.7564, Loss: 0.5188\n",
      "Epoch: 1017, Train Acc: 0.7624, Test Acc: 0.7783, Val Acc: 0.7549, Loss: 0.5167\n",
      "Epoch: 1018, Train Acc: 0.7688, Test Acc: 0.7829, Val Acc: 0.7610, Loss: 0.5176\n",
      "Epoch: 1019, Train Acc: 0.7622, Test Acc: 0.7759, Val Acc: 0.7595, Loss: 0.5189\n",
      "Epoch: 1020, Train Acc: 0.7634, Test Acc: 0.7738, Val Acc: 0.7595, Loss: 0.5198\n",
      "Epoch: 1021, Train Acc: 0.7712, Test Acc: 0.7835, Val Acc: 0.7791, Loss: 0.5211\n",
      "Epoch: 1022, Train Acc: 0.7717, Test Acc: 0.7838, Val Acc: 0.7746, Loss: 0.5234\n",
      "Epoch: 1023, Train Acc: 0.7622, Test Acc: 0.7780, Val Acc: 0.7625, Loss: 0.5203\n",
      "Epoch: 1024, Train Acc: 0.7644, Test Acc: 0.7771, Val Acc: 0.7655, Loss: 0.5250\n",
      "Epoch: 1025, Train Acc: 0.7701, Test Acc: 0.7808, Val Acc: 0.7579, Loss: 0.5182\n",
      "Epoch: 1026, Train Acc: 0.7658, Test Acc: 0.7780, Val Acc: 0.7595, Loss: 0.5152\n",
      "Epoch: 1027, Train Acc: 0.7657, Test Acc: 0.7805, Val Acc: 0.7610, Loss: 0.5267\n",
      "Epoch: 1028, Train Acc: 0.7669, Test Acc: 0.7802, Val Acc: 0.7625, Loss: 0.5193\n",
      "Epoch: 1029, Train Acc: 0.7637, Test Acc: 0.7783, Val Acc: 0.7534, Loss: 0.5231\n",
      "Epoch: 1030, Train Acc: 0.7602, Test Acc: 0.7750, Val Acc: 0.7519, Loss: 0.5223\n",
      "Epoch: 1031, Train Acc: 0.7599, Test Acc: 0.7711, Val Acc: 0.7625, Loss: 0.5211\n",
      "Epoch: 1032, Train Acc: 0.7669, Test Acc: 0.7780, Val Acc: 0.7640, Loss: 0.5241\n",
      "Epoch: 1033, Train Acc: 0.7638, Test Acc: 0.7753, Val Acc: 0.7579, Loss: 0.5158\n",
      "Epoch: 1034, Train Acc: 0.7600, Test Acc: 0.7711, Val Acc: 0.7655, Loss: 0.5168\n",
      "Epoch: 1035, Train Acc: 0.7659, Test Acc: 0.7744, Val Acc: 0.7700, Loss: 0.5266\n",
      "Epoch: 1036, Train Acc: 0.7678, Test Acc: 0.7750, Val Acc: 0.7670, Loss: 0.5168\n",
      "Epoch: 1037, Train Acc: 0.7654, Test Acc: 0.7774, Val Acc: 0.7761, Loss: 0.5226\n",
      "Epoch: 1038, Train Acc: 0.7638, Test Acc: 0.7747, Val Acc: 0.7761, Loss: 0.5247\n",
      "Epoch: 1039, Train Acc: 0.7634, Test Acc: 0.7741, Val Acc: 0.7791, Loss: 0.5286\n",
      "Epoch: 1040, Train Acc: 0.7632, Test Acc: 0.7735, Val Acc: 0.7716, Loss: 0.5232\n",
      "Epoch: 1041, Train Acc: 0.7640, Test Acc: 0.7723, Val Acc: 0.7595, Loss: 0.5245\n",
      "Epoch: 1042, Train Acc: 0.7583, Test Acc: 0.7705, Val Acc: 0.7579, Loss: 0.5181\n",
      "Epoch: 1043, Train Acc: 0.7620, Test Acc: 0.7756, Val Acc: 0.7564, Loss: 0.5230\n",
      "Epoch: 1044, Train Acc: 0.7675, Test Acc: 0.7796, Val Acc: 0.7579, Loss: 0.5198\n",
      "Epoch: 1045, Train Acc: 0.7681, Test Acc: 0.7811, Val Acc: 0.7670, Loss: 0.5220\n",
      "Epoch: 1046, Train Acc: 0.7648, Test Acc: 0.7783, Val Acc: 0.7685, Loss: 0.5213\n",
      "Epoch: 1047, Train Acc: 0.7644, Test Acc: 0.7759, Val Acc: 0.7610, Loss: 0.5209\n",
      "Epoch: 1048, Train Acc: 0.7670, Test Acc: 0.7762, Val Acc: 0.7655, Loss: 0.5169\n",
      "Epoch: 1049, Train Acc: 0.7660, Test Acc: 0.7777, Val Acc: 0.7625, Loss: 0.5246\n",
      "Epoch: 1050, Train Acc: 0.7649, Test Acc: 0.7753, Val Acc: 0.7610, Loss: 0.5150\n",
      "Epoch: 1051, Train Acc: 0.7700, Test Acc: 0.7796, Val Acc: 0.7731, Loss: 0.5225\n",
      "Epoch: 1052, Train Acc: 0.7700, Test Acc: 0.7787, Val Acc: 0.7716, Loss: 0.5164\n",
      "Epoch: 1053, Train Acc: 0.7690, Test Acc: 0.7793, Val Acc: 0.7625, Loss: 0.5162\n",
      "Epoch: 1054, Train Acc: 0.7671, Test Acc: 0.7787, Val Acc: 0.7625, Loss: 0.5163\n",
      "Epoch: 1055, Train Acc: 0.7658, Test Acc: 0.7747, Val Acc: 0.7595, Loss: 0.5164\n",
      "Epoch: 1056, Train Acc: 0.7662, Test Acc: 0.7741, Val Acc: 0.7625, Loss: 0.5221\n",
      "Epoch: 1057, Train Acc: 0.7634, Test Acc: 0.7729, Val Acc: 0.7610, Loss: 0.5154\n",
      "Epoch: 1058, Train Acc: 0.7661, Test Acc: 0.7787, Val Acc: 0.7625, Loss: 0.5214\n",
      "Epoch: 1059, Train Acc: 0.7663, Test Acc: 0.7759, Val Acc: 0.7579, Loss: 0.5200\n",
      "Epoch: 1060, Train Acc: 0.7626, Test Acc: 0.7771, Val Acc: 0.7534, Loss: 0.5204\n",
      "Epoch: 1061, Train Acc: 0.7624, Test Acc: 0.7750, Val Acc: 0.7474, Loss: 0.5213\n",
      "Epoch: 1062, Train Acc: 0.7674, Test Acc: 0.7765, Val Acc: 0.7595, Loss: 0.5209\n",
      "Epoch: 1063, Train Acc: 0.7719, Test Acc: 0.7820, Val Acc: 0.7595, Loss: 0.5196\n",
      "Epoch: 1064, Train Acc: 0.7669, Test Acc: 0.7777, Val Acc: 0.7579, Loss: 0.5169\n",
      "Epoch: 1065, Train Acc: 0.7601, Test Acc: 0.7690, Val Acc: 0.7595, Loss: 0.5191\n",
      "Epoch: 1066, Train Acc: 0.7649, Test Acc: 0.7787, Val Acc: 0.7504, Loss: 0.5157\n",
      "Epoch: 1067, Train Acc: 0.7684, Test Acc: 0.7823, Val Acc: 0.7700, Loss: 0.5231\n",
      "Epoch: 1068, Train Acc: 0.7573, Test Acc: 0.7720, Val Acc: 0.7640, Loss: 0.5191\n",
      "Epoch: 1069, Train Acc: 0.7640, Test Acc: 0.7759, Val Acc: 0.7731, Loss: 0.5235\n",
      "Epoch: 1070, Train Acc: 0.7718, Test Acc: 0.7808, Val Acc: 0.7746, Loss: 0.5243\n",
      "Epoch: 1071, Train Acc: 0.7658, Test Acc: 0.7756, Val Acc: 0.7625, Loss: 0.5189\n",
      "Epoch: 1072, Train Acc: 0.7608, Test Acc: 0.7732, Val Acc: 0.7640, Loss: 0.5234\n",
      "Epoch: 1073, Train Acc: 0.7663, Test Acc: 0.7787, Val Acc: 0.7640, Loss: 0.5177\n",
      "Epoch: 1074, Train Acc: 0.7705, Test Acc: 0.7826, Val Acc: 0.7685, Loss: 0.5230\n",
      "Epoch: 1075, Train Acc: 0.7659, Test Acc: 0.7735, Val Acc: 0.7640, Loss: 0.5172\n",
      "Epoch: 1076, Train Acc: 0.7580, Test Acc: 0.7650, Val Acc: 0.7534, Loss: 0.5097\n",
      "Epoch: 1077, Train Acc: 0.7649, Test Acc: 0.7696, Val Acc: 0.7595, Loss: 0.5203\n",
      "Epoch: 1078, Train Acc: 0.7696, Test Acc: 0.7741, Val Acc: 0.7670, Loss: 0.5163\n",
      "Epoch: 1079, Train Acc: 0.7674, Test Acc: 0.7753, Val Acc: 0.7564, Loss: 0.5193\n",
      "Epoch: 1080, Train Acc: 0.7677, Test Acc: 0.7777, Val Acc: 0.7610, Loss: 0.5189\n",
      "Epoch: 1081, Train Acc: 0.7695, Test Acc: 0.7780, Val Acc: 0.7685, Loss: 0.5218\n",
      "Epoch: 1082, Train Acc: 0.7662, Test Acc: 0.7787, Val Acc: 0.7685, Loss: 0.5226\n",
      "Epoch: 1083, Train Acc: 0.7596, Test Acc: 0.7714, Val Acc: 0.7625, Loss: 0.5234\n",
      "Epoch: 1084, Train Acc: 0.7600, Test Acc: 0.7723, Val Acc: 0.7655, Loss: 0.5253\n",
      "Epoch: 1085, Train Acc: 0.7681, Test Acc: 0.7780, Val Acc: 0.7670, Loss: 0.5210\n",
      "Epoch: 1086, Train Acc: 0.7605, Test Acc: 0.7741, Val Acc: 0.7610, Loss: 0.5200\n",
      "Epoch: 1087, Train Acc: 0.7591, Test Acc: 0.7690, Val Acc: 0.7549, Loss: 0.5168\n",
      "Epoch: 1088, Train Acc: 0.7680, Test Acc: 0.7711, Val Acc: 0.7716, Loss: 0.5216\n",
      "Epoch: 1089, Train Acc: 0.7721, Test Acc: 0.7732, Val Acc: 0.7670, Loss: 0.5216\n",
      "Epoch: 1090, Train Acc: 0.7654, Test Acc: 0.7735, Val Acc: 0.7716, Loss: 0.5193\n",
      "Epoch: 1091, Train Acc: 0.7660, Test Acc: 0.7756, Val Acc: 0.7625, Loss: 0.5218\n",
      "Epoch: 1092, Train Acc: 0.7653, Test Acc: 0.7750, Val Acc: 0.7670, Loss: 0.5172\n",
      "Epoch: 1093, Train Acc: 0.7640, Test Acc: 0.7729, Val Acc: 0.7610, Loss: 0.5235\n",
      "Epoch: 1094, Train Acc: 0.7621, Test Acc: 0.7708, Val Acc: 0.7640, Loss: 0.5188\n",
      "Epoch: 1095, Train Acc: 0.7682, Test Acc: 0.7768, Val Acc: 0.7640, Loss: 0.5207\n",
      "Epoch: 1096, Train Acc: 0.7699, Test Acc: 0.7793, Val Acc: 0.7640, Loss: 0.5240\n",
      "Epoch: 1097, Train Acc: 0.7602, Test Acc: 0.7726, Val Acc: 0.7579, Loss: 0.5173\n",
      "Epoch: 1098, Train Acc: 0.7631, Test Acc: 0.7714, Val Acc: 0.7579, Loss: 0.5232\n",
      "Epoch: 1099, Train Acc: 0.7668, Test Acc: 0.7750, Val Acc: 0.7670, Loss: 0.5177\n",
      "Epoch: 1100, Train Acc: 0.7660, Test Acc: 0.7744, Val Acc: 0.7731, Loss: 0.5189\n",
      "Epoch: 1101, Train Acc: 0.7644, Test Acc: 0.7774, Val Acc: 0.7746, Loss: 0.5166\n",
      "Epoch: 1102, Train Acc: 0.7680, Test Acc: 0.7780, Val Acc: 0.7761, Loss: 0.5222\n",
      "Epoch: 1103, Train Acc: 0.7688, Test Acc: 0.7793, Val Acc: 0.7731, Loss: 0.5203\n",
      "Epoch: 1104, Train Acc: 0.7619, Test Acc: 0.7735, Val Acc: 0.7640, Loss: 0.5149\n",
      "Epoch: 1105, Train Acc: 0.7692, Test Acc: 0.7765, Val Acc: 0.7670, Loss: 0.5187\n",
      "Epoch: 1106, Train Acc: 0.7711, Test Acc: 0.7799, Val Acc: 0.7731, Loss: 0.5183\n",
      "Epoch: 1107, Train Acc: 0.7675, Test Acc: 0.7768, Val Acc: 0.7670, Loss: 0.5220\n",
      "Epoch: 1108, Train Acc: 0.7593, Test Acc: 0.7690, Val Acc: 0.7519, Loss: 0.5200\n",
      "Epoch: 1109, Train Acc: 0.7690, Test Acc: 0.7796, Val Acc: 0.7640, Loss: 0.5216\n",
      "Epoch: 1110, Train Acc: 0.7700, Test Acc: 0.7817, Val Acc: 0.7640, Loss: 0.5165\n",
      "Epoch: 1111, Train Acc: 0.7699, Test Acc: 0.7820, Val Acc: 0.7579, Loss: 0.5245\n",
      "Epoch: 1112, Train Acc: 0.7653, Test Acc: 0.7765, Val Acc: 0.7579, Loss: 0.5146\n",
      "Epoch: 1113, Train Acc: 0.7691, Test Acc: 0.7799, Val Acc: 0.7821, Loss: 0.5218\n",
      "Epoch: 1114, Train Acc: 0.7685, Test Acc: 0.7802, Val Acc: 0.7867, Loss: 0.5201\n",
      "Epoch: 1115, Train Acc: 0.7670, Test Acc: 0.7787, Val Acc: 0.7806, Loss: 0.5199\n",
      "Epoch: 1116, Train Acc: 0.7572, Test Acc: 0.7708, Val Acc: 0.7625, Loss: 0.5203\n",
      "Epoch: 1117, Train Acc: 0.7646, Test Acc: 0.7744, Val Acc: 0.7761, Loss: 0.5221\n",
      "Epoch: 1118, Train Acc: 0.7658, Test Acc: 0.7771, Val Acc: 0.7700, Loss: 0.5191\n",
      "Epoch: 1119, Train Acc: 0.7648, Test Acc: 0.7762, Val Acc: 0.7670, Loss: 0.5155\n",
      "Epoch: 1120, Train Acc: 0.7587, Test Acc: 0.7711, Val Acc: 0.7504, Loss: 0.5223\n",
      "Epoch: 1121, Train Acc: 0.7594, Test Acc: 0.7723, Val Acc: 0.7534, Loss: 0.5188\n",
      "Epoch: 1122, Train Acc: 0.7686, Test Acc: 0.7790, Val Acc: 0.7610, Loss: 0.5153\n",
      "Epoch: 1123, Train Acc: 0.7684, Test Acc: 0.7762, Val Acc: 0.7564, Loss: 0.5241\n",
      "Epoch: 1124, Train Acc: 0.7616, Test Acc: 0.7741, Val Acc: 0.7534, Loss: 0.5195\n",
      "Epoch: 1125, Train Acc: 0.7649, Test Acc: 0.7732, Val Acc: 0.7564, Loss: 0.5209\n",
      "Epoch: 1126, Train Acc: 0.7704, Test Acc: 0.7768, Val Acc: 0.7731, Loss: 0.5186\n",
      "Epoch: 1127, Train Acc: 0.7685, Test Acc: 0.7771, Val Acc: 0.7670, Loss: 0.5206\n",
      "Epoch: 1128, Train Acc: 0.7660, Test Acc: 0.7702, Val Acc: 0.7610, Loss: 0.5225\n",
      "Epoch: 1129, Train Acc: 0.7681, Test Acc: 0.7774, Val Acc: 0.7700, Loss: 0.5186\n",
      "Epoch: 1130, Train Acc: 0.7704, Test Acc: 0.7783, Val Acc: 0.7700, Loss: 0.5167\n",
      "Epoch: 1131, Train Acc: 0.7678, Test Acc: 0.7796, Val Acc: 0.7746, Loss: 0.5185\n",
      "Epoch: 1132, Train Acc: 0.7635, Test Acc: 0.7780, Val Acc: 0.7640, Loss: 0.5214\n",
      "Epoch: 1133, Train Acc: 0.7638, Test Acc: 0.7780, Val Acc: 0.7564, Loss: 0.5172\n",
      "Epoch: 1134, Train Acc: 0.7691, Test Acc: 0.7808, Val Acc: 0.7746, Loss: 0.5234\n",
      "Epoch: 1135, Train Acc: 0.7658, Test Acc: 0.7750, Val Acc: 0.7685, Loss: 0.5162\n",
      "Epoch: 1136, Train Acc: 0.7604, Test Acc: 0.7741, Val Acc: 0.7595, Loss: 0.5176\n",
      "Epoch: 1137, Train Acc: 0.7626, Test Acc: 0.7735, Val Acc: 0.7685, Loss: 0.5176\n",
      "Epoch: 1138, Train Acc: 0.7671, Test Acc: 0.7777, Val Acc: 0.7761, Loss: 0.5170\n",
      "Epoch: 1139, Train Acc: 0.7675, Test Acc: 0.7790, Val Acc: 0.7700, Loss: 0.5172\n",
      "Epoch: 1140, Train Acc: 0.7634, Test Acc: 0.7756, Val Acc: 0.7610, Loss: 0.5156\n",
      "Epoch: 1141, Train Acc: 0.7716, Test Acc: 0.7856, Val Acc: 0.7670, Loss: 0.5283\n",
      "Epoch: 1142, Train Acc: 0.7735, Test Acc: 0.7847, Val Acc: 0.7670, Loss: 0.5166\n",
      "Epoch: 1143, Train Acc: 0.7645, Test Acc: 0.7759, Val Acc: 0.7595, Loss: 0.5209\n",
      "Epoch: 1144, Train Acc: 0.7633, Test Acc: 0.7720, Val Acc: 0.7655, Loss: 0.5173\n",
      "Epoch: 1145, Train Acc: 0.7653, Test Acc: 0.7759, Val Acc: 0.7655, Loss: 0.5210\n",
      "Epoch: 1146, Train Acc: 0.7714, Test Acc: 0.7762, Val Acc: 0.7746, Loss: 0.5168\n",
      "Epoch: 1147, Train Acc: 0.7690, Test Acc: 0.7756, Val Acc: 0.7670, Loss: 0.5168\n",
      "Epoch: 1148, Train Acc: 0.7643, Test Acc: 0.7753, Val Acc: 0.7685, Loss: 0.5200\n",
      "Epoch: 1149, Train Acc: 0.7656, Test Acc: 0.7817, Val Acc: 0.7761, Loss: 0.5194\n",
      "Epoch: 1150, Train Acc: 0.7661, Test Acc: 0.7796, Val Acc: 0.7731, Loss: 0.5148\n",
      "Epoch: 1151, Train Acc: 0.7644, Test Acc: 0.7799, Val Acc: 0.7761, Loss: 0.5189\n",
      "Epoch: 1152, Train Acc: 0.7678, Test Acc: 0.7790, Val Acc: 0.7700, Loss: 0.5157\n",
      "Epoch: 1153, Train Acc: 0.7705, Test Acc: 0.7780, Val Acc: 0.7761, Loss: 0.5201\n",
      "Epoch: 1154, Train Acc: 0.7673, Test Acc: 0.7774, Val Acc: 0.7776, Loss: 0.5175\n",
      "Epoch: 1155, Train Acc: 0.7626, Test Acc: 0.7765, Val Acc: 0.7716, Loss: 0.5193\n",
      "Epoch: 1156, Train Acc: 0.7655, Test Acc: 0.7765, Val Acc: 0.7700, Loss: 0.5180\n",
      "Epoch: 1157, Train Acc: 0.7676, Test Acc: 0.7774, Val Acc: 0.7700, Loss: 0.5183\n",
      "Epoch: 1158, Train Acc: 0.7667, Test Acc: 0.7759, Val Acc: 0.7670, Loss: 0.5174\n",
      "Epoch: 1159, Train Acc: 0.7615, Test Acc: 0.7699, Val Acc: 0.7564, Loss: 0.5203\n",
      "Epoch: 1160, Train Acc: 0.7646, Test Acc: 0.7741, Val Acc: 0.7595, Loss: 0.5203\n",
      "Epoch: 1161, Train Acc: 0.7620, Test Acc: 0.7744, Val Acc: 0.7564, Loss: 0.5188\n",
      "Epoch: 1162, Train Acc: 0.7638, Test Acc: 0.7808, Val Acc: 0.7489, Loss: 0.5206\n",
      "Epoch: 1163, Train Acc: 0.7659, Test Acc: 0.7817, Val Acc: 0.7670, Loss: 0.5182\n",
      "Epoch: 1164, Train Acc: 0.7669, Test Acc: 0.7823, Val Acc: 0.7670, Loss: 0.5203\n",
      "Epoch: 1165, Train Acc: 0.7672, Test Acc: 0.7823, Val Acc: 0.7640, Loss: 0.5172\n",
      "Epoch: 1166, Train Acc: 0.7628, Test Acc: 0.7771, Val Acc: 0.7625, Loss: 0.5206\n",
      "Epoch: 1167, Train Acc: 0.7667, Test Acc: 0.7817, Val Acc: 0.7716, Loss: 0.5267\n",
      "Epoch: 1168, Train Acc: 0.7653, Test Acc: 0.7783, Val Acc: 0.7610, Loss: 0.5212\n",
      "Epoch: 1169, Train Acc: 0.7634, Test Acc: 0.7747, Val Acc: 0.7610, Loss: 0.5269\n",
      "Epoch: 1170, Train Acc: 0.7587, Test Acc: 0.7768, Val Acc: 0.7534, Loss: 0.5159\n",
      "Epoch: 1171, Train Acc: 0.7620, Test Acc: 0.7756, Val Acc: 0.7610, Loss: 0.5238\n",
      "Epoch: 1172, Train Acc: 0.7665, Test Acc: 0.7783, Val Acc: 0.7761, Loss: 0.5212\n",
      "Epoch: 1173, Train Acc: 0.7659, Test Acc: 0.7753, Val Acc: 0.7731, Loss: 0.5228\n",
      "Epoch: 1174, Train Acc: 0.7607, Test Acc: 0.7732, Val Acc: 0.7640, Loss: 0.5139\n",
      "Epoch: 1175, Train Acc: 0.7671, Test Acc: 0.7796, Val Acc: 0.7625, Loss: 0.5211\n",
      "Epoch: 1176, Train Acc: 0.7675, Test Acc: 0.7765, Val Acc: 0.7625, Loss: 0.5220\n",
      "Epoch: 1177, Train Acc: 0.7641, Test Acc: 0.7790, Val Acc: 0.7579, Loss: 0.5180\n",
      "Epoch: 1178, Train Acc: 0.7670, Test Acc: 0.7744, Val Acc: 0.7640, Loss: 0.5203\n",
      "Epoch: 1179, Train Acc: 0.7694, Test Acc: 0.7762, Val Acc: 0.7716, Loss: 0.5144\n",
      "Epoch: 1180, Train Acc: 0.7672, Test Acc: 0.7762, Val Acc: 0.7776, Loss: 0.5191\n",
      "Epoch: 1181, Train Acc: 0.7665, Test Acc: 0.7768, Val Acc: 0.7776, Loss: 0.5221\n",
      "Epoch: 1182, Train Acc: 0.7659, Test Acc: 0.7777, Val Acc: 0.7761, Loss: 0.5241\n",
      "Epoch: 1183, Train Acc: 0.7661, Test Acc: 0.7738, Val Acc: 0.7791, Loss: 0.5191\n",
      "Epoch: 1184, Train Acc: 0.7678, Test Acc: 0.7768, Val Acc: 0.7700, Loss: 0.5226\n",
      "Epoch: 1185, Train Acc: 0.7677, Test Acc: 0.7780, Val Acc: 0.7685, Loss: 0.5216\n",
      "Epoch: 1186, Train Acc: 0.7659, Test Acc: 0.7783, Val Acc: 0.7791, Loss: 0.5126\n",
      "Epoch: 1187, Train Acc: 0.7634, Test Acc: 0.7787, Val Acc: 0.7806, Loss: 0.5194\n",
      "Epoch: 1188, Train Acc: 0.7671, Test Acc: 0.7799, Val Acc: 0.7837, Loss: 0.5185\n",
      "Epoch: 1189, Train Acc: 0.7654, Test Acc: 0.7787, Val Acc: 0.7746, Loss: 0.5229\n",
      "Epoch: 1190, Train Acc: 0.7637, Test Acc: 0.7783, Val Acc: 0.7685, Loss: 0.5217\n",
      "Epoch: 1191, Train Acc: 0.7636, Test Acc: 0.7765, Val Acc: 0.7534, Loss: 0.5192\n",
      "Epoch: 1192, Train Acc: 0.7635, Test Acc: 0.7732, Val Acc: 0.7504, Loss: 0.5204\n",
      "Epoch: 1193, Train Acc: 0.7676, Test Acc: 0.7744, Val Acc: 0.7519, Loss: 0.5206\n",
      "Epoch: 1194, Train Acc: 0.7662, Test Acc: 0.7750, Val Acc: 0.7474, Loss: 0.5209\n",
      "Epoch: 1195, Train Acc: 0.7648, Test Acc: 0.7747, Val Acc: 0.7474, Loss: 0.5179\n",
      "Epoch: 1196, Train Acc: 0.7670, Test Acc: 0.7759, Val Acc: 0.7489, Loss: 0.5177\n",
      "Epoch: 1197, Train Acc: 0.7688, Test Acc: 0.7774, Val Acc: 0.7489, Loss: 0.5265\n",
      "Epoch: 1198, Train Acc: 0.7682, Test Acc: 0.7765, Val Acc: 0.7504, Loss: 0.5199\n",
      "Epoch: 1199, Train Acc: 0.7676, Test Acc: 0.7780, Val Acc: 0.7549, Loss: 0.5204\n",
      "Epoch: 1200, Train Acc: 0.7677, Test Acc: 0.7787, Val Acc: 0.7655, Loss: 0.5201\n",
      "Epoch: 1201, Train Acc: 0.7713, Test Acc: 0.7823, Val Acc: 0.7700, Loss: 0.5211\n",
      "Epoch: 1202, Train Acc: 0.7681, Test Acc: 0.7802, Val Acc: 0.7716, Loss: 0.5160\n",
      "Epoch: 1203, Train Acc: 0.7646, Test Acc: 0.7799, Val Acc: 0.7640, Loss: 0.5186\n",
      "Epoch: 1204, Train Acc: 0.7681, Test Acc: 0.7799, Val Acc: 0.7655, Loss: 0.5182\n",
      "Epoch: 1205, Train Acc: 0.7656, Test Acc: 0.7805, Val Acc: 0.7670, Loss: 0.5163\n",
      "Epoch: 1206, Train Acc: 0.7629, Test Acc: 0.7796, Val Acc: 0.7731, Loss: 0.5194\n",
      "Epoch: 1207, Train Acc: 0.7634, Test Acc: 0.7799, Val Acc: 0.7716, Loss: 0.5199\n",
      "Epoch: 1208, Train Acc: 0.7623, Test Acc: 0.7783, Val Acc: 0.7685, Loss: 0.5173\n",
      "Epoch: 1209, Train Acc: 0.7607, Test Acc: 0.7762, Val Acc: 0.7655, Loss: 0.5250\n",
      "Epoch: 1210, Train Acc: 0.7580, Test Acc: 0.7741, Val Acc: 0.7489, Loss: 0.5197\n",
      "Epoch: 1211, Train Acc: 0.7605, Test Acc: 0.7738, Val Acc: 0.7579, Loss: 0.5232\n",
      "Epoch: 1212, Train Acc: 0.7669, Test Acc: 0.7777, Val Acc: 0.7625, Loss: 0.5227\n",
      "Epoch: 1213, Train Acc: 0.7636, Test Acc: 0.7774, Val Acc: 0.7549, Loss: 0.5162\n",
      "Epoch: 1214, Train Acc: 0.7591, Test Acc: 0.7729, Val Acc: 0.7519, Loss: 0.5197\n",
      "Epoch: 1215, Train Acc: 0.7648, Test Acc: 0.7750, Val Acc: 0.7579, Loss: 0.5249\n",
      "Epoch: 1216, Train Acc: 0.7627, Test Acc: 0.7723, Val Acc: 0.7579, Loss: 0.5242\n",
      "Epoch: 1217, Train Acc: 0.7629, Test Acc: 0.7747, Val Acc: 0.7640, Loss: 0.5213\n",
      "Epoch: 1218, Train Acc: 0.7643, Test Acc: 0.7768, Val Acc: 0.7716, Loss: 0.5146\n",
      "Epoch: 1219, Train Acc: 0.7637, Test Acc: 0.7753, Val Acc: 0.7761, Loss: 0.5196\n",
      "Epoch: 1220, Train Acc: 0.7622, Test Acc: 0.7747, Val Acc: 0.7670, Loss: 0.5131\n",
      "Epoch: 1221, Train Acc: 0.7647, Test Acc: 0.7774, Val Acc: 0.7670, Loss: 0.5216\n",
      "Epoch: 1222, Train Acc: 0.7673, Test Acc: 0.7793, Val Acc: 0.7655, Loss: 0.5221\n",
      "Epoch: 1223, Train Acc: 0.7663, Test Acc: 0.7790, Val Acc: 0.7640, Loss: 0.5169\n",
      "Epoch: 1224, Train Acc: 0.7658, Test Acc: 0.7771, Val Acc: 0.7625, Loss: 0.5176\n",
      "Epoch: 1225, Train Acc: 0.7608, Test Acc: 0.7720, Val Acc: 0.7655, Loss: 0.5207\n",
      "Epoch: 1226, Train Acc: 0.7550, Test Acc: 0.7714, Val Acc: 0.7549, Loss: 0.5174\n",
      "Epoch: 1227, Train Acc: 0.7589, Test Acc: 0.7708, Val Acc: 0.7579, Loss: 0.5175\n",
      "Epoch: 1228, Train Acc: 0.7647, Test Acc: 0.7732, Val Acc: 0.7655, Loss: 0.5151\n",
      "Epoch: 1229, Train Acc: 0.7607, Test Acc: 0.7732, Val Acc: 0.7685, Loss: 0.5171\n",
      "Epoch: 1230, Train Acc: 0.7634, Test Acc: 0.7762, Val Acc: 0.7761, Loss: 0.5203\n",
      "Epoch: 1231, Train Acc: 0.7725, Test Acc: 0.7774, Val Acc: 0.7700, Loss: 0.5208\n",
      "Epoch: 1232, Train Acc: 0.7609, Test Acc: 0.7711, Val Acc: 0.7564, Loss: 0.5242\n",
      "Epoch: 1233, Train Acc: 0.7613, Test Acc: 0.7708, Val Acc: 0.7579, Loss: 0.5256\n",
      "Epoch: 1234, Train Acc: 0.7646, Test Acc: 0.7744, Val Acc: 0.7625, Loss: 0.5289\n",
      "Epoch: 1235, Train Acc: 0.7642, Test Acc: 0.7756, Val Acc: 0.7610, Loss: 0.5191\n",
      "Epoch: 1236, Train Acc: 0.7562, Test Acc: 0.7681, Val Acc: 0.7519, Loss: 0.5219\n",
      "Epoch: 1237, Train Acc: 0.7610, Test Acc: 0.7699, Val Acc: 0.7564, Loss: 0.5204\n",
      "Epoch: 1238, Train Acc: 0.7667, Test Acc: 0.7747, Val Acc: 0.7700, Loss: 0.5234\n",
      "Epoch: 1239, Train Acc: 0.7653, Test Acc: 0.7732, Val Acc: 0.7655, Loss: 0.5235\n",
      "Epoch: 1240, Train Acc: 0.7573, Test Acc: 0.7702, Val Acc: 0.7670, Loss: 0.5195\n",
      "Epoch: 1241, Train Acc: 0.7646, Test Acc: 0.7732, Val Acc: 0.7746, Loss: 0.5216\n",
      "Epoch: 1242, Train Acc: 0.7680, Test Acc: 0.7771, Val Acc: 0.7731, Loss: 0.5268\n",
      "Epoch: 1243, Train Acc: 0.7618, Test Acc: 0.7732, Val Acc: 0.7655, Loss: 0.5226\n",
      "Epoch: 1244, Train Acc: 0.7563, Test Acc: 0.7678, Val Acc: 0.7670, Loss: 0.5153\n",
      "Epoch: 1245, Train Acc: 0.7647, Test Acc: 0.7711, Val Acc: 0.7670, Loss: 0.5239\n",
      "Epoch: 1246, Train Acc: 0.7651, Test Acc: 0.7744, Val Acc: 0.7655, Loss: 0.5220\n",
      "Epoch: 1247, Train Acc: 0.7600, Test Acc: 0.7684, Val Acc: 0.7655, Loss: 0.5249\n",
      "Epoch: 1248, Train Acc: 0.7627, Test Acc: 0.7735, Val Acc: 0.7700, Loss: 0.5224\n",
      "Epoch: 1249, Train Acc: 0.7674, Test Acc: 0.7783, Val Acc: 0.7700, Loss: 0.5241\n",
      "Epoch: 1250, Train Acc: 0.7668, Test Acc: 0.7793, Val Acc: 0.7610, Loss: 0.5232\n",
      "Epoch: 1251, Train Acc: 0.7632, Test Acc: 0.7741, Val Acc: 0.7610, Loss: 0.5194\n",
      "Epoch: 1252, Train Acc: 0.7642, Test Acc: 0.7726, Val Acc: 0.7625, Loss: 0.5143\n",
      "Epoch: 1253, Train Acc: 0.7670, Test Acc: 0.7771, Val Acc: 0.7700, Loss: 0.5157\n",
      "Epoch: 1254, Train Acc: 0.7665, Test Acc: 0.7759, Val Acc: 0.7640, Loss: 0.5189\n",
      "Epoch: 1255, Train Acc: 0.7653, Test Acc: 0.7765, Val Acc: 0.7670, Loss: 0.5144\n",
      "Epoch: 1256, Train Acc: 0.7668, Test Acc: 0.7793, Val Acc: 0.7640, Loss: 0.5240\n",
      "Epoch: 1257, Train Acc: 0.7672, Test Acc: 0.7787, Val Acc: 0.7685, Loss: 0.5243\n",
      "Epoch: 1258, Train Acc: 0.7615, Test Acc: 0.7741, Val Acc: 0.7595, Loss: 0.5213\n",
      "Epoch: 1259, Train Acc: 0.7628, Test Acc: 0.7738, Val Acc: 0.7625, Loss: 0.5228\n",
      "Epoch: 1260, Train Acc: 0.7675, Test Acc: 0.7790, Val Acc: 0.7670, Loss: 0.5208\n",
      "Epoch: 1261, Train Acc: 0.7640, Test Acc: 0.7741, Val Acc: 0.7670, Loss: 0.5174\n",
      "Epoch: 1262, Train Acc: 0.7633, Test Acc: 0.7747, Val Acc: 0.7595, Loss: 0.5235\n",
      "Epoch: 1263, Train Acc: 0.7695, Test Acc: 0.7802, Val Acc: 0.7655, Loss: 0.5223\n",
      "Epoch: 1264, Train Acc: 0.7697, Test Acc: 0.7793, Val Acc: 0.7595, Loss: 0.5172\n",
      "Epoch: 1265, Train Acc: 0.7673, Test Acc: 0.7774, Val Acc: 0.7579, Loss: 0.5183\n",
      "Epoch: 1266, Train Acc: 0.7683, Test Acc: 0.7793, Val Acc: 0.7716, Loss: 0.5170\n",
      "Epoch: 1267, Train Acc: 0.7689, Test Acc: 0.7835, Val Acc: 0.7761, Loss: 0.5176\n",
      "Epoch: 1268, Train Acc: 0.7675, Test Acc: 0.7802, Val Acc: 0.7746, Loss: 0.5227\n",
      "Epoch: 1269, Train Acc: 0.7635, Test Acc: 0.7787, Val Acc: 0.7595, Loss: 0.5236\n",
      "Epoch: 1270, Train Acc: 0.7635, Test Acc: 0.7765, Val Acc: 0.7534, Loss: 0.5123\n",
      "Epoch: 1271, Train Acc: 0.7650, Test Acc: 0.7820, Val Acc: 0.7625, Loss: 0.5152\n",
      "Epoch: 1272, Train Acc: 0.7651, Test Acc: 0.7811, Val Acc: 0.7579, Loss: 0.5222\n",
      "Epoch: 1273, Train Acc: 0.7641, Test Acc: 0.7799, Val Acc: 0.7685, Loss: 0.5193\n",
      "Epoch: 1274, Train Acc: 0.7633, Test Acc: 0.7793, Val Acc: 0.7700, Loss: 0.5268\n",
      "Epoch: 1275, Train Acc: 0.7676, Test Acc: 0.7808, Val Acc: 0.7625, Loss: 0.5242\n",
      "Epoch: 1276, Train Acc: 0.7626, Test Acc: 0.7790, Val Acc: 0.7549, Loss: 0.5205\n",
      "Epoch: 1277, Train Acc: 0.7557, Test Acc: 0.7708, Val Acc: 0.7670, Loss: 0.5217\n",
      "Epoch: 1278, Train Acc: 0.7638, Test Acc: 0.7750, Val Acc: 0.7700, Loss: 0.5225\n",
      "Epoch: 1279, Train Acc: 0.7664, Test Acc: 0.7762, Val Acc: 0.7746, Loss: 0.5169\n",
      "Epoch: 1280, Train Acc: 0.7599, Test Acc: 0.7699, Val Acc: 0.7670, Loss: 0.5219\n",
      "Epoch: 1281, Train Acc: 0.7644, Test Acc: 0.7735, Val Acc: 0.7655, Loss: 0.5258\n",
      "Epoch: 1282, Train Acc: 0.7641, Test Acc: 0.7762, Val Acc: 0.7700, Loss: 0.5208\n",
      "Epoch: 1283, Train Acc: 0.7616, Test Acc: 0.7747, Val Acc: 0.7670, Loss: 0.5183\n",
      "Epoch: 1284, Train Acc: 0.7640, Test Acc: 0.7774, Val Acc: 0.7534, Loss: 0.5199\n",
      "Epoch: 1285, Train Acc: 0.7638, Test Acc: 0.7729, Val Acc: 0.7519, Loss: 0.5202\n",
      "Epoch: 1286, Train Acc: 0.7651, Test Acc: 0.7744, Val Acc: 0.7549, Loss: 0.5234\n",
      "Epoch: 1287, Train Acc: 0.7657, Test Acc: 0.7783, Val Acc: 0.7640, Loss: 0.5237\n",
      "Epoch: 1288, Train Acc: 0.7626, Test Acc: 0.7765, Val Acc: 0.7655, Loss: 0.5185\n",
      "Epoch: 1289, Train Acc: 0.7635, Test Acc: 0.7735, Val Acc: 0.7700, Loss: 0.5156\n",
      "Epoch: 1290, Train Acc: 0.7680, Test Acc: 0.7780, Val Acc: 0.7655, Loss: 0.5166\n",
      "Epoch: 1291, Train Acc: 0.7673, Test Acc: 0.7774, Val Acc: 0.7655, Loss: 0.5229\n",
      "Epoch: 1292, Train Acc: 0.7620, Test Acc: 0.7753, Val Acc: 0.7731, Loss: 0.5186\n",
      "Epoch: 1293, Train Acc: 0.7635, Test Acc: 0.7711, Val Acc: 0.7700, Loss: 0.5195\n",
      "Epoch: 1294, Train Acc: 0.7682, Test Acc: 0.7790, Val Acc: 0.7746, Loss: 0.5201\n",
      "Epoch: 1295, Train Acc: 0.7689, Test Acc: 0.7796, Val Acc: 0.7625, Loss: 0.5239\n",
      "Epoch: 1296, Train Acc: 0.7619, Test Acc: 0.7705, Val Acc: 0.7549, Loss: 0.5172\n",
      "Epoch: 1297, Train Acc: 0.7673, Test Acc: 0.7768, Val Acc: 0.7655, Loss: 0.5193\n",
      "Epoch: 1298, Train Acc: 0.7713, Test Acc: 0.7835, Val Acc: 0.7700, Loss: 0.5161\n",
      "Epoch: 1299, Train Acc: 0.7606, Test Acc: 0.7735, Val Acc: 0.7549, Loss: 0.5233\n",
      "Epoch: 1300, Train Acc: 0.7590, Test Acc: 0.7720, Val Acc: 0.7489, Loss: 0.5201\n",
      "Epoch: 1301, Train Acc: 0.7681, Test Acc: 0.7814, Val Acc: 0.7640, Loss: 0.5211\n",
      "Epoch: 1302, Train Acc: 0.7647, Test Acc: 0.7762, Val Acc: 0.7610, Loss: 0.5231\n",
      "Epoch: 1303, Train Acc: 0.7597, Test Acc: 0.7726, Val Acc: 0.7519, Loss: 0.5204\n",
      "Epoch: 1304, Train Acc: 0.7602, Test Acc: 0.7732, Val Acc: 0.7534, Loss: 0.5191\n",
      "Epoch: 1305, Train Acc: 0.7671, Test Acc: 0.7793, Val Acc: 0.7731, Loss: 0.5145\n",
      "Epoch: 1306, Train Acc: 0.7659, Test Acc: 0.7783, Val Acc: 0.7700, Loss: 0.5227\n",
      "Epoch: 1307, Train Acc: 0.7643, Test Acc: 0.7762, Val Acc: 0.7746, Loss: 0.5219\n",
      "Epoch: 1308, Train Acc: 0.7616, Test Acc: 0.7711, Val Acc: 0.7716, Loss: 0.5184\n",
      "Epoch: 1309, Train Acc: 0.7669, Test Acc: 0.7787, Val Acc: 0.7685, Loss: 0.5222\n",
      "Epoch: 1310, Train Acc: 0.7729, Test Acc: 0.7808, Val Acc: 0.7655, Loss: 0.5181\n",
      "Epoch: 1311, Train Acc: 0.7665, Test Acc: 0.7756, Val Acc: 0.7579, Loss: 0.5208\n",
      "Epoch: 1312, Train Acc: 0.7663, Test Acc: 0.7747, Val Acc: 0.7655, Loss: 0.5142\n",
      "Epoch: 1313, Train Acc: 0.7669, Test Acc: 0.7774, Val Acc: 0.7640, Loss: 0.5252\n",
      "Epoch: 1314, Train Acc: 0.7668, Test Acc: 0.7796, Val Acc: 0.7564, Loss: 0.5210\n",
      "Epoch: 1315, Train Acc: 0.7659, Test Acc: 0.7787, Val Acc: 0.7579, Loss: 0.5270\n",
      "Epoch: 1316, Train Acc: 0.7686, Test Acc: 0.7832, Val Acc: 0.7670, Loss: 0.5183\n",
      "Epoch: 1317, Train Acc: 0.7695, Test Acc: 0.7832, Val Acc: 0.7716, Loss: 0.5183\n",
      "Epoch: 1318, Train Acc: 0.7641, Test Acc: 0.7780, Val Acc: 0.7670, Loss: 0.5250\n",
      "Epoch: 1319, Train Acc: 0.7601, Test Acc: 0.7732, Val Acc: 0.7670, Loss: 0.5240\n",
      "Epoch: 1320, Train Acc: 0.7594, Test Acc: 0.7723, Val Acc: 0.7595, Loss: 0.5158\n",
      "Epoch: 1321, Train Acc: 0.7671, Test Acc: 0.7814, Val Acc: 0.7610, Loss: 0.5222\n",
      "Epoch: 1322, Train Acc: 0.7632, Test Acc: 0.7747, Val Acc: 0.7670, Loss: 0.5214\n",
      "Epoch: 1323, Train Acc: 0.7618, Test Acc: 0.7726, Val Acc: 0.7655, Loss: 0.5239\n",
      "Epoch: 1324, Train Acc: 0.7689, Test Acc: 0.7790, Val Acc: 0.7685, Loss: 0.5190\n",
      "Epoch: 1325, Train Acc: 0.7618, Test Acc: 0.7738, Val Acc: 0.7579, Loss: 0.5215\n",
      "Epoch: 1326, Train Acc: 0.7542, Test Acc: 0.7699, Val Acc: 0.7595, Loss: 0.5225\n",
      "Epoch: 1327, Train Acc: 0.7635, Test Acc: 0.7780, Val Acc: 0.7670, Loss: 0.5193\n",
      "Epoch: 1328, Train Acc: 0.7661, Test Acc: 0.7802, Val Acc: 0.7655, Loss: 0.5154\n",
      "Epoch: 1329, Train Acc: 0.7587, Test Acc: 0.7732, Val Acc: 0.7640, Loss: 0.5209\n",
      "Epoch: 1330, Train Acc: 0.7604, Test Acc: 0.7729, Val Acc: 0.7625, Loss: 0.5188\n",
      "Epoch: 1331, Train Acc: 0.7672, Test Acc: 0.7802, Val Acc: 0.7670, Loss: 0.5218\n",
      "Epoch: 1332, Train Acc: 0.7660, Test Acc: 0.7805, Val Acc: 0.7700, Loss: 0.5237\n",
      "Epoch: 1333, Train Acc: 0.7642, Test Acc: 0.7774, Val Acc: 0.7640, Loss: 0.5198\n",
      "Epoch: 1334, Train Acc: 0.7667, Test Acc: 0.7780, Val Acc: 0.7579, Loss: 0.5216\n",
      "Epoch: 1335, Train Acc: 0.7635, Test Acc: 0.7747, Val Acc: 0.7534, Loss: 0.5196\n",
      "Epoch: 1336, Train Acc: 0.7643, Test Acc: 0.7753, Val Acc: 0.7579, Loss: 0.5220\n",
      "Epoch: 1337, Train Acc: 0.7667, Test Acc: 0.7796, Val Acc: 0.7579, Loss: 0.5237\n",
      "Epoch: 1338, Train Acc: 0.7665, Test Acc: 0.7777, Val Acc: 0.7670, Loss: 0.5295\n",
      "Epoch: 1339, Train Acc: 0.7660, Test Acc: 0.7783, Val Acc: 0.7655, Loss: 0.5245\n",
      "Epoch: 1340, Train Acc: 0.7613, Test Acc: 0.7735, Val Acc: 0.7625, Loss: 0.5179\n",
      "Epoch: 1341, Train Acc: 0.7695, Test Acc: 0.7823, Val Acc: 0.7625, Loss: 0.5236\n",
      "Epoch: 1342, Train Acc: 0.7659, Test Acc: 0.7787, Val Acc: 0.7610, Loss: 0.5179\n",
      "Epoch: 1343, Train Acc: 0.7588, Test Acc: 0.7696, Val Acc: 0.7534, Loss: 0.5207\n",
      "Epoch: 1344, Train Acc: 0.7646, Test Acc: 0.7765, Val Acc: 0.7519, Loss: 0.5214\n",
      "Epoch: 1345, Train Acc: 0.7646, Test Acc: 0.7750, Val Acc: 0.7625, Loss: 0.5197\n",
      "Epoch: 1346, Train Acc: 0.7596, Test Acc: 0.7693, Val Acc: 0.7625, Loss: 0.5233\n",
      "Epoch: 1347, Train Acc: 0.7630, Test Acc: 0.7759, Val Acc: 0.7716, Loss: 0.5204\n",
      "Epoch: 1348, Train Acc: 0.7669, Test Acc: 0.7774, Val Acc: 0.7670, Loss: 0.5162\n",
      "Epoch: 1349, Train Acc: 0.7663, Test Acc: 0.7777, Val Acc: 0.7640, Loss: 0.5179\n",
      "Epoch: 1350, Train Acc: 0.7660, Test Acc: 0.7790, Val Acc: 0.7625, Loss: 0.5166\n",
      "Epoch: 1351, Train Acc: 0.7674, Test Acc: 0.7826, Val Acc: 0.7655, Loss: 0.5176\n",
      "Epoch: 1352, Train Acc: 0.7619, Test Acc: 0.7771, Val Acc: 0.7579, Loss: 0.5214\n",
      "Epoch: 1353, Train Acc: 0.7670, Test Acc: 0.7780, Val Acc: 0.7595, Loss: 0.5224\n",
      "Epoch: 1354, Train Acc: 0.7638, Test Acc: 0.7744, Val Acc: 0.7564, Loss: 0.5210\n",
      "Epoch: 1355, Train Acc: 0.7587, Test Acc: 0.7690, Val Acc: 0.7625, Loss: 0.5216\n",
      "Epoch: 1356, Train Acc: 0.7614, Test Acc: 0.7744, Val Acc: 0.7655, Loss: 0.5129\n",
      "Epoch: 1357, Train Acc: 0.7643, Test Acc: 0.7747, Val Acc: 0.7625, Loss: 0.5240\n",
      "Epoch: 1358, Train Acc: 0.7620, Test Acc: 0.7738, Val Acc: 0.7640, Loss: 0.5209\n",
      "Epoch: 1359, Train Acc: 0.7578, Test Acc: 0.7747, Val Acc: 0.7670, Loss: 0.5206\n",
      "Epoch: 1360, Train Acc: 0.7698, Test Acc: 0.7856, Val Acc: 0.7837, Loss: 0.5224\n",
      "Epoch: 1361, Train Acc: 0.7678, Test Acc: 0.7820, Val Acc: 0.7806, Loss: 0.5197\n",
      "Epoch: 1362, Train Acc: 0.7597, Test Acc: 0.7741, Val Acc: 0.7685, Loss: 0.5231\n",
      "Epoch: 1363, Train Acc: 0.7596, Test Acc: 0.7777, Val Acc: 0.7564, Loss: 0.5179\n",
      "Epoch: 1364, Train Acc: 0.7650, Test Acc: 0.7811, Val Acc: 0.7579, Loss: 0.5238\n",
      "Epoch: 1365, Train Acc: 0.7575, Test Acc: 0.7726, Val Acc: 0.7504, Loss: 0.5202\n",
      "Epoch: 1366, Train Acc: 0.7579, Test Acc: 0.7750, Val Acc: 0.7625, Loss: 0.5236\n",
      "Epoch: 1367, Train Acc: 0.7654, Test Acc: 0.7783, Val Acc: 0.7610, Loss: 0.5178\n",
      "Epoch: 1368, Train Acc: 0.7677, Test Acc: 0.7814, Val Acc: 0.7549, Loss: 0.5185\n",
      "Epoch: 1369, Train Acc: 0.7616, Test Acc: 0.7735, Val Acc: 0.7549, Loss: 0.5188\n",
      "Epoch: 1370, Train Acc: 0.7664, Test Acc: 0.7823, Val Acc: 0.7610, Loss: 0.5185\n",
      "Epoch: 1371, Train Acc: 0.7672, Test Acc: 0.7780, Val Acc: 0.7595, Loss: 0.5200\n",
      "Epoch: 1372, Train Acc: 0.7613, Test Acc: 0.7726, Val Acc: 0.7655, Loss: 0.5197\n",
      "Epoch: 1373, Train Acc: 0.7656, Test Acc: 0.7777, Val Acc: 0.7685, Loss: 0.5247\n",
      "Epoch: 1374, Train Acc: 0.7681, Test Acc: 0.7790, Val Acc: 0.7700, Loss: 0.5143\n",
      "Epoch: 1375, Train Acc: 0.7607, Test Acc: 0.7759, Val Acc: 0.7564, Loss: 0.5268\n",
      "Epoch: 1376, Train Acc: 0.7603, Test Acc: 0.7753, Val Acc: 0.7519, Loss: 0.5255\n",
      "Epoch: 1377, Train Acc: 0.7635, Test Acc: 0.7805, Val Acc: 0.7579, Loss: 0.5242\n",
      "Epoch: 1378, Train Acc: 0.7604, Test Acc: 0.7750, Val Acc: 0.7549, Loss: 0.5172\n",
      "Epoch: 1379, Train Acc: 0.7582, Test Acc: 0.7714, Val Acc: 0.7579, Loss: 0.5226\n",
      "Epoch: 1380, Train Acc: 0.7569, Test Acc: 0.7705, Val Acc: 0.7595, Loss: 0.5152\n",
      "Epoch: 1381, Train Acc: 0.7604, Test Acc: 0.7735, Val Acc: 0.7640, Loss: 0.5198\n",
      "Epoch: 1382, Train Acc: 0.7641, Test Acc: 0.7808, Val Acc: 0.7640, Loss: 0.5184\n",
      "Epoch: 1383, Train Acc: 0.7678, Test Acc: 0.7823, Val Acc: 0.7670, Loss: 0.5201\n",
      "Epoch: 1384, Train Acc: 0.7635, Test Acc: 0.7796, Val Acc: 0.7685, Loss: 0.5175\n",
      "Epoch: 1385, Train Acc: 0.7669, Test Acc: 0.7817, Val Acc: 0.7640, Loss: 0.5238\n",
      "Epoch: 1386, Train Acc: 0.7692, Test Acc: 0.7780, Val Acc: 0.7746, Loss: 0.5231\n",
      "Epoch: 1387, Train Acc: 0.7655, Test Acc: 0.7732, Val Acc: 0.7595, Loss: 0.5204\n",
      "Epoch: 1388, Train Acc: 0.7607, Test Acc: 0.7702, Val Acc: 0.7534, Loss: 0.5216\n",
      "Epoch: 1389, Train Acc: 0.7601, Test Acc: 0.7702, Val Acc: 0.7595, Loss: 0.5209\n",
      "Epoch: 1390, Train Acc: 0.7591, Test Acc: 0.7690, Val Acc: 0.7534, Loss: 0.5169\n",
      "Epoch: 1391, Train Acc: 0.7579, Test Acc: 0.7675, Val Acc: 0.7610, Loss: 0.5245\n",
      "Epoch: 1392, Train Acc: 0.7620, Test Acc: 0.7735, Val Acc: 0.7610, Loss: 0.5210\n",
      "Epoch: 1393, Train Acc: 0.7692, Test Acc: 0.7790, Val Acc: 0.7640, Loss: 0.5159\n",
      "Epoch: 1394, Train Acc: 0.7633, Test Acc: 0.7735, Val Acc: 0.7685, Loss: 0.5264\n",
      "Epoch: 1395, Train Acc: 0.7620, Test Acc: 0.7699, Val Acc: 0.7595, Loss: 0.5185\n",
      "Epoch: 1396, Train Acc: 0.7643, Test Acc: 0.7720, Val Acc: 0.7670, Loss: 0.5202\n",
      "Epoch: 1397, Train Acc: 0.7583, Test Acc: 0.7690, Val Acc: 0.7655, Loss: 0.5182\n",
      "Epoch: 1398, Train Acc: 0.7552, Test Acc: 0.7635, Val Acc: 0.7579, Loss: 0.5201\n",
      "Epoch: 1399, Train Acc: 0.7631, Test Acc: 0.7711, Val Acc: 0.7655, Loss: 0.5195\n",
      "Epoch: 1400, Train Acc: 0.7663, Test Acc: 0.7747, Val Acc: 0.7700, Loss: 0.5247\n",
      "Epoch: 1401, Train Acc: 0.7599, Test Acc: 0.7696, Val Acc: 0.7610, Loss: 0.5190\n",
      "Epoch: 1402, Train Acc: 0.7617, Test Acc: 0.7723, Val Acc: 0.7640, Loss: 0.5166\n",
      "Epoch: 1403, Train Acc: 0.7678, Test Acc: 0.7796, Val Acc: 0.7761, Loss: 0.5222\n",
      "Epoch: 1404, Train Acc: 0.7634, Test Acc: 0.7762, Val Acc: 0.7625, Loss: 0.5193\n",
      "Epoch: 1405, Train Acc: 0.7576, Test Acc: 0.7729, Val Acc: 0.7564, Loss: 0.5239\n",
      "Epoch: 1406, Train Acc: 0.7622, Test Acc: 0.7741, Val Acc: 0.7640, Loss: 0.5210\n",
      "Epoch: 1407, Train Acc: 0.7663, Test Acc: 0.7765, Val Acc: 0.7564, Loss: 0.5187\n",
      "Epoch: 1408, Train Acc: 0.7657, Test Acc: 0.7762, Val Acc: 0.7534, Loss: 0.5146\n",
      "Epoch: 1409, Train Acc: 0.7646, Test Acc: 0.7720, Val Acc: 0.7519, Loss: 0.5201\n",
      "Epoch: 1410, Train Acc: 0.7690, Test Acc: 0.7808, Val Acc: 0.7564, Loss: 0.5180\n",
      "Epoch: 1411, Train Acc: 0.7667, Test Acc: 0.7768, Val Acc: 0.7655, Loss: 0.5207\n",
      "Epoch: 1412, Train Acc: 0.7593, Test Acc: 0.7690, Val Acc: 0.7670, Loss: 0.5241\n",
      "Epoch: 1413, Train Acc: 0.7667, Test Acc: 0.7768, Val Acc: 0.7625, Loss: 0.5185\n",
      "Epoch: 1414, Train Acc: 0.7682, Test Acc: 0.7796, Val Acc: 0.7700, Loss: 0.5199\n",
      "Epoch: 1415, Train Acc: 0.7669, Test Acc: 0.7759, Val Acc: 0.7716, Loss: 0.5199\n",
      "Epoch: 1416, Train Acc: 0.7647, Test Acc: 0.7741, Val Acc: 0.7625, Loss: 0.5198\n",
      "Epoch: 1417, Train Acc: 0.7669, Test Acc: 0.7790, Val Acc: 0.7716, Loss: 0.5214\n",
      "Epoch: 1418, Train Acc: 0.7676, Test Acc: 0.7780, Val Acc: 0.7700, Loss: 0.5230\n",
      "Epoch: 1419, Train Acc: 0.7631, Test Acc: 0.7738, Val Acc: 0.7625, Loss: 0.5232\n",
      "Epoch: 1420, Train Acc: 0.7599, Test Acc: 0.7726, Val Acc: 0.7595, Loss: 0.5179\n",
      "Epoch: 1421, Train Acc: 0.7650, Test Acc: 0.7774, Val Acc: 0.7625, Loss: 0.5186\n",
      "Epoch: 1422, Train Acc: 0.7672, Test Acc: 0.7783, Val Acc: 0.7685, Loss: 0.5209\n",
      "Epoch: 1423, Train Acc: 0.7616, Test Acc: 0.7726, Val Acc: 0.7655, Loss: 0.5231\n",
      "Epoch: 1424, Train Acc: 0.7606, Test Acc: 0.7741, Val Acc: 0.7640, Loss: 0.5248\n",
      "Epoch: 1425, Train Acc: 0.7659, Test Acc: 0.7762, Val Acc: 0.7610, Loss: 0.5208\n",
      "Epoch: 1426, Train Acc: 0.7672, Test Acc: 0.7762, Val Acc: 0.7655, Loss: 0.5194\n",
      "Epoch: 1427, Train Acc: 0.7642, Test Acc: 0.7714, Val Acc: 0.7761, Loss: 0.5189\n",
      "Epoch: 1428, Train Acc: 0.7624, Test Acc: 0.7693, Val Acc: 0.7716, Loss: 0.5159\n",
      "Epoch: 1429, Train Acc: 0.7696, Test Acc: 0.7796, Val Acc: 0.7640, Loss: 0.5203\n",
      "Epoch: 1430, Train Acc: 0.7687, Test Acc: 0.7768, Val Acc: 0.7685, Loss: 0.5236\n",
      "Epoch: 1431, Train Acc: 0.7542, Test Acc: 0.7656, Val Acc: 0.7625, Loss: 0.5219\n",
      "Epoch: 1432, Train Acc: 0.7608, Test Acc: 0.7696, Val Acc: 0.7640, Loss: 0.5162\n",
      "Epoch: 1433, Train Acc: 0.7717, Test Acc: 0.7777, Val Acc: 0.7655, Loss: 0.5229\n",
      "Epoch: 1434, Train Acc: 0.7650, Test Acc: 0.7756, Val Acc: 0.7564, Loss: 0.5209\n",
      "Epoch: 1435, Train Acc: 0.7620, Test Acc: 0.7747, Val Acc: 0.7534, Loss: 0.5219\n",
      "Epoch: 1436, Train Acc: 0.7657, Test Acc: 0.7774, Val Acc: 0.7443, Loss: 0.5151\n",
      "Epoch: 1437, Train Acc: 0.7680, Test Acc: 0.7805, Val Acc: 0.7413, Loss: 0.5206\n",
      "Epoch: 1438, Train Acc: 0.7681, Test Acc: 0.7771, Val Acc: 0.7458, Loss: 0.5183\n",
      "Epoch: 1439, Train Acc: 0.7662, Test Acc: 0.7774, Val Acc: 0.7549, Loss: 0.5195\n",
      "Epoch: 1440, Train Acc: 0.7701, Test Acc: 0.7780, Val Acc: 0.7670, Loss: 0.5194\n",
      "Epoch: 1441, Train Acc: 0.7662, Test Acc: 0.7753, Val Acc: 0.7655, Loss: 0.5231\n",
      "Epoch: 1442, Train Acc: 0.7600, Test Acc: 0.7717, Val Acc: 0.7625, Loss: 0.5159\n",
      "Epoch: 1443, Train Acc: 0.7674, Test Acc: 0.7765, Val Acc: 0.7610, Loss: 0.5215\n",
      "Epoch: 1444, Train Acc: 0.7697, Test Acc: 0.7787, Val Acc: 0.7625, Loss: 0.5184\n",
      "Epoch: 1445, Train Acc: 0.7635, Test Acc: 0.7735, Val Acc: 0.7564, Loss: 0.5189\n",
      "Epoch: 1446, Train Acc: 0.7627, Test Acc: 0.7723, Val Acc: 0.7564, Loss: 0.5202\n",
      "Epoch: 1447, Train Acc: 0.7670, Test Acc: 0.7753, Val Acc: 0.7655, Loss: 0.5186\n",
      "Epoch: 1448, Train Acc: 0.7675, Test Acc: 0.7783, Val Acc: 0.7564, Loss: 0.5197\n",
      "Epoch: 1449, Train Acc: 0.7673, Test Acc: 0.7780, Val Acc: 0.7595, Loss: 0.5186\n",
      "Epoch: 1450, Train Acc: 0.7670, Test Acc: 0.7805, Val Acc: 0.7655, Loss: 0.5211\n",
      "Epoch: 1451, Train Acc: 0.7619, Test Acc: 0.7711, Val Acc: 0.7610, Loss: 0.5176\n",
      "Epoch: 1452, Train Acc: 0.7630, Test Acc: 0.7759, Val Acc: 0.7640, Loss: 0.5205\n",
      "Epoch: 1453, Train Acc: 0.7669, Test Acc: 0.7759, Val Acc: 0.7685, Loss: 0.5213\n",
      "Epoch: 1454, Train Acc: 0.7628, Test Acc: 0.7708, Val Acc: 0.7670, Loss: 0.5186\n",
      "Epoch: 1455, Train Acc: 0.7570, Test Acc: 0.7641, Val Acc: 0.7640, Loss: 0.5138\n",
      "Epoch: 1456, Train Acc: 0.7647, Test Acc: 0.7753, Val Acc: 0.7700, Loss: 0.5232\n",
      "Epoch: 1457, Train Acc: 0.7642, Test Acc: 0.7774, Val Acc: 0.7670, Loss: 0.5129\n",
      "Epoch: 1458, Train Acc: 0.7609, Test Acc: 0.7741, Val Acc: 0.7549, Loss: 0.5240\n",
      "Epoch: 1459, Train Acc: 0.7559, Test Acc: 0.7723, Val Acc: 0.7413, Loss: 0.5245\n",
      "Epoch: 1460, Train Acc: 0.7595, Test Acc: 0.7750, Val Acc: 0.7519, Loss: 0.5229\n",
      "Epoch: 1461, Train Acc: 0.7637, Test Acc: 0.7768, Val Acc: 0.7610, Loss: 0.5178\n",
      "Epoch: 1462, Train Acc: 0.7644, Test Acc: 0.7750, Val Acc: 0.7640, Loss: 0.5201\n",
      "Epoch: 1463, Train Acc: 0.7648, Test Acc: 0.7759, Val Acc: 0.7731, Loss: 0.5176\n",
      "Epoch: 1464, Train Acc: 0.7654, Test Acc: 0.7780, Val Acc: 0.7716, Loss: 0.5248\n",
      "Epoch: 1465, Train Acc: 0.7657, Test Acc: 0.7771, Val Acc: 0.7670, Loss: 0.5187\n",
      "Epoch: 1466, Train Acc: 0.7668, Test Acc: 0.7783, Val Acc: 0.7549, Loss: 0.5223\n",
      "Epoch: 1467, Train Acc: 0.7665, Test Acc: 0.7790, Val Acc: 0.7564, Loss: 0.5184\n",
      "Epoch: 1468, Train Acc: 0.7655, Test Acc: 0.7750, Val Acc: 0.7519, Loss: 0.5198\n",
      "Epoch: 1469, Train Acc: 0.7614, Test Acc: 0.7714, Val Acc: 0.7564, Loss: 0.5166\n",
      "Epoch: 1470, Train Acc: 0.7588, Test Acc: 0.7711, Val Acc: 0.7579, Loss: 0.5174\n",
      "Epoch: 1471, Train Acc: 0.7648, Test Acc: 0.7753, Val Acc: 0.7655, Loss: 0.5216\n",
      "Epoch: 1472, Train Acc: 0.7646, Test Acc: 0.7774, Val Acc: 0.7655, Loss: 0.5170\n",
      "Epoch: 1473, Train Acc: 0.7627, Test Acc: 0.7771, Val Acc: 0.7670, Loss: 0.5204\n",
      "Epoch: 1474, Train Acc: 0.7657, Test Acc: 0.7780, Val Acc: 0.7700, Loss: 0.5173\n",
      "Epoch: 1475, Train Acc: 0.7694, Test Acc: 0.7790, Val Acc: 0.7806, Loss: 0.5237\n",
      "Epoch: 1476, Train Acc: 0.7615, Test Acc: 0.7720, Val Acc: 0.7700, Loss: 0.5229\n",
      "Epoch: 1477, Train Acc: 0.7634, Test Acc: 0.7777, Val Acc: 0.7685, Loss: 0.5200\n",
      "Epoch: 1478, Train Acc: 0.7661, Test Acc: 0.7783, Val Acc: 0.7655, Loss: 0.5161\n",
      "Epoch: 1479, Train Acc: 0.7638, Test Acc: 0.7735, Val Acc: 0.7640, Loss: 0.5225\n",
      "Epoch: 1480, Train Acc: 0.7573, Test Acc: 0.7708, Val Acc: 0.7610, Loss: 0.5175\n",
      "Epoch: 1481, Train Acc: 0.7648, Test Acc: 0.7744, Val Acc: 0.7534, Loss: 0.5241\n",
      "Epoch: 1482, Train Acc: 0.7659, Test Acc: 0.7774, Val Acc: 0.7564, Loss: 0.5202\n",
      "Epoch: 1483, Train Acc: 0.7654, Test Acc: 0.7756, Val Acc: 0.7549, Loss: 0.5239\n",
      "Epoch: 1484, Train Acc: 0.7640, Test Acc: 0.7768, Val Acc: 0.7640, Loss: 0.5284\n",
      "Epoch: 1485, Train Acc: 0.7630, Test Acc: 0.7787, Val Acc: 0.7716, Loss: 0.5213\n",
      "Epoch: 1486, Train Acc: 0.7618, Test Acc: 0.7762, Val Acc: 0.7746, Loss: 0.5194\n",
      "Epoch: 1487, Train Acc: 0.7631, Test Acc: 0.7747, Val Acc: 0.7700, Loss: 0.5226\n",
      "Epoch: 1488, Train Acc: 0.7662, Test Acc: 0.7774, Val Acc: 0.7700, Loss: 0.5211\n",
      "Epoch: 1489, Train Acc: 0.7604, Test Acc: 0.7690, Val Acc: 0.7625, Loss: 0.5201\n",
      "Epoch: 1490, Train Acc: 0.7620, Test Acc: 0.7723, Val Acc: 0.7595, Loss: 0.5161\n",
      "Epoch: 1491, Train Acc: 0.7688, Test Acc: 0.7793, Val Acc: 0.7700, Loss: 0.5244\n",
      "Epoch: 1492, Train Acc: 0.7665, Test Acc: 0.7756, Val Acc: 0.7716, Loss: 0.5215\n",
      "Epoch: 1493, Train Acc: 0.7561, Test Acc: 0.7635, Val Acc: 0.7625, Loss: 0.5169\n",
      "Epoch: 1494, Train Acc: 0.7620, Test Acc: 0.7711, Val Acc: 0.7655, Loss: 0.5220\n",
      "Epoch: 1495, Train Acc: 0.7672, Test Acc: 0.7796, Val Acc: 0.7685, Loss: 0.5225\n",
      "Epoch: 1496, Train Acc: 0.7601, Test Acc: 0.7717, Val Acc: 0.7489, Loss: 0.5195\n",
      "Epoch: 1497, Train Acc: 0.7576, Test Acc: 0.7714, Val Acc: 0.7458, Loss: 0.5247\n",
      "Epoch: 1498, Train Acc: 0.7651, Test Acc: 0.7756, Val Acc: 0.7640, Loss: 0.5172\n",
      "Epoch: 1499, Train Acc: 0.7617, Test Acc: 0.7687, Val Acc: 0.7610, Loss: 0.5208\n",
      "Epoch: 1500, Train Acc: 0.7588, Test Acc: 0.7663, Val Acc: 0.7610, Loss: 0.5172\n",
      "Epoch: 1501, Train Acc: 0.7691, Test Acc: 0.7750, Val Acc: 0.7579, Loss: 0.5147\n",
      "Epoch: 1502, Train Acc: 0.7687, Test Acc: 0.7771, Val Acc: 0.7519, Loss: 0.5188\n",
      "Epoch: 1503, Train Acc: 0.7641, Test Acc: 0.7750, Val Acc: 0.7595, Loss: 0.5194\n",
      "Epoch: 1504, Train Acc: 0.7675, Test Acc: 0.7835, Val Acc: 0.7625, Loss: 0.5147\n",
      "Epoch: 1505, Train Acc: 0.7668, Test Acc: 0.7811, Val Acc: 0.7625, Loss: 0.5200\n",
      "Epoch: 1506, Train Acc: 0.7572, Test Acc: 0.7684, Val Acc: 0.7640, Loss: 0.5240\n",
      "Epoch: 1507, Train Acc: 0.7640, Test Acc: 0.7747, Val Acc: 0.7685, Loss: 0.5205\n",
      "Epoch: 1508, Train Acc: 0.7680, Test Acc: 0.7753, Val Acc: 0.7655, Loss: 0.5203\n",
      "Epoch: 1509, Train Acc: 0.7578, Test Acc: 0.7705, Val Acc: 0.7610, Loss: 0.5249\n",
      "Epoch: 1510, Train Acc: 0.7515, Test Acc: 0.7617, Val Acc: 0.7474, Loss: 0.5183\n",
      "Epoch: 1511, Train Acc: 0.7647, Test Acc: 0.7759, Val Acc: 0.7716, Loss: 0.5170\n",
      "Epoch: 1512, Train Acc: 0.7590, Test Acc: 0.7729, Val Acc: 0.7504, Loss: 0.5210\n",
      "Epoch: 1513, Train Acc: 0.7576, Test Acc: 0.7693, Val Acc: 0.7534, Loss: 0.5226\n",
      "Epoch: 1514, Train Acc: 0.7621, Test Acc: 0.7714, Val Acc: 0.7595, Loss: 0.5159\n",
      "Epoch: 1515, Train Acc: 0.7682, Test Acc: 0.7814, Val Acc: 0.7746, Loss: 0.5154\n",
      "Epoch: 1516, Train Acc: 0.7674, Test Acc: 0.7768, Val Acc: 0.7731, Loss: 0.5199\n",
      "Epoch: 1517, Train Acc: 0.7605, Test Acc: 0.7726, Val Acc: 0.7610, Loss: 0.5164\n",
      "Epoch: 1518, Train Acc: 0.7637, Test Acc: 0.7787, Val Acc: 0.7700, Loss: 0.5229\n",
      "Epoch: 1519, Train Acc: 0.7660, Test Acc: 0.7783, Val Acc: 0.7700, Loss: 0.5219\n",
      "Epoch: 1520, Train Acc: 0.7587, Test Acc: 0.7693, Val Acc: 0.7685, Loss: 0.5242\n",
      "Epoch: 1521, Train Acc: 0.7514, Test Acc: 0.7653, Val Acc: 0.7564, Loss: 0.5183\n",
      "Epoch: 1522, Train Acc: 0.7654, Test Acc: 0.7787, Val Acc: 0.7625, Loss: 0.5265\n",
      "Epoch: 1523, Train Acc: 0.7655, Test Acc: 0.7802, Val Acc: 0.7640, Loss: 0.5192\n",
      "Epoch: 1524, Train Acc: 0.7545, Test Acc: 0.7684, Val Acc: 0.7474, Loss: 0.5222\n",
      "Epoch: 1525, Train Acc: 0.7613, Test Acc: 0.7753, Val Acc: 0.7579, Loss: 0.5209\n",
      "Epoch: 1526, Train Acc: 0.7696, Test Acc: 0.7796, Val Acc: 0.7761, Loss: 0.5215\n",
      "Epoch: 1527, Train Acc: 0.7638, Test Acc: 0.7787, Val Acc: 0.7595, Loss: 0.5198\n",
      "Epoch: 1528, Train Acc: 0.7582, Test Acc: 0.7690, Val Acc: 0.7564, Loss: 0.5201\n",
      "Epoch: 1529, Train Acc: 0.7627, Test Acc: 0.7762, Val Acc: 0.7670, Loss: 0.5192\n",
      "Epoch: 1530, Train Acc: 0.7646, Test Acc: 0.7747, Val Acc: 0.7610, Loss: 0.5196\n",
      "Epoch: 1531, Train Acc: 0.7556, Test Acc: 0.7647, Val Acc: 0.7549, Loss: 0.5174\n",
      "Epoch: 1532, Train Acc: 0.7596, Test Acc: 0.7708, Val Acc: 0.7519, Loss: 0.5157\n",
      "Epoch: 1533, Train Acc: 0.7659, Test Acc: 0.7756, Val Acc: 0.7625, Loss: 0.5176\n",
      "Epoch: 1534, Train Acc: 0.7647, Test Acc: 0.7732, Val Acc: 0.7549, Loss: 0.5172\n",
      "Epoch: 1535, Train Acc: 0.7628, Test Acc: 0.7705, Val Acc: 0.7549, Loss: 0.5191\n",
      "Epoch: 1536, Train Acc: 0.7688, Test Acc: 0.7811, Val Acc: 0.7685, Loss: 0.5135\n",
      "Epoch: 1537, Train Acc: 0.7688, Test Acc: 0.7768, Val Acc: 0.7716, Loss: 0.5225\n",
      "Epoch: 1538, Train Acc: 0.7599, Test Acc: 0.7705, Val Acc: 0.7655, Loss: 0.5205\n",
      "Epoch: 1539, Train Acc: 0.7635, Test Acc: 0.7729, Val Acc: 0.7655, Loss: 0.5204\n",
      "Epoch: 1540, Train Acc: 0.7697, Test Acc: 0.7771, Val Acc: 0.7700, Loss: 0.5147\n",
      "Epoch: 1541, Train Acc: 0.7657, Test Acc: 0.7780, Val Acc: 0.7700, Loss: 0.5169\n",
      "Epoch: 1542, Train Acc: 0.7609, Test Acc: 0.7726, Val Acc: 0.7806, Loss: 0.5162\n",
      "Epoch: 1543, Train Acc: 0.7603, Test Acc: 0.7723, Val Acc: 0.7746, Loss: 0.5183\n",
      "Epoch: 1544, Train Acc: 0.7661, Test Acc: 0.7771, Val Acc: 0.7716, Loss: 0.5167\n",
      "Epoch: 1545, Train Acc: 0.7618, Test Acc: 0.7732, Val Acc: 0.7625, Loss: 0.5174\n",
      "Epoch: 1546, Train Acc: 0.7606, Test Acc: 0.7714, Val Acc: 0.7625, Loss: 0.5165\n",
      "Epoch: 1547, Train Acc: 0.7644, Test Acc: 0.7747, Val Acc: 0.7731, Loss: 0.5190\n",
      "Epoch: 1548, Train Acc: 0.7676, Test Acc: 0.7774, Val Acc: 0.7731, Loss: 0.5216\n",
      "Epoch: 1549, Train Acc: 0.7668, Test Acc: 0.7777, Val Acc: 0.7700, Loss: 0.5238\n",
      "Epoch: 1550, Train Acc: 0.7631, Test Acc: 0.7732, Val Acc: 0.7519, Loss: 0.5211\n",
      "Epoch: 1551, Train Acc: 0.7623, Test Acc: 0.7699, Val Acc: 0.7489, Loss: 0.5219\n",
      "Epoch: 1552, Train Acc: 0.7595, Test Acc: 0.7663, Val Acc: 0.7474, Loss: 0.5246\n",
      "Epoch: 1553, Train Acc: 0.7590, Test Acc: 0.7717, Val Acc: 0.7504, Loss: 0.5210\n",
      "Epoch: 1554, Train Acc: 0.7581, Test Acc: 0.7711, Val Acc: 0.7489, Loss: 0.5212\n",
      "Epoch: 1555, Train Acc: 0.7608, Test Acc: 0.7699, Val Acc: 0.7625, Loss: 0.5171\n",
      "Epoch: 1556, Train Acc: 0.7626, Test Acc: 0.7738, Val Acc: 0.7716, Loss: 0.5185\n",
      "Epoch: 1557, Train Acc: 0.7599, Test Acc: 0.7708, Val Acc: 0.7625, Loss: 0.5225\n",
      "Epoch: 1558, Train Acc: 0.7644, Test Acc: 0.7738, Val Acc: 0.7595, Loss: 0.5193\n",
      "Epoch: 1559, Train Acc: 0.7637, Test Acc: 0.7738, Val Acc: 0.7640, Loss: 0.5209\n",
      "Epoch: 1560, Train Acc: 0.7594, Test Acc: 0.7720, Val Acc: 0.7534, Loss: 0.5185\n",
      "Epoch: 1561, Train Acc: 0.7653, Test Acc: 0.7802, Val Acc: 0.7655, Loss: 0.5191\n",
      "Epoch: 1562, Train Acc: 0.7645, Test Acc: 0.7787, Val Acc: 0.7579, Loss: 0.5237\n",
      "Epoch: 1563, Train Acc: 0.7596, Test Acc: 0.7714, Val Acc: 0.7595, Loss: 0.5214\n",
      "Epoch: 1564, Train Acc: 0.7669, Test Acc: 0.7762, Val Acc: 0.7685, Loss: 0.5218\n",
      "Epoch: 1565, Train Acc: 0.7667, Test Acc: 0.7783, Val Acc: 0.7640, Loss: 0.5233\n",
      "Epoch: 1566, Train Acc: 0.7647, Test Acc: 0.7783, Val Acc: 0.7610, Loss: 0.5221\n",
      "Epoch: 1567, Train Acc: 0.7699, Test Acc: 0.7850, Val Acc: 0.7746, Loss: 0.5194\n",
      "Epoch: 1568, Train Acc: 0.7675, Test Acc: 0.7832, Val Acc: 0.7716, Loss: 0.5258\n",
      "Epoch: 1569, Train Acc: 0.7653, Test Acc: 0.7750, Val Acc: 0.7685, Loss: 0.5176\n",
      "Epoch: 1570, Train Acc: 0.7619, Test Acc: 0.7747, Val Acc: 0.7731, Loss: 0.5224\n",
      "Epoch: 1571, Train Acc: 0.7648, Test Acc: 0.7771, Val Acc: 0.7746, Loss: 0.5127\n",
      "Epoch: 1572, Train Acc: 0.7629, Test Acc: 0.7729, Val Acc: 0.7761, Loss: 0.5174\n",
      "Epoch: 1573, Train Acc: 0.7619, Test Acc: 0.7720, Val Acc: 0.7731, Loss: 0.5201\n",
      "Epoch: 1574, Train Acc: 0.7634, Test Acc: 0.7753, Val Acc: 0.7640, Loss: 0.5203\n",
      "Epoch: 1575, Train Acc: 0.7620, Test Acc: 0.7747, Val Acc: 0.7579, Loss: 0.5211\n",
      "Epoch: 1576, Train Acc: 0.7617, Test Acc: 0.7741, Val Acc: 0.7519, Loss: 0.5165\n",
      "Epoch: 1577, Train Acc: 0.7611, Test Acc: 0.7747, Val Acc: 0.7474, Loss: 0.5263\n",
      "Epoch: 1578, Train Acc: 0.7628, Test Acc: 0.7732, Val Acc: 0.7504, Loss: 0.5254\n",
      "Epoch: 1579, Train Acc: 0.7622, Test Acc: 0.7738, Val Acc: 0.7685, Loss: 0.5187\n",
      "Epoch: 1580, Train Acc: 0.7624, Test Acc: 0.7771, Val Acc: 0.7640, Loss: 0.5203\n",
      "Epoch: 1581, Train Acc: 0.7638, Test Acc: 0.7774, Val Acc: 0.7655, Loss: 0.5208\n",
      "Epoch: 1582, Train Acc: 0.7655, Test Acc: 0.7762, Val Acc: 0.7685, Loss: 0.5240\n",
      "Epoch: 1583, Train Acc: 0.7638, Test Acc: 0.7762, Val Acc: 0.7549, Loss: 0.5214\n",
      "Epoch: 1584, Train Acc: 0.7633, Test Acc: 0.7729, Val Acc: 0.7670, Loss: 0.5193\n",
      "Epoch: 1585, Train Acc: 0.7661, Test Acc: 0.7787, Val Acc: 0.7746, Loss: 0.5169\n",
      "Epoch: 1586, Train Acc: 0.7614, Test Acc: 0.7741, Val Acc: 0.7700, Loss: 0.5190\n",
      "Epoch: 1587, Train Acc: 0.7603, Test Acc: 0.7732, Val Acc: 0.7731, Loss: 0.5164\n",
      "Epoch: 1588, Train Acc: 0.7632, Test Acc: 0.7714, Val Acc: 0.7655, Loss: 0.5197\n",
      "Epoch: 1589, Train Acc: 0.7614, Test Acc: 0.7684, Val Acc: 0.7655, Loss: 0.5209\n",
      "Epoch: 1590, Train Acc: 0.7594, Test Acc: 0.7687, Val Acc: 0.7579, Loss: 0.5209\n",
      "Epoch: 1591, Train Acc: 0.7607, Test Acc: 0.7708, Val Acc: 0.7655, Loss: 0.5219\n",
      "Epoch: 1592, Train Acc: 0.7624, Test Acc: 0.7723, Val Acc: 0.7610, Loss: 0.5178\n",
      "Epoch: 1593, Train Acc: 0.7657, Test Acc: 0.7777, Val Acc: 0.7655, Loss: 0.5167\n",
      "Epoch: 1594, Train Acc: 0.7659, Test Acc: 0.7811, Val Acc: 0.7670, Loss: 0.5188\n",
      "Epoch: 1595, Train Acc: 0.7630, Test Acc: 0.7738, Val Acc: 0.7640, Loss: 0.5208\n",
      "Epoch: 1596, Train Acc: 0.7631, Test Acc: 0.7729, Val Acc: 0.7685, Loss: 0.5281\n",
      "Epoch: 1597, Train Acc: 0.7651, Test Acc: 0.7777, Val Acc: 0.7700, Loss: 0.5168\n",
      "Epoch: 1598, Train Acc: 0.7605, Test Acc: 0.7741, Val Acc: 0.7625, Loss: 0.5168\n",
      "Epoch: 1599, Train Acc: 0.7605, Test Acc: 0.7744, Val Acc: 0.7655, Loss: 0.5186\n",
      "Epoch: 1600, Train Acc: 0.7674, Test Acc: 0.7814, Val Acc: 0.7731, Loss: 0.5209\n",
      "Epoch: 1601, Train Acc: 0.7675, Test Acc: 0.7771, Val Acc: 0.7716, Loss: 0.5176\n",
      "Epoch: 1602, Train Acc: 0.7592, Test Acc: 0.7723, Val Acc: 0.7625, Loss: 0.5269\n",
      "Epoch: 1603, Train Acc: 0.7637, Test Acc: 0.7720, Val Acc: 0.7655, Loss: 0.5213\n",
      "Epoch: 1604, Train Acc: 0.7640, Test Acc: 0.7687, Val Acc: 0.7610, Loss: 0.5197\n",
      "Epoch: 1605, Train Acc: 0.7586, Test Acc: 0.7666, Val Acc: 0.7549, Loss: 0.5153\n",
      "Epoch: 1606, Train Acc: 0.7615, Test Acc: 0.7717, Val Acc: 0.7625, Loss: 0.5187\n",
      "Epoch: 1607, Train Acc: 0.7641, Test Acc: 0.7744, Val Acc: 0.7716, Loss: 0.5148\n",
      "Epoch: 1608, Train Acc: 0.7685, Test Acc: 0.7790, Val Acc: 0.7776, Loss: 0.5186\n",
      "Epoch: 1609, Train Acc: 0.7606, Test Acc: 0.7735, Val Acc: 0.7595, Loss: 0.5209\n",
      "Epoch: 1610, Train Acc: 0.7616, Test Acc: 0.7783, Val Acc: 0.7640, Loss: 0.5191\n",
      "Epoch: 1611, Train Acc: 0.7675, Test Acc: 0.7799, Val Acc: 0.7655, Loss: 0.5150\n",
      "Epoch: 1612, Train Acc: 0.7661, Test Acc: 0.7765, Val Acc: 0.7746, Loss: 0.5234\n",
      "Epoch: 1613, Train Acc: 0.7646, Test Acc: 0.7762, Val Acc: 0.7716, Loss: 0.5249\n",
      "Epoch: 1614, Train Acc: 0.7682, Test Acc: 0.7787, Val Acc: 0.7746, Loss: 0.5194\n",
      "Epoch: 1615, Train Acc: 0.7686, Test Acc: 0.7808, Val Acc: 0.7746, Loss: 0.5203\n",
      "Epoch: 1616, Train Acc: 0.7665, Test Acc: 0.7759, Val Acc: 0.7655, Loss: 0.5222\n",
      "Epoch: 1617, Train Acc: 0.7592, Test Acc: 0.7711, Val Acc: 0.7519, Loss: 0.5166\n",
      "Epoch: 1618, Train Acc: 0.7623, Test Acc: 0.7708, Val Acc: 0.7579, Loss: 0.5169\n",
      "Epoch: 1619, Train Acc: 0.7646, Test Acc: 0.7771, Val Acc: 0.7640, Loss: 0.5254\n",
      "Epoch: 1620, Train Acc: 0.7622, Test Acc: 0.7711, Val Acc: 0.7579, Loss: 0.5277\n",
      "Epoch: 1621, Train Acc: 0.7569, Test Acc: 0.7632, Val Acc: 0.7564, Loss: 0.5177\n",
      "Epoch: 1622, Train Acc: 0.7644, Test Acc: 0.7735, Val Acc: 0.7519, Loss: 0.5264\n",
      "Epoch: 1623, Train Acc: 0.7633, Test Acc: 0.7735, Val Acc: 0.7595, Loss: 0.5233\n",
      "Epoch: 1624, Train Acc: 0.7562, Test Acc: 0.7687, Val Acc: 0.7564, Loss: 0.5220\n",
      "Epoch: 1625, Train Acc: 0.7607, Test Acc: 0.7690, Val Acc: 0.7534, Loss: 0.5228\n",
      "Epoch: 1626, Train Acc: 0.7669, Test Acc: 0.7762, Val Acc: 0.7625, Loss: 0.5209\n",
      "Epoch: 1627, Train Acc: 0.7606, Test Acc: 0.7693, Val Acc: 0.7610, Loss: 0.5185\n",
      "Epoch: 1628, Train Acc: 0.7629, Test Acc: 0.7747, Val Acc: 0.7610, Loss: 0.5227\n",
      "Epoch: 1629, Train Acc: 0.7645, Test Acc: 0.7732, Val Acc: 0.7640, Loss: 0.5216\n",
      "Epoch: 1630, Train Acc: 0.7609, Test Acc: 0.7756, Val Acc: 0.7610, Loss: 0.5234\n",
      "Epoch: 1631, Train Acc: 0.7578, Test Acc: 0.7723, Val Acc: 0.7534, Loss: 0.5215\n",
      "Epoch: 1632, Train Acc: 0.7611, Test Acc: 0.7726, Val Acc: 0.7655, Loss: 0.5182\n",
      "Epoch: 1633, Train Acc: 0.7669, Test Acc: 0.7765, Val Acc: 0.7670, Loss: 0.5219\n",
      "Epoch: 1634, Train Acc: 0.7660, Test Acc: 0.7750, Val Acc: 0.7579, Loss: 0.5194\n",
      "Epoch: 1635, Train Acc: 0.7659, Test Acc: 0.7732, Val Acc: 0.7595, Loss: 0.5201\n",
      "Epoch: 1636, Train Acc: 0.7631, Test Acc: 0.7732, Val Acc: 0.7579, Loss: 0.5205\n",
      "Epoch: 1637, Train Acc: 0.7676, Test Acc: 0.7759, Val Acc: 0.7504, Loss: 0.5182\n",
      "Epoch: 1638, Train Acc: 0.7627, Test Acc: 0.7723, Val Acc: 0.7670, Loss: 0.5222\n",
      "Epoch: 1639, Train Acc: 0.7592, Test Acc: 0.7708, Val Acc: 0.7640, Loss: 0.5233\n",
      "Epoch: 1640, Train Acc: 0.7674, Test Acc: 0.7790, Val Acc: 0.7791, Loss: 0.5160\n",
      "Epoch: 1641, Train Acc: 0.7630, Test Acc: 0.7762, Val Acc: 0.7731, Loss: 0.5178\n",
      "Epoch: 1642, Train Acc: 0.7588, Test Acc: 0.7738, Val Acc: 0.7625, Loss: 0.5175\n",
      "Epoch: 1643, Train Acc: 0.7646, Test Acc: 0.7790, Val Acc: 0.7685, Loss: 0.5191\n",
      "Epoch: 1644, Train Acc: 0.7601, Test Acc: 0.7753, Val Acc: 0.7625, Loss: 0.5264\n",
      "Epoch: 1645, Train Acc: 0.7567, Test Acc: 0.7714, Val Acc: 0.7625, Loss: 0.5237\n",
      "Epoch: 1646, Train Acc: 0.7635, Test Acc: 0.7735, Val Acc: 0.7670, Loss: 0.5200\n",
      "Epoch: 1647, Train Acc: 0.7663, Test Acc: 0.7732, Val Acc: 0.7640, Loss: 0.5237\n",
      "Epoch: 1648, Train Acc: 0.7616, Test Acc: 0.7714, Val Acc: 0.7670, Loss: 0.5223\n",
      "Epoch: 1649, Train Acc: 0.7592, Test Acc: 0.7717, Val Acc: 0.7670, Loss: 0.5136\n",
      "Epoch: 1650, Train Acc: 0.7645, Test Acc: 0.7765, Val Acc: 0.7731, Loss: 0.5265\n",
      "Epoch: 1651, Train Acc: 0.7685, Test Acc: 0.7762, Val Acc: 0.7670, Loss: 0.5207\n",
      "Epoch: 1652, Train Acc: 0.7580, Test Acc: 0.7666, Val Acc: 0.7610, Loss: 0.5261\n",
      "Epoch: 1653, Train Acc: 0.7632, Test Acc: 0.7726, Val Acc: 0.7640, Loss: 0.5216\n",
      "Epoch: 1654, Train Acc: 0.7672, Test Acc: 0.7768, Val Acc: 0.7761, Loss: 0.5239\n",
      "Epoch: 1655, Train Acc: 0.7624, Test Acc: 0.7774, Val Acc: 0.7655, Loss: 0.5168\n",
      "Epoch: 1656, Train Acc: 0.7576, Test Acc: 0.7774, Val Acc: 0.7610, Loss: 0.5266\n",
      "Epoch: 1657, Train Acc: 0.7675, Test Acc: 0.7835, Val Acc: 0.7716, Loss: 0.5229\n",
      "Epoch: 1658, Train Acc: 0.7653, Test Acc: 0.7802, Val Acc: 0.7685, Loss: 0.5269\n",
      "Epoch: 1659, Train Acc: 0.7559, Test Acc: 0.7684, Val Acc: 0.7610, Loss: 0.5174\n",
      "Epoch: 1660, Train Acc: 0.7589, Test Acc: 0.7687, Val Acc: 0.7625, Loss: 0.5200\n",
      "Epoch: 1661, Train Acc: 0.7653, Test Acc: 0.7738, Val Acc: 0.7746, Loss: 0.5233\n",
      "Epoch: 1662, Train Acc: 0.7600, Test Acc: 0.7717, Val Acc: 0.7625, Loss: 0.5167\n",
      "Epoch: 1663, Train Acc: 0.7569, Test Acc: 0.7717, Val Acc: 0.7549, Loss: 0.5227\n",
      "Epoch: 1664, Train Acc: 0.7615, Test Acc: 0.7732, Val Acc: 0.7610, Loss: 0.5248\n",
      "Epoch: 1665, Train Acc: 0.7658, Test Acc: 0.7756, Val Acc: 0.7685, Loss: 0.5199\n",
      "Epoch: 1666, Train Acc: 0.7645, Test Acc: 0.7738, Val Acc: 0.7640, Loss: 0.5170\n",
      "Epoch: 1667, Train Acc: 0.7683, Test Acc: 0.7750, Val Acc: 0.7776, Loss: 0.5189\n",
      "Epoch: 1668, Train Acc: 0.7691, Test Acc: 0.7759, Val Acc: 0.7806, Loss: 0.5207\n",
      "Epoch: 1669, Train Acc: 0.7657, Test Acc: 0.7756, Val Acc: 0.7761, Loss: 0.5219\n",
      "Epoch: 1670, Train Acc: 0.7646, Test Acc: 0.7735, Val Acc: 0.7655, Loss: 0.5179\n",
      "Epoch: 1671, Train Acc: 0.7702, Test Acc: 0.7799, Val Acc: 0.7655, Loss: 0.5192\n",
      "Epoch: 1672, Train Acc: 0.7620, Test Acc: 0.7702, Val Acc: 0.7595, Loss: 0.5205\n",
      "Epoch: 1673, Train Acc: 0.7593, Test Acc: 0.7717, Val Acc: 0.7504, Loss: 0.5188\n",
      "Epoch: 1674, Train Acc: 0.7638, Test Acc: 0.7759, Val Acc: 0.7564, Loss: 0.5210\n",
      "Epoch: 1675, Train Acc: 0.7613, Test Acc: 0.7726, Val Acc: 0.7640, Loss: 0.5178\n",
      "Epoch: 1676, Train Acc: 0.7619, Test Acc: 0.7720, Val Acc: 0.7716, Loss: 0.5215\n",
      "Epoch: 1677, Train Acc: 0.7654, Test Acc: 0.7759, Val Acc: 0.7731, Loss: 0.5237\n",
      "Epoch: 1678, Train Acc: 0.7657, Test Acc: 0.7756, Val Acc: 0.7670, Loss: 0.5235\n",
      "Epoch: 1679, Train Acc: 0.7619, Test Acc: 0.7693, Val Acc: 0.7640, Loss: 0.5174\n",
      "Epoch: 1680, Train Acc: 0.7632, Test Acc: 0.7771, Val Acc: 0.7610, Loss: 0.5231\n",
      "Epoch: 1681, Train Acc: 0.7633, Test Acc: 0.7756, Val Acc: 0.7670, Loss: 0.5199\n",
      "Epoch: 1682, Train Acc: 0.7627, Test Acc: 0.7756, Val Acc: 0.7610, Loss: 0.5249\n",
      "Epoch: 1683, Train Acc: 0.7610, Test Acc: 0.7747, Val Acc: 0.7595, Loss: 0.5173\n",
      "Epoch: 1684, Train Acc: 0.7630, Test Acc: 0.7756, Val Acc: 0.7534, Loss: 0.5189\n",
      "Epoch: 1685, Train Acc: 0.7676, Test Acc: 0.7790, Val Acc: 0.7685, Loss: 0.5202\n",
      "Epoch: 1686, Train Acc: 0.7638, Test Acc: 0.7783, Val Acc: 0.7670, Loss: 0.5093\n",
      "Epoch: 1687, Train Acc: 0.7660, Test Acc: 0.7805, Val Acc: 0.7670, Loss: 0.5221\n",
      "Epoch: 1688, Train Acc: 0.7702, Test Acc: 0.7823, Val Acc: 0.7625, Loss: 0.5242\n",
      "Epoch: 1689, Train Acc: 0.7642, Test Acc: 0.7793, Val Acc: 0.7610, Loss: 0.5259\n",
      "Epoch: 1690, Train Acc: 0.7611, Test Acc: 0.7735, Val Acc: 0.7564, Loss: 0.5248\n",
      "Epoch: 1691, Train Acc: 0.7665, Test Acc: 0.7756, Val Acc: 0.7640, Loss: 0.5195\n",
      "Epoch: 1692, Train Acc: 0.7669, Test Acc: 0.7790, Val Acc: 0.7655, Loss: 0.5231\n",
      "Epoch: 1693, Train Acc: 0.7606, Test Acc: 0.7744, Val Acc: 0.7670, Loss: 0.5251\n",
      "Epoch: 1694, Train Acc: 0.7597, Test Acc: 0.7729, Val Acc: 0.7731, Loss: 0.5183\n",
      "Epoch: 1695, Train Acc: 0.7690, Test Acc: 0.7765, Val Acc: 0.7655, Loss: 0.5185\n",
      "Epoch: 1696, Train Acc: 0.7621, Test Acc: 0.7717, Val Acc: 0.7716, Loss: 0.5255\n",
      "Epoch: 1697, Train Acc: 0.7565, Test Acc: 0.7629, Val Acc: 0.7595, Loss: 0.5203\n",
      "Epoch: 1698, Train Acc: 0.7654, Test Acc: 0.7714, Val Acc: 0.7625, Loss: 0.5163\n",
      "Epoch: 1699, Train Acc: 0.7681, Test Acc: 0.7777, Val Acc: 0.7655, Loss: 0.5219\n",
      "Epoch: 1700, Train Acc: 0.7562, Test Acc: 0.7672, Val Acc: 0.7579, Loss: 0.5200\n",
      "Epoch: 1701, Train Acc: 0.7650, Test Acc: 0.7771, Val Acc: 0.7670, Loss: 0.5198\n",
      "Epoch: 1702, Train Acc: 0.7691, Test Acc: 0.7820, Val Acc: 0.7776, Loss: 0.5184\n",
      "Epoch: 1703, Train Acc: 0.7662, Test Acc: 0.7783, Val Acc: 0.7655, Loss: 0.5180\n",
      "Epoch: 1704, Train Acc: 0.7634, Test Acc: 0.7774, Val Acc: 0.7579, Loss: 0.5171\n",
      "Epoch: 1705, Train Acc: 0.7670, Test Acc: 0.7811, Val Acc: 0.7655, Loss: 0.5252\n",
      "Epoch: 1706, Train Acc: 0.7636, Test Acc: 0.7796, Val Acc: 0.7564, Loss: 0.5180\n",
      "Epoch: 1707, Train Acc: 0.7593, Test Acc: 0.7744, Val Acc: 0.7610, Loss: 0.5203\n",
      "Epoch: 1708, Train Acc: 0.7632, Test Acc: 0.7759, Val Acc: 0.7700, Loss: 0.5238\n",
      "Epoch: 1709, Train Acc: 0.7627, Test Acc: 0.7729, Val Acc: 0.7791, Loss: 0.5205\n",
      "Epoch: 1710, Train Acc: 0.7609, Test Acc: 0.7723, Val Acc: 0.7655, Loss: 0.5223\n",
      "Epoch: 1711, Train Acc: 0.7604, Test Acc: 0.7723, Val Acc: 0.7579, Loss: 0.5187\n",
      "Epoch: 1712, Train Acc: 0.7615, Test Acc: 0.7744, Val Acc: 0.7534, Loss: 0.5198\n",
      "Epoch: 1713, Train Acc: 0.7644, Test Acc: 0.7811, Val Acc: 0.7549, Loss: 0.5170\n",
      "Epoch: 1714, Train Acc: 0.7634, Test Acc: 0.7783, Val Acc: 0.7564, Loss: 0.5147\n",
      "Epoch: 1715, Train Acc: 0.7662, Test Acc: 0.7823, Val Acc: 0.7640, Loss: 0.5179\n",
      "Epoch: 1716, Train Acc: 0.7668, Test Acc: 0.7805, Val Acc: 0.7700, Loss: 0.5203\n",
      "Epoch: 1717, Train Acc: 0.7637, Test Acc: 0.7793, Val Acc: 0.7700, Loss: 0.5177\n",
      "Epoch: 1718, Train Acc: 0.7660, Test Acc: 0.7783, Val Acc: 0.7685, Loss: 0.5177\n",
      "Epoch: 1719, Train Acc: 0.7619, Test Acc: 0.7777, Val Acc: 0.7640, Loss: 0.5199\n",
      "Epoch: 1720, Train Acc: 0.7622, Test Acc: 0.7771, Val Acc: 0.7731, Loss: 0.5164\n",
      "Epoch: 1721, Train Acc: 0.7698, Test Acc: 0.7811, Val Acc: 0.7716, Loss: 0.5189\n",
      "Epoch: 1722, Train Acc: 0.7622, Test Acc: 0.7768, Val Acc: 0.7776, Loss: 0.5256\n",
      "Epoch: 1723, Train Acc: 0.7591, Test Acc: 0.7714, Val Acc: 0.7655, Loss: 0.5197\n",
      "Epoch: 1724, Train Acc: 0.7685, Test Acc: 0.7783, Val Acc: 0.7746, Loss: 0.5189\n",
      "Epoch: 1725, Train Acc: 0.7707, Test Acc: 0.7811, Val Acc: 0.7746, Loss: 0.5193\n",
      "Epoch: 1726, Train Acc: 0.7650, Test Acc: 0.7777, Val Acc: 0.7716, Loss: 0.5212\n",
      "Epoch: 1727, Train Acc: 0.7631, Test Acc: 0.7738, Val Acc: 0.7731, Loss: 0.5160\n",
      "Epoch: 1728, Train Acc: 0.7675, Test Acc: 0.7796, Val Acc: 0.7685, Loss: 0.5147\n",
      "Epoch: 1729, Train Acc: 0.7654, Test Acc: 0.7802, Val Acc: 0.7716, Loss: 0.5211\n",
      "Epoch: 1730, Train Acc: 0.7620, Test Acc: 0.7735, Val Acc: 0.7670, Loss: 0.5210\n",
      "Epoch: 1731, Train Acc: 0.7622, Test Acc: 0.7723, Val Acc: 0.7670, Loss: 0.5224\n",
      "Epoch: 1732, Train Acc: 0.7645, Test Acc: 0.7762, Val Acc: 0.7655, Loss: 0.5234\n",
      "Epoch: 1733, Train Acc: 0.7619, Test Acc: 0.7723, Val Acc: 0.7595, Loss: 0.5242\n",
      "Epoch: 1734, Train Acc: 0.7535, Test Acc: 0.7656, Val Acc: 0.7640, Loss: 0.5198\n",
      "Epoch: 1735, Train Acc: 0.7647, Test Acc: 0.7777, Val Acc: 0.7761, Loss: 0.5247\n",
      "Epoch: 1736, Train Acc: 0.7672, Test Acc: 0.7835, Val Acc: 0.7685, Loss: 0.5228\n",
      "Epoch: 1737, Train Acc: 0.7562, Test Acc: 0.7696, Val Acc: 0.7534, Loss: 0.5163\n",
      "Epoch: 1738, Train Acc: 0.7663, Test Acc: 0.7859, Val Acc: 0.7564, Loss: 0.5168\n",
      "Epoch: 1739, Train Acc: 0.7704, Test Acc: 0.7844, Val Acc: 0.7625, Loss: 0.5242\n",
      "Epoch: 1740, Train Acc: 0.7560, Test Acc: 0.7666, Val Acc: 0.7564, Loss: 0.5190\n",
      "Epoch: 1741, Train Acc: 0.7674, Test Acc: 0.7790, Val Acc: 0.7761, Loss: 0.5223\n",
      "Epoch: 1742, Train Acc: 0.7674, Test Acc: 0.7802, Val Acc: 0.7700, Loss: 0.5211\n",
      "Epoch: 1743, Train Acc: 0.7525, Test Acc: 0.7647, Val Acc: 0.7685, Loss: 0.5223\n",
      "Epoch: 1744, Train Acc: 0.7628, Test Acc: 0.7768, Val Acc: 0.7716, Loss: 0.5262\n",
      "Epoch: 1745, Train Acc: 0.7634, Test Acc: 0.7783, Val Acc: 0.7731, Loss: 0.5177\n",
      "Epoch: 1746, Train Acc: 0.7551, Test Acc: 0.7644, Val Acc: 0.7549, Loss: 0.5185\n",
      "Epoch: 1747, Train Acc: 0.7619, Test Acc: 0.7744, Val Acc: 0.7519, Loss: 0.5199\n",
      "Epoch: 1748, Train Acc: 0.7681, Test Acc: 0.7808, Val Acc: 0.7685, Loss: 0.5208\n",
      "Epoch: 1749, Train Acc: 0.7643, Test Acc: 0.7783, Val Acc: 0.7579, Loss: 0.5268\n",
      "Epoch: 1750, Train Acc: 0.7621, Test Acc: 0.7735, Val Acc: 0.7579, Loss: 0.5184\n",
      "Epoch: 1751, Train Acc: 0.7647, Test Acc: 0.7774, Val Acc: 0.7519, Loss: 0.5230\n",
      "Epoch: 1752, Train Acc: 0.7655, Test Acc: 0.7787, Val Acc: 0.7610, Loss: 0.5154\n",
      "Epoch: 1753, Train Acc: 0.7617, Test Acc: 0.7783, Val Acc: 0.7625, Loss: 0.5242\n",
      "Epoch: 1754, Train Acc: 0.7676, Test Acc: 0.7780, Val Acc: 0.7716, Loss: 0.5204\n",
      "Epoch: 1755, Train Acc: 0.7649, Test Acc: 0.7768, Val Acc: 0.7700, Loss: 0.5238\n",
      "Epoch: 1756, Train Acc: 0.7608, Test Acc: 0.7723, Val Acc: 0.7670, Loss: 0.5162\n",
      "Epoch: 1757, Train Acc: 0.7640, Test Acc: 0.7771, Val Acc: 0.7610, Loss: 0.5181\n",
      "Epoch: 1758, Train Acc: 0.7631, Test Acc: 0.7720, Val Acc: 0.7549, Loss: 0.5238\n",
      "Epoch: 1759, Train Acc: 0.7580, Test Acc: 0.7653, Val Acc: 0.7534, Loss: 0.5234\n",
      "Epoch: 1760, Train Acc: 0.7577, Test Acc: 0.7666, Val Acc: 0.7564, Loss: 0.5185\n",
      "Epoch: 1761, Train Acc: 0.7629, Test Acc: 0.7750, Val Acc: 0.7595, Loss: 0.5217\n",
      "Epoch: 1762, Train Acc: 0.7631, Test Acc: 0.7759, Val Acc: 0.7595, Loss: 0.5249\n",
      "Epoch: 1763, Train Acc: 0.7567, Test Acc: 0.7687, Val Acc: 0.7595, Loss: 0.5193\n",
      "Epoch: 1764, Train Acc: 0.7619, Test Acc: 0.7741, Val Acc: 0.7655, Loss: 0.5223\n",
      "Epoch: 1765, Train Acc: 0.7649, Test Acc: 0.7732, Val Acc: 0.7655, Loss: 0.5149\n",
      "Epoch: 1766, Train Acc: 0.7587, Test Acc: 0.7669, Val Acc: 0.7625, Loss: 0.5240\n",
      "Epoch: 1767, Train Acc: 0.7645, Test Acc: 0.7744, Val Acc: 0.7685, Loss: 0.5193\n",
      "Epoch: 1768, Train Acc: 0.7673, Test Acc: 0.7762, Val Acc: 0.7716, Loss: 0.5147\n",
      "Epoch: 1769, Train Acc: 0.7669, Test Acc: 0.7759, Val Acc: 0.7564, Loss: 0.5212\n",
      "Epoch: 1770, Train Acc: 0.7591, Test Acc: 0.7720, Val Acc: 0.7489, Loss: 0.5242\n",
      "Epoch: 1771, Train Acc: 0.7615, Test Acc: 0.7735, Val Acc: 0.7443, Loss: 0.5185\n",
      "Epoch: 1772, Train Acc: 0.7660, Test Acc: 0.7774, Val Acc: 0.7519, Loss: 0.5258\n",
      "Epoch: 1773, Train Acc: 0.7648, Test Acc: 0.7720, Val Acc: 0.7579, Loss: 0.5174\n",
      "Epoch: 1774, Train Acc: 0.7611, Test Acc: 0.7693, Val Acc: 0.7534, Loss: 0.5164\n",
      "Epoch: 1775, Train Acc: 0.7622, Test Acc: 0.7699, Val Acc: 0.7519, Loss: 0.5202\n",
      "Epoch: 1776, Train Acc: 0.7638, Test Acc: 0.7732, Val Acc: 0.7595, Loss: 0.5210\n",
      "Epoch: 1777, Train Acc: 0.7622, Test Acc: 0.7738, Val Acc: 0.7564, Loss: 0.5199\n",
      "Epoch: 1778, Train Acc: 0.7587, Test Acc: 0.7729, Val Acc: 0.7534, Loss: 0.5218\n",
      "Epoch: 1779, Train Acc: 0.7635, Test Acc: 0.7774, Val Acc: 0.7595, Loss: 0.5166\n",
      "Epoch: 1780, Train Acc: 0.7653, Test Acc: 0.7787, Val Acc: 0.7595, Loss: 0.5143\n",
      "Epoch: 1781, Train Acc: 0.7647, Test Acc: 0.7783, Val Acc: 0.7595, Loss: 0.5159\n",
      "Epoch: 1782, Train Acc: 0.7672, Test Acc: 0.7832, Val Acc: 0.7610, Loss: 0.5177\n",
      "Epoch: 1783, Train Acc: 0.7642, Test Acc: 0.7774, Val Acc: 0.7685, Loss: 0.5111\n",
      "Epoch: 1784, Train Acc: 0.7643, Test Acc: 0.7738, Val Acc: 0.7640, Loss: 0.5184\n",
      "Epoch: 1785, Train Acc: 0.7663, Test Acc: 0.7783, Val Acc: 0.7716, Loss: 0.5195\n",
      "Epoch: 1786, Train Acc: 0.7645, Test Acc: 0.7732, Val Acc: 0.7610, Loss: 0.5191\n",
      "Epoch: 1787, Train Acc: 0.7564, Test Acc: 0.7644, Val Acc: 0.7474, Loss: 0.5202\n",
      "Epoch: 1788, Train Acc: 0.7702, Test Acc: 0.7820, Val Acc: 0.7716, Loss: 0.5234\n",
      "Epoch: 1789, Train Acc: 0.7678, Test Acc: 0.7811, Val Acc: 0.7640, Loss: 0.5228\n",
      "Epoch: 1790, Train Acc: 0.7526, Test Acc: 0.7641, Val Acc: 0.7549, Loss: 0.5213\n",
      "Epoch: 1791, Train Acc: 0.7710, Test Acc: 0.7862, Val Acc: 0.7731, Loss: 0.5205\n",
      "Epoch: 1792, Train Acc: 0.7695, Test Acc: 0.7820, Val Acc: 0.7761, Loss: 0.5164\n",
      "Epoch: 1793, Train Acc: 0.7609, Test Acc: 0.7678, Val Acc: 0.7579, Loss: 0.5176\n",
      "Epoch: 1794, Train Acc: 0.7645, Test Acc: 0.7729, Val Acc: 0.7640, Loss: 0.5257\n",
      "Epoch: 1795, Train Acc: 0.7673, Test Acc: 0.7717, Val Acc: 0.7685, Loss: 0.5216\n",
      "Epoch: 1796, Train Acc: 0.7676, Test Acc: 0.7744, Val Acc: 0.7731, Loss: 0.5185\n",
      "Epoch: 1797, Train Acc: 0.7634, Test Acc: 0.7756, Val Acc: 0.7655, Loss: 0.5189\n",
      "Epoch: 1798, Train Acc: 0.7653, Test Acc: 0.7756, Val Acc: 0.7716, Loss: 0.5249\n",
      "Epoch: 1799, Train Acc: 0.7617, Test Acc: 0.7720, Val Acc: 0.7640, Loss: 0.5179\n",
      "Epoch: 1800, Train Acc: 0.7587, Test Acc: 0.7732, Val Acc: 0.7595, Loss: 0.5181\n",
      "Epoch: 1801, Train Acc: 0.7630, Test Acc: 0.7783, Val Acc: 0.7670, Loss: 0.5215\n",
      "Epoch: 1802, Train Acc: 0.7641, Test Acc: 0.7753, Val Acc: 0.7670, Loss: 0.5182\n",
      "Epoch: 1803, Train Acc: 0.7649, Test Acc: 0.7783, Val Acc: 0.7700, Loss: 0.5264\n",
      "Epoch: 1804, Train Acc: 0.7668, Test Acc: 0.7787, Val Acc: 0.7670, Loss: 0.5206\n",
      "Epoch: 1805, Train Acc: 0.7642, Test Acc: 0.7708, Val Acc: 0.7625, Loss: 0.5272\n",
      "Epoch: 1806, Train Acc: 0.7609, Test Acc: 0.7684, Val Acc: 0.7640, Loss: 0.5198\n",
      "Epoch: 1807, Train Acc: 0.7644, Test Acc: 0.7738, Val Acc: 0.7716, Loss: 0.5169\n",
      "Epoch: 1808, Train Acc: 0.7596, Test Acc: 0.7732, Val Acc: 0.7625, Loss: 0.5170\n",
      "Epoch: 1809, Train Acc: 0.7614, Test Acc: 0.7738, Val Acc: 0.7731, Loss: 0.5208\n",
      "Epoch: 1810, Train Acc: 0.7672, Test Acc: 0.7811, Val Acc: 0.7761, Loss: 0.5218\n",
      "Epoch: 1811, Train Acc: 0.7637, Test Acc: 0.7747, Val Acc: 0.7640, Loss: 0.5170\n",
      "Epoch: 1812, Train Acc: 0.7542, Test Acc: 0.7656, Val Acc: 0.7579, Loss: 0.5159\n",
      "Epoch: 1813, Train Acc: 0.7713, Test Acc: 0.7844, Val Acc: 0.7731, Loss: 0.5242\n",
      "Epoch: 1814, Train Acc: 0.7712, Test Acc: 0.7783, Val Acc: 0.7700, Loss: 0.5128\n",
      "Epoch: 1815, Train Acc: 0.7587, Test Acc: 0.7699, Val Acc: 0.7670, Loss: 0.5181\n",
      "Epoch: 1816, Train Acc: 0.7665, Test Acc: 0.7738, Val Acc: 0.7625, Loss: 0.5179\n",
      "Epoch: 1817, Train Acc: 0.7681, Test Acc: 0.7820, Val Acc: 0.7625, Loss: 0.5220\n",
      "Epoch: 1818, Train Acc: 0.7592, Test Acc: 0.7696, Val Acc: 0.7534, Loss: 0.5219\n",
      "Epoch: 1819, Train Acc: 0.7624, Test Acc: 0.7765, Val Acc: 0.7564, Loss: 0.5212\n",
      "Epoch: 1820, Train Acc: 0.7675, Test Acc: 0.7832, Val Acc: 0.7595, Loss: 0.5199\n",
      "Epoch: 1821, Train Acc: 0.7645, Test Acc: 0.7762, Val Acc: 0.7655, Loss: 0.5216\n",
      "Epoch: 1822, Train Acc: 0.7581, Test Acc: 0.7681, Val Acc: 0.7504, Loss: 0.5207\n",
      "Epoch: 1823, Train Acc: 0.7669, Test Acc: 0.7759, Val Acc: 0.7685, Loss: 0.5223\n",
      "Epoch: 1824, Train Acc: 0.7688, Test Acc: 0.7805, Val Acc: 0.7700, Loss: 0.5143\n",
      "Epoch: 1825, Train Acc: 0.7602, Test Acc: 0.7735, Val Acc: 0.7579, Loss: 0.5228\n",
      "Epoch: 1826, Train Acc: 0.7623, Test Acc: 0.7726, Val Acc: 0.7595, Loss: 0.5247\n",
      "Epoch: 1827, Train Acc: 0.7670, Test Acc: 0.7774, Val Acc: 0.7595, Loss: 0.5111\n",
      "Epoch: 1828, Train Acc: 0.7668, Test Acc: 0.7777, Val Acc: 0.7655, Loss: 0.5265\n",
      "Epoch: 1829, Train Acc: 0.7605, Test Acc: 0.7744, Val Acc: 0.7655, Loss: 0.5234\n",
      "Epoch: 1830, Train Acc: 0.7647, Test Acc: 0.7793, Val Acc: 0.7625, Loss: 0.5265\n",
      "Epoch: 1831, Train Acc: 0.7658, Test Acc: 0.7811, Val Acc: 0.7685, Loss: 0.5235\n",
      "Epoch: 1832, Train Acc: 0.7529, Test Acc: 0.7629, Val Acc: 0.7549, Loss: 0.5210\n",
      "Epoch: 1833, Train Acc: 0.7568, Test Acc: 0.7656, Val Acc: 0.7489, Loss: 0.5239\n",
      "Epoch: 1834, Train Acc: 0.7658, Test Acc: 0.7777, Val Acc: 0.7564, Loss: 0.5232\n",
      "Epoch: 1835, Train Acc: 0.7599, Test Acc: 0.7714, Val Acc: 0.7534, Loss: 0.5218\n",
      "Epoch: 1836, Train Acc: 0.7538, Test Acc: 0.7666, Val Acc: 0.7458, Loss: 0.5212\n",
      "Epoch: 1837, Train Acc: 0.7667, Test Acc: 0.7820, Val Acc: 0.7670, Loss: 0.5228\n",
      "Epoch: 1838, Train Acc: 0.7661, Test Acc: 0.7826, Val Acc: 0.7731, Loss: 0.5255\n",
      "Epoch: 1839, Train Acc: 0.7590, Test Acc: 0.7711, Val Acc: 0.7640, Loss: 0.5220\n",
      "Epoch: 1840, Train Acc: 0.7633, Test Acc: 0.7717, Val Acc: 0.7655, Loss: 0.5243\n",
      "Epoch: 1841, Train Acc: 0.7640, Test Acc: 0.7747, Val Acc: 0.7806, Loss: 0.5256\n",
      "Epoch: 1842, Train Acc: 0.7594, Test Acc: 0.7675, Val Acc: 0.7625, Loss: 0.5239\n",
      "Epoch: 1843, Train Acc: 0.7604, Test Acc: 0.7693, Val Acc: 0.7625, Loss: 0.5202\n",
      "Epoch: 1844, Train Acc: 0.7651, Test Acc: 0.7765, Val Acc: 0.7806, Loss: 0.5222\n",
      "Epoch: 1845, Train Acc: 0.7603, Test Acc: 0.7735, Val Acc: 0.7564, Loss: 0.5181\n",
      "Epoch: 1846, Train Acc: 0.7597, Test Acc: 0.7720, Val Acc: 0.7534, Loss: 0.5196\n",
      "Epoch: 1847, Train Acc: 0.7657, Test Acc: 0.7783, Val Acc: 0.7625, Loss: 0.5257\n",
      "Epoch: 1848, Train Acc: 0.7668, Test Acc: 0.7777, Val Acc: 0.7595, Loss: 0.5175\n",
      "Epoch: 1849, Train Acc: 0.7576, Test Acc: 0.7699, Val Acc: 0.7534, Loss: 0.5251\n",
      "Epoch: 1850, Train Acc: 0.7640, Test Acc: 0.7741, Val Acc: 0.7564, Loss: 0.5229\n",
      "Epoch: 1851, Train Acc: 0.7654, Test Acc: 0.7744, Val Acc: 0.7610, Loss: 0.5244\n",
      "Epoch: 1852, Train Acc: 0.7620, Test Acc: 0.7720, Val Acc: 0.7519, Loss: 0.5154\n",
      "Epoch: 1853, Train Acc: 0.7623, Test Acc: 0.7723, Val Acc: 0.7504, Loss: 0.5221\n",
      "Epoch: 1854, Train Acc: 0.7654, Test Acc: 0.7741, Val Acc: 0.7549, Loss: 0.5192\n",
      "Epoch: 1855, Train Acc: 0.7653, Test Acc: 0.7747, Val Acc: 0.7625, Loss: 0.5254\n",
      "Epoch: 1856, Train Acc: 0.7677, Test Acc: 0.7787, Val Acc: 0.7716, Loss: 0.5209\n",
      "Epoch: 1857, Train Acc: 0.7672, Test Acc: 0.7790, Val Acc: 0.7776, Loss: 0.5156\n",
      "Epoch: 1858, Train Acc: 0.7627, Test Acc: 0.7750, Val Acc: 0.7655, Loss: 0.5218\n",
      "Epoch: 1859, Train Acc: 0.7624, Test Acc: 0.7744, Val Acc: 0.7655, Loss: 0.5228\n",
      "Epoch: 1860, Train Acc: 0.7682, Test Acc: 0.7823, Val Acc: 0.7670, Loss: 0.5199\n",
      "Epoch: 1861, Train Acc: 0.7618, Test Acc: 0.7741, Val Acc: 0.7564, Loss: 0.5178\n",
      "Epoch: 1862, Train Acc: 0.7570, Test Acc: 0.7738, Val Acc: 0.7398, Loss: 0.5239\n",
      "Epoch: 1863, Train Acc: 0.7607, Test Acc: 0.7756, Val Acc: 0.7549, Loss: 0.5196\n",
      "Epoch: 1864, Train Acc: 0.7646, Test Acc: 0.7759, Val Acc: 0.7579, Loss: 0.5131\n",
      "Epoch: 1865, Train Acc: 0.7605, Test Acc: 0.7735, Val Acc: 0.7519, Loss: 0.5242\n",
      "Epoch: 1866, Train Acc: 0.7601, Test Acc: 0.7711, Val Acc: 0.7564, Loss: 0.5237\n",
      "Epoch: 1867, Train Acc: 0.7631, Test Acc: 0.7759, Val Acc: 0.7610, Loss: 0.5158\n",
      "Epoch: 1868, Train Acc: 0.7613, Test Acc: 0.7759, Val Acc: 0.7579, Loss: 0.5222\n",
      "Epoch: 1869, Train Acc: 0.7565, Test Acc: 0.7708, Val Acc: 0.7504, Loss: 0.5214\n",
      "Epoch: 1870, Train Acc: 0.7595, Test Acc: 0.7732, Val Acc: 0.7640, Loss: 0.5178\n",
      "Epoch: 1871, Train Acc: 0.7649, Test Acc: 0.7777, Val Acc: 0.7655, Loss: 0.5238\n",
      "Epoch: 1872, Train Acc: 0.7623, Test Acc: 0.7747, Val Acc: 0.7625, Loss: 0.5194\n",
      "Epoch: 1873, Train Acc: 0.7626, Test Acc: 0.7717, Val Acc: 0.7655, Loss: 0.5184\n",
      "Epoch: 1874, Train Acc: 0.7684, Test Acc: 0.7802, Val Acc: 0.7610, Loss: 0.5205\n",
      "Epoch: 1875, Train Acc: 0.7667, Test Acc: 0.7780, Val Acc: 0.7564, Loss: 0.5218\n",
      "Epoch: 1876, Train Acc: 0.7650, Test Acc: 0.7753, Val Acc: 0.7534, Loss: 0.5198\n",
      "Epoch: 1877, Train Acc: 0.7682, Test Acc: 0.7814, Val Acc: 0.7640, Loss: 0.5179\n",
      "Epoch: 1878, Train Acc: 0.7717, Test Acc: 0.7862, Val Acc: 0.7761, Loss: 0.5194\n",
      "Epoch: 1879, Train Acc: 0.7681, Test Acc: 0.7753, Val Acc: 0.7761, Loss: 0.5133\n",
      "Epoch: 1880, Train Acc: 0.7674, Test Acc: 0.7783, Val Acc: 0.7761, Loss: 0.5178\n",
      "Epoch: 1881, Train Acc: 0.7674, Test Acc: 0.7805, Val Acc: 0.7776, Loss: 0.5209\n",
      "Epoch: 1882, Train Acc: 0.7620, Test Acc: 0.7726, Val Acc: 0.7716, Loss: 0.5170\n",
      "Epoch: 1883, Train Acc: 0.7619, Test Acc: 0.7762, Val Acc: 0.7776, Loss: 0.5194\n",
      "Epoch: 1884, Train Acc: 0.7673, Test Acc: 0.7823, Val Acc: 0.7821, Loss: 0.5217\n",
      "Epoch: 1885, Train Acc: 0.7643, Test Acc: 0.7793, Val Acc: 0.7700, Loss: 0.5148\n",
      "Epoch: 1886, Train Acc: 0.7692, Test Acc: 0.7811, Val Acc: 0.7806, Loss: 0.5240\n",
      "Epoch: 1887, Train Acc: 0.7658, Test Acc: 0.7790, Val Acc: 0.7670, Loss: 0.5192\n",
      "Epoch: 1888, Train Acc: 0.7605, Test Acc: 0.7771, Val Acc: 0.7640, Loss: 0.5162\n",
      "Epoch: 1889, Train Acc: 0.7650, Test Acc: 0.7777, Val Acc: 0.7640, Loss: 0.5207\n",
      "Epoch: 1890, Train Acc: 0.7676, Test Acc: 0.7787, Val Acc: 0.7610, Loss: 0.5180\n",
      "Epoch: 1891, Train Acc: 0.7624, Test Acc: 0.7696, Val Acc: 0.7610, Loss: 0.5166\n",
      "Epoch: 1892, Train Acc: 0.7680, Test Acc: 0.7765, Val Acc: 0.7549, Loss: 0.5247\n",
      "Epoch: 1893, Train Acc: 0.7722, Test Acc: 0.7850, Val Acc: 0.7700, Loss: 0.5218\n",
      "Epoch: 1894, Train Acc: 0.7593, Test Acc: 0.7753, Val Acc: 0.7549, Loss: 0.5230\n",
      "Epoch: 1895, Train Acc: 0.7595, Test Acc: 0.7714, Val Acc: 0.7489, Loss: 0.5202\n",
      "Epoch: 1896, Train Acc: 0.7684, Test Acc: 0.7808, Val Acc: 0.7579, Loss: 0.5210\n",
      "Epoch: 1897, Train Acc: 0.7650, Test Acc: 0.7738, Val Acc: 0.7564, Loss: 0.5258\n",
      "Epoch: 1898, Train Acc: 0.7584, Test Acc: 0.7690, Val Acc: 0.7489, Loss: 0.5185\n",
      "Epoch: 1899, Train Acc: 0.7659, Test Acc: 0.7753, Val Acc: 0.7716, Loss: 0.5209\n",
      "Epoch: 1900, Train Acc: 0.7647, Test Acc: 0.7744, Val Acc: 0.7716, Loss: 0.5236\n",
      "Epoch: 1901, Train Acc: 0.7606, Test Acc: 0.7708, Val Acc: 0.7625, Loss: 0.5165\n",
      "Epoch: 1902, Train Acc: 0.7611, Test Acc: 0.7738, Val Acc: 0.7685, Loss: 0.5157\n",
      "Epoch: 1903, Train Acc: 0.7632, Test Acc: 0.7723, Val Acc: 0.7685, Loss: 0.5203\n",
      "Epoch: 1904, Train Acc: 0.7646, Test Acc: 0.7735, Val Acc: 0.7685, Loss: 0.5153\n",
      "Epoch: 1905, Train Acc: 0.7710, Test Acc: 0.7783, Val Acc: 0.7655, Loss: 0.5185\n",
      "Epoch: 1906, Train Acc: 0.7707, Test Acc: 0.7774, Val Acc: 0.7655, Loss: 0.5188\n",
      "Epoch: 1907, Train Acc: 0.7669, Test Acc: 0.7793, Val Acc: 0.7670, Loss: 0.5286\n",
      "Epoch: 1908, Train Acc: 0.7659, Test Acc: 0.7808, Val Acc: 0.7685, Loss: 0.5138\n",
      "Epoch: 1909, Train Acc: 0.7695, Test Acc: 0.7862, Val Acc: 0.7670, Loss: 0.5218\n",
      "Epoch: 1910, Train Acc: 0.7650, Test Acc: 0.7808, Val Acc: 0.7564, Loss: 0.5186\n",
      "Epoch: 1911, Train Acc: 0.7611, Test Acc: 0.7696, Val Acc: 0.7474, Loss: 0.5189\n",
      "Epoch: 1912, Train Acc: 0.7704, Test Acc: 0.7808, Val Acc: 0.7504, Loss: 0.5165\n",
      "Epoch: 1913, Train Acc: 0.7690, Test Acc: 0.7814, Val Acc: 0.7595, Loss: 0.5166\n",
      "Epoch: 1914, Train Acc: 0.7617, Test Acc: 0.7720, Val Acc: 0.7655, Loss: 0.5254\n",
      "Epoch: 1915, Train Acc: 0.7588, Test Acc: 0.7696, Val Acc: 0.7549, Loss: 0.5135\n",
      "Epoch: 1916, Train Acc: 0.7630, Test Acc: 0.7756, Val Acc: 0.7655, Loss: 0.5197\n",
      "Epoch: 1917, Train Acc: 0.7619, Test Acc: 0.7787, Val Acc: 0.7610, Loss: 0.5223\n",
      "Epoch: 1918, Train Acc: 0.7549, Test Acc: 0.7726, Val Acc: 0.7625, Loss: 0.5147\n",
      "Epoch: 1919, Train Acc: 0.7602, Test Acc: 0.7756, Val Acc: 0.7837, Loss: 0.5197\n",
      "Epoch: 1920, Train Acc: 0.7686, Test Acc: 0.7814, Val Acc: 0.7912, Loss: 0.5206\n",
      "Epoch: 1921, Train Acc: 0.7622, Test Acc: 0.7744, Val Acc: 0.7640, Loss: 0.5213\n",
      "Epoch: 1922, Train Acc: 0.7580, Test Acc: 0.7681, Val Acc: 0.7610, Loss: 0.5227\n",
      "Epoch: 1923, Train Acc: 0.7676, Test Acc: 0.7811, Val Acc: 0.7579, Loss: 0.5184\n",
      "Epoch: 1924, Train Acc: 0.7659, Test Acc: 0.7793, Val Acc: 0.7625, Loss: 0.5212\n",
      "Epoch: 1925, Train Acc: 0.7584, Test Acc: 0.7705, Val Acc: 0.7534, Loss: 0.5170\n",
      "Epoch: 1926, Train Acc: 0.7673, Test Acc: 0.7814, Val Acc: 0.7595, Loss: 0.5267\n",
      "Epoch: 1927, Train Acc: 0.7689, Test Acc: 0.7832, Val Acc: 0.7700, Loss: 0.5212\n",
      "Epoch: 1928, Train Acc: 0.7556, Test Acc: 0.7678, Val Acc: 0.7625, Loss: 0.5206\n",
      "Epoch: 1929, Train Acc: 0.7579, Test Acc: 0.7684, Val Acc: 0.7655, Loss: 0.5198\n",
      "Epoch: 1930, Train Acc: 0.7676, Test Acc: 0.7796, Val Acc: 0.7731, Loss: 0.5313\n",
      "Epoch: 1931, Train Acc: 0.7622, Test Acc: 0.7756, Val Acc: 0.7685, Loss: 0.5174\n",
      "Epoch: 1932, Train Acc: 0.7554, Test Acc: 0.7681, Val Acc: 0.7595, Loss: 0.5212\n",
      "Epoch: 1933, Train Acc: 0.7657, Test Acc: 0.7762, Val Acc: 0.7625, Loss: 0.5207\n",
      "Epoch: 1934, Train Acc: 0.7677, Test Acc: 0.7771, Val Acc: 0.7640, Loss: 0.5214\n",
      "Epoch: 1935, Train Acc: 0.7593, Test Acc: 0.7699, Val Acc: 0.7579, Loss: 0.5221\n",
      "Epoch: 1936, Train Acc: 0.7621, Test Acc: 0.7714, Val Acc: 0.7595, Loss: 0.5220\n",
      "Epoch: 1937, Train Acc: 0.7708, Test Acc: 0.7771, Val Acc: 0.7625, Loss: 0.5224\n",
      "Epoch: 1938, Train Acc: 0.7645, Test Acc: 0.7720, Val Acc: 0.7610, Loss: 0.5268\n",
      "Epoch: 1939, Train Acc: 0.7537, Test Acc: 0.7608, Val Acc: 0.7519, Loss: 0.5140\n",
      "Epoch: 1940, Train Acc: 0.7668, Test Acc: 0.7768, Val Acc: 0.7670, Loss: 0.5208\n",
      "Epoch: 1941, Train Acc: 0.7689, Test Acc: 0.7805, Val Acc: 0.7731, Loss: 0.5198\n",
      "Epoch: 1942, Train Acc: 0.7549, Test Acc: 0.7672, Val Acc: 0.7564, Loss: 0.5213\n",
      "Epoch: 1943, Train Acc: 0.7630, Test Acc: 0.7774, Val Acc: 0.7685, Loss: 0.5255\n",
      "Epoch: 1944, Train Acc: 0.7660, Test Acc: 0.7799, Val Acc: 0.7821, Loss: 0.5264\n",
      "Epoch: 1945, Train Acc: 0.7540, Test Acc: 0.7702, Val Acc: 0.7564, Loss: 0.5261\n",
      "Epoch: 1946, Train Acc: 0.7471, Test Acc: 0.7614, Val Acc: 0.7383, Loss: 0.5165\n",
      "Epoch: 1947, Train Acc: 0.7640, Test Acc: 0.7771, Val Acc: 0.7640, Loss: 0.5235\n",
      "Epoch: 1948, Train Acc: 0.7660, Test Acc: 0.7787, Val Acc: 0.7595, Loss: 0.5234\n",
      "Epoch: 1949, Train Acc: 0.7487, Test Acc: 0.7617, Val Acc: 0.7474, Loss: 0.5263\n",
      "Epoch: 1950, Train Acc: 0.7589, Test Acc: 0.7726, Val Acc: 0.7595, Loss: 0.5194\n",
      "Epoch: 1951, Train Acc: 0.7661, Test Acc: 0.7774, Val Acc: 0.7655, Loss: 0.5185\n",
      "Epoch: 1952, Train Acc: 0.7599, Test Acc: 0.7741, Val Acc: 0.7595, Loss: 0.5239\n",
      "Epoch: 1953, Train Acc: 0.7591, Test Acc: 0.7720, Val Acc: 0.7685, Loss: 0.5263\n",
      "Epoch: 1954, Train Acc: 0.7696, Test Acc: 0.7808, Val Acc: 0.7776, Loss: 0.5221\n",
      "Epoch: 1955, Train Acc: 0.7643, Test Acc: 0.7750, Val Acc: 0.7761, Loss: 0.5183\n",
      "Epoch: 1956, Train Acc: 0.7610, Test Acc: 0.7726, Val Acc: 0.7761, Loss: 0.5172\n",
      "Epoch: 1957, Train Acc: 0.7651, Test Acc: 0.7756, Val Acc: 0.7716, Loss: 0.5207\n",
      "Epoch: 1958, Train Acc: 0.7677, Test Acc: 0.7768, Val Acc: 0.7716, Loss: 0.5180\n",
      "Epoch: 1959, Train Acc: 0.7611, Test Acc: 0.7768, Val Acc: 0.7595, Loss: 0.5245\n",
      "Epoch: 1960, Train Acc: 0.7607, Test Acc: 0.7756, Val Acc: 0.7625, Loss: 0.5221\n",
      "Epoch: 1961, Train Acc: 0.7648, Test Acc: 0.7762, Val Acc: 0.7746, Loss: 0.5241\n",
      "Epoch: 1962, Train Acc: 0.7636, Test Acc: 0.7753, Val Acc: 0.7746, Loss: 0.5154\n",
      "Epoch: 1963, Train Acc: 0.7572, Test Acc: 0.7720, Val Acc: 0.7474, Loss: 0.5206\n",
      "Epoch: 1964, Train Acc: 0.7649, Test Acc: 0.7696, Val Acc: 0.7549, Loss: 0.5195\n",
      "Epoch: 1965, Train Acc: 0.7667, Test Acc: 0.7726, Val Acc: 0.7549, Loss: 0.5201\n",
      "Epoch: 1966, Train Acc: 0.7605, Test Acc: 0.7666, Val Acc: 0.7489, Loss: 0.5189\n",
      "Epoch: 1967, Train Acc: 0.7570, Test Acc: 0.7666, Val Acc: 0.7595, Loss: 0.5191\n",
      "Epoch: 1968, Train Acc: 0.7635, Test Acc: 0.7762, Val Acc: 0.7670, Loss: 0.5222\n",
      "Epoch: 1969, Train Acc: 0.7641, Test Acc: 0.7762, Val Acc: 0.7731, Loss: 0.5228\n",
      "Epoch: 1970, Train Acc: 0.7583, Test Acc: 0.7723, Val Acc: 0.7685, Loss: 0.5209\n",
      "Epoch: 1971, Train Acc: 0.7660, Test Acc: 0.7814, Val Acc: 0.7806, Loss: 0.5237\n",
      "Epoch: 1972, Train Acc: 0.7656, Test Acc: 0.7793, Val Acc: 0.7700, Loss: 0.5249\n",
      "Epoch: 1973, Train Acc: 0.7582, Test Acc: 0.7675, Val Acc: 0.7640, Loss: 0.5212\n",
      "Epoch: 1974, Train Acc: 0.7611, Test Acc: 0.7717, Val Acc: 0.7595, Loss: 0.5166\n",
      "Epoch: 1975, Train Acc: 0.7657, Test Acc: 0.7765, Val Acc: 0.7700, Loss: 0.5172\n",
      "Epoch: 1976, Train Acc: 0.7613, Test Acc: 0.7693, Val Acc: 0.7640, Loss: 0.5191\n",
      "Epoch: 1977, Train Acc: 0.7640, Test Acc: 0.7756, Val Acc: 0.7534, Loss: 0.5211\n",
      "Epoch: 1978, Train Acc: 0.7649, Test Acc: 0.7750, Val Acc: 0.7489, Loss: 0.5199\n",
      "Epoch: 1979, Train Acc: 0.7653, Test Acc: 0.7759, Val Acc: 0.7595, Loss: 0.5157\n",
      "Epoch: 1980, Train Acc: 0.7610, Test Acc: 0.7708, Val Acc: 0.7655, Loss: 0.5151\n",
      "Epoch: 1981, Train Acc: 0.7656, Test Acc: 0.7768, Val Acc: 0.7731, Loss: 0.5124\n",
      "Epoch: 1982, Train Acc: 0.7663, Test Acc: 0.7796, Val Acc: 0.7731, Loss: 0.5272\n",
      "Epoch: 1983, Train Acc: 0.7626, Test Acc: 0.7723, Val Acc: 0.7625, Loss: 0.5187\n",
      "Epoch: 1984, Train Acc: 0.7619, Test Acc: 0.7714, Val Acc: 0.7595, Loss: 0.5227\n",
      "Epoch: 1985, Train Acc: 0.7597, Test Acc: 0.7693, Val Acc: 0.7640, Loss: 0.5256\n",
      "Epoch: 1986, Train Acc: 0.7592, Test Acc: 0.7696, Val Acc: 0.7685, Loss: 0.5227\n",
      "Epoch: 1987, Train Acc: 0.7633, Test Acc: 0.7735, Val Acc: 0.7670, Loss: 0.5181\n",
      "Epoch: 1988, Train Acc: 0.7635, Test Acc: 0.7726, Val Acc: 0.7761, Loss: 0.5246\n",
      "Epoch: 1989, Train Acc: 0.7643, Test Acc: 0.7729, Val Acc: 0.7700, Loss: 0.5223\n",
      "Epoch: 1990, Train Acc: 0.7618, Test Acc: 0.7684, Val Acc: 0.7579, Loss: 0.5216\n",
      "Epoch: 1991, Train Acc: 0.7631, Test Acc: 0.7714, Val Acc: 0.7564, Loss: 0.5190\n",
      "Epoch: 1992, Train Acc: 0.7649, Test Acc: 0.7796, Val Acc: 0.7579, Loss: 0.5227\n",
      "Epoch: 1993, Train Acc: 0.7614, Test Acc: 0.7774, Val Acc: 0.7579, Loss: 0.5161\n",
      "Epoch: 1994, Train Acc: 0.7614, Test Acc: 0.7774, Val Acc: 0.7731, Loss: 0.5179\n",
      "Epoch: 1995, Train Acc: 0.7671, Test Acc: 0.7823, Val Acc: 0.7731, Loss: 0.5245\n",
      "Epoch: 1996, Train Acc: 0.7627, Test Acc: 0.7765, Val Acc: 0.7685, Loss: 0.5156\n",
      "Epoch: 1997, Train Acc: 0.7581, Test Acc: 0.7735, Val Acc: 0.7549, Loss: 0.5182\n",
      "Epoch: 1998, Train Acc: 0.7595, Test Acc: 0.7687, Val Acc: 0.7610, Loss: 0.5230\n",
      "Epoch: 1999, Train Acc: 0.7603, Test Acc: 0.7705, Val Acc: 0.7685, Loss: 0.5149\n",
      "Epoch: 2000, Train Acc: 0.7600, Test Acc: 0.7708, Val Acc: 0.7746, Loss: 0.5265\n"
     ]
    }
   ],
   "source": [
    "if m != 'lessinform':\n",
    "    model = GCN(16, input_feat = 12, classes = 2).to(device)\n",
    "else:\n",
    "    model = GCN(16, input_feat = 22, classes = 2).to(device)\n",
    "print(model)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "for epoch in range(1, 2001):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    loss = criterion(out[train_mask], data.y[train_mask])\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(),2.0)\n",
    "    optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data.x, data.edge_index)\n",
    "\n",
    "    # Evaluate train\n",
    "    train_acc = evaluate(out[train_mask], data.y[train_mask])\n",
    "    test_acc = evaluate(out[test_mask], data.y[test_mask])\n",
    "    val_acc = evaluate(out[val_mask], data.y[val_mask])\n",
    "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}, Val Acc: {val_acc:.4f}, Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72ddd28c-83d9-4aa6-97ea-67195491a59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_exp = Gr.explanations\n",
    "num_hops = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17897b67-85e2-49a1-a775-c5f2f1a3101d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    lr = trial.suggest_float('lrs', 1e-6, 0.2)\n",
    "    alpha = trial.suggest_float('a', 0.1, 1)\n",
    "    beta = trial.suggest_float('b', 0.1, 1)\n",
    "    explainer = BetaExplainer(model, data.x, data.edge_index, torch.device('cpu'), alpha, beta)\n",
    "    explainer.train(5, lr)\n",
    "    betaem = explainer.edge_mask()\n",
    "    best_acc = 0\n",
    "    for i in range(0, len(gt_exp)):\n",
    "        subset, sub_edge_index, mapping, hard_edge_mask = \\\n",
    "            k_hop_subgraph(i, num_hops, data.edge_index,\n",
    "                          relabel_nodes=False)\n",
    "        ei = data.edge_index[:,hard_edge_mask]\n",
    "        exp = betaem[hard_edge_mask]\n",
    "        accuracy, f1, prec, rec = graph_exp_acc(gt_exp[i], exp, node_thresh_factor = 0.5)\n",
    "        if accuracy >= best_acc:\n",
    "            best_acc = accuracy\n",
    "            best_faith = faithfulness(model, data.x, ei, exp)\n",
    "    print(best_acc, best_faith)\n",
    "    return best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06bffc3e-659b-461b-806e-9176b1318133",
   "metadata": {},
   "outputs": [],
   "source": [
    "pruner = optuna.pruners.MedianPruner()\n",
    "study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(), pruner=pruner)\n",
    "study.optimize(objective, n_trials=50)\n",
    "print('Best hyperparameters:', study.best_params)\n",
    "print('Best accuracy:', study.best_value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
